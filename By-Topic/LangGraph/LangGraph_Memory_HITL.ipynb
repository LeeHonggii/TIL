{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: 메모리 추가 💾\n",
    "\n",
    "## 🎯 목적\n",
    "세션 간에도 사용자 정보를 유지하는 **영구 상태 관리**를 추가합니다.\n",
    "\n",
    "### Before\n",
    "- 세션이 바뀌면 이전 정보 소실\n",
    "\n",
    "### After\n",
    "- 사용자별로 중요한 정보를 저장하고 재사용\n",
    "\n",
    "### 핵심 개념\n",
    "- **💾 Checkpointer**: 대화 상태 저장/복원\n",
    "- **🏷️ Thread ID**: 세션 식별자\n",
    "- **🗂️ Persistent State**: 누적 이력 기반 컨텍스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "from typing import List, Optional\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 메모리 추출기 설정\n",
    "\n",
    "대화에서 중요한 정보를 자동으로 추출하여 구조화된 형태로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic 모델 정의\n",
    "class MemoryItem(BaseModel):\n",
    "    \"\"\"개별 메모리 아이템\"\"\"\n",
    "    key: str = Field(description=\"메모리 키 (예: user_name, preference, fact)\")\n",
    "    value: str = Field(description=\"메모리 값\")\n",
    "    category: str = Field(\n",
    "        description=\"카테고리 (personal_info, preference, interest, relationship, fact, etc.)\"\n",
    "    )\n",
    "    importance: int = Field(description=\"중요도 (1-5, 5가 가장 중요)\", ge=1, le=5)\n",
    "    confidence: float = Field(description=\"추출 신뢰도 (0.0-1.0)\", ge=0.0, le=1.0)\n",
    "\n",
    "\n",
    "class ExtractedMemories(BaseModel):\n",
    "    \"\"\"추출된 메모리 컬렉션\"\"\"\n",
    "    memories: List[MemoryItem] = Field(description=\"추출된 메모리 아이템 리스트\")\n",
    "    summary: str = Field(description=\"대화 내용 요약\")\n",
    "    timestamp: str = Field(\n",
    "        default_factory=lambda: datetime.now().isoformat(), \n",
    "        description=\"추출 시간\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_memory_extractor(model: str = \"gpt-4o\"):\n",
    "    \"\"\"메모리 추출기를 생성합니다.\"\"\"\n",
    "    \n",
    "    # Output Parser 생성\n",
    "    memory_parser = PydanticOutputParser(pydantic_object=ExtractedMemories)\n",
    "    \n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"\"\"You are an expert memory extraction assistant. \n",
    "    Extract important information from user conversations and convert them into structured key-value pairs.\n",
    "    \n",
    "    Extract ALL relevant information including:\n",
    "    - Personal information (name, age, location, occupation, etc.)\n",
    "    - Preferences and interests\n",
    "    - Important facts or events mentioned\n",
    "    - Goals and aspirations\"\"\"\n",
    "    \n",
    "    # 프롬프트 템플릿\n",
    "    template = f\"\"\"{system_prompt}\n",
    "    \n",
    "    User Input: {{input}}\n",
    "    \n",
    "    {{format_instructions}}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        template,\n",
    "        partial_variables={\n",
    "            \"format_instructions\": memory_parser.get_format_instructions()\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # 모델 설정\n",
    "    llm = ChatOpenAI(model=model, temperature=0)\n",
    "    \n",
    "    # 메모리 추출 체인 생성\n",
    "    memory_extractor = prompt | llm | memory_parser\n",
    "    \n",
    "    return memory_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 메모리 기능이 있는 그래프 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.store.base import BaseStore\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "memory_extractor = create_memory_extractor(model=\"gpt-4o\")\n",
    "\n",
    "\n",
    "def call_model(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    *,\n",
    "    store: BaseStore,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"LLM 모델을 호출하고 사용자 메모리를 관리합니다.\n",
    "    \n",
    "    Args:\n",
    "        state: 현재 상태 (메시지 포함)\n",
    "        config: 실행 설정\n",
    "        store: 메모리 저장소\n",
    "    \"\"\"\n",
    "    # 마지막 메시지에서 user_id 추출\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memories\", user_id)\n",
    "    \n",
    "    # 유저의 메모리 검색\n",
    "    memories = store.search(namespace, query=str(state[\"messages\"][-1].content))\n",
    "    info = \"\\n\".join([f\"{memory.key}: {memory.value}\" for memory in memories])\n",
    "    system_msg = f\"You are a helpful assistant. User info: {info}\" if info else \"You are a helpful assistant.\"\n",
    "    \n",
    "    # 사용자가 기억 요청 시 메모리 저장\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if \"remember\" in last_message.content.lower():\n",
    "        result = memory_extractor.invoke({\"input\": str(state[\"messages\"][-1].content)})\n",
    "        for memory in result.memories:\n",
    "            print(f\"💾 저장: {memory.key} = {memory.value}\")\n",
    "            store.put(namespace, str(uuid.uuid4()), {memory.key: memory.value})\n",
    "    \n",
    "    # LLM 호출\n",
    "    response = model.invoke(\n",
    "        [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n",
    "    )\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# 그래프 빌드\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "\n",
    "# 메모리 체크포인터 생성\n",
    "# 실제 프로덕션에서는 PostgresSaver 사용 권장\n",
    "memory_saver = InMemorySaver()\n",
    "memory_store = InMemoryStore()\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph_with_memory = builder.compile(\n",
    "    checkpointer=memory_saver,\n",
    "    store=memory_store,\n",
    ")\n",
    "\n",
    "print(\"✅ 메모리 기능이 있는 그래프 생성 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메모리 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_graph(msg, thread_id=\"default\", user_id=\"default\"):\n",
    "    \"\"\"그래프를 실행하는 헬퍼 함수\"\"\"\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": thread_id + user_id,\n",
    "            \"user_id\": user_id,\n",
    "        }\n",
    "    }\n",
    "    print(f\"\\n[유저🙋] {msg}\")\n",
    "    \n",
    "    result = graph_with_memory.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": msg}]},\n",
    "        config=config,\n",
    "    )\n",
    "    \n",
    "    print(f\"[AI🤖] {result['messages'][-1].content}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메시지, thread_id, user_id 전달\n",
    "run_graph(\"안녕? 내 이름은 테디야\", \"1\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 같은 thread에서 대화 계속\n",
    "run_graph(\"내 이름이 뭐라고?\", \"1\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 thread에서는 기억하지 못함\n",
    "run_graph(\"내 이름이 뭐라고?\", \"2\", \"someone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧠 장기 기억 저장: `remember` 키워드\n",
    "\n",
    "메시지에 `remember` 가 포함되면 중요 정보를 장기 저장소에 기록합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember 키워드로 장기 기억에 저장\n",
    "run_graph(\"내 이름이 테디야 remember\", \"2\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가 정보 저장\n",
    "run_graph(\n",
    "    \"내 직업은 AI Engineer 야. 내 취미는 Netflix 보기 야. remember\", \n",
    "    \"4\", \n",
    "    \"someone\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 스레드에서도 장기 기억은 유지됨\n",
    "run_graph(\"내 이름, 직업, 취미 알려줘\", \"100\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 user_id로 실행한 경우는 기억하지 못함\n",
    "run_graph(\"내 이름, 직업, 취미 알려줘\", \"100\", \"other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 State 확인\n",
    "\n",
    "현재 저장된 상태를 조회해 메시지 이력과 체크포인트 정보를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 Config 설정\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"100\" + \"someone\",\n",
    "        \"user_id\": \"someone\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# 현재 상태 가져오기\n",
    "snapshot = graph_with_memory.get_state(config)\n",
    "\n",
    "print(\"📊 현재 상태 정보:\")\n",
    "print(f\"- 메시지 수: {len(snapshot.values['messages'])}개\")\n",
    "print(f\"- 체크포인트 ID: {snapshot.config['configurable']['checkpoint_id']}\")\n",
    "\n",
    "# 최근 메시지 몇 개 표시\n",
    "print(\"\\n[최근 메시지]\")\n",
    "for msg in snapshot.values[\"messages\"][-3:]:\n",
    "    role = msg.type if hasattr(msg, \"type\") else \"unknown\"\n",
    "    content = msg.content if hasattr(msg, \"content\") else str(msg)\n",
    "    print(f\"  [{role}]: {content[:100]}...\" if len(content) > 100 else f\"  [{role}]: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Human-in-the-Loop 🙋\n",
    "\n",
    "## 🎯 목적\n",
    "고위험/중요 작업에 대해 AI가 스스로 멈추고 인간 승인을 요청하는 승인 기반 흐름을 도입합니다.\n",
    "\n",
    "### 언제 승인이 필요한가\n",
    "- **💰 금융 처리**: 결제/이체/투자\n",
    "- **🏥 의료 조언**: 처방/치료 권고\n",
    "- **📧 대외 커뮤니케이션**: 공지/발송\n",
    "- **🔐 보안 변경**: 권한/설정\n",
    "\n",
    "### 핵심 개념\n",
    "- **⏸️ interrupt**: 실행 일시정지 및 승인 대기\n",
    "- **📋 Command**: 승인/거부 후 재개 명령\n",
    "- **💡 Human Approval**: 승인 워크플로우(검토 → 결정 → 재개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "@tool\n",
    "def human_assistance(query: str) -> str:\n",
    "    \"\"\"Request assistance from an expert(human).\"\"\"\n",
    "    # interrupt를 호출하여 실행 일시 중지\n",
    "    # 사람의 응답을 기다림\n",
    "    human_response = interrupt({\"query\": query})\n",
    "    \n",
    "    # 사람의 응답 반환\n",
    "    return human_response[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🗺️ HITL 그래프 구성\n",
    "\n",
    "`human_assistance` 도구를 통해 승인 지점을 구현합니다.\n",
    "\n",
    "- **역할**: 필요 시 interrupt 로 중단 후 인간 답변 대기\n",
    "- **흐름**: chatbot → tools(`human_assistance`) → chatbot → END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# State 정의\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# 도구 리스트\n",
    "tools_with_human = [human_assistance]\n",
    "\n",
    "# 새로운 그래프 구성\n",
    "graph_builder_hitl = StateGraph(State)\n",
    "\n",
    "# LLM에 도구 바인딩\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "llm_with_human_tools = llm.bind_tools(tools_with_human)\n",
    "\n",
    "\n",
    "def chatbot_with_human(state: State):\n",
    "    \"\"\"Human Interruption 요청할 수 있는 챗봇\"\"\"\n",
    "    message = llm_with_human_tools.invoke(state[\"messages\"])\n",
    "    \n",
    "    # interrupt 중 병렬 도구 호출 방지\n",
    "    # (재개 시 도구 호출이 반복되는 것을 방지)\n",
    "    if hasattr(message, \"tool_calls\"):\n",
    "        assert (\n",
    "            len(message.tool_calls) <= 1\n",
    "        ), \"병렬 도구 호출은 interrupt와 함께 사용할 수 없습니다\"\n",
    "    \n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "\n",
    "# 노드 추가\n",
    "graph_builder_hitl.add_node(\"chatbot_with_human\", chatbot_with_human)\n",
    "\n",
    "# ToolNode 추가\n",
    "tool_node_hitl = ToolNode(tools=tools_with_human)\n",
    "graph_builder_hitl.add_node(\"tools\", tool_node_hitl)\n",
    "\n",
    "# 엣지 추가\n",
    "graph_builder_hitl.add_conditional_edges(\"chatbot_with_human\", tools_condition)\n",
    "graph_builder_hitl.add_edge(\"tools\", \"chatbot_with_human\")\n",
    "graph_builder_hitl.add_edge(START, \"chatbot_with_human\")\n",
    "\n",
    "# 메모리와 함께 컴파일\n",
    "memory_hitl = InMemorySaver()\n",
    "graph_hitl = graph_builder_hitl.compile(checkpointer=memory_hitl)\n",
    "\n",
    "print(\"✅ Human-in-the-Loop 그래프 생성 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎬 HITL 테스트\n",
    "\n",
    "사람에게 조언을 요청하는 질문으로 interrupt 와 재개 흐름을 검증합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# 인간 지원을 요청하는 메시지\n",
    "user_input = \"LangGraph가 뭐야? 사람한테 듣고 싶어.\"\n",
    "config_hitl = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "print(f\"User: {user_input}\\n\")\n",
    "\n",
    "# 실행 (interrupt에서 중단될 것임)\n",
    "try:\n",
    "    result = graph_hitl.invoke(\n",
    "        {\"messages\": [HumanMessage(content=user_input)]},\n",
    "        config=config_hitl,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"⏸️ 실행이 중단되었습니다: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 확인 - 어느 노드에서 중단되었는지 확인\n",
    "snapshot = graph_hitl.get_state(config_hitl)\n",
    "print(f\"\\n📊 현재 상태:\")\n",
    "print(f\"  다음 실행할 노드: {snapshot.next}\")\n",
    "print(f\"  체크포인트 ID: {snapshot.config['configurable']['checkpoint_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인간의 응답으로 실행 재개\n",
    "human_response = \"\"\"## 전문가의 조언:\n",
    "LangGraph는 LangChain 팀에서 개발한 프레임워크로, 상태 기반 AI 애플리케이션을 \n",
    "그래프 구조로 구현할 수 있게 해줍니다. 주요 특징:\n",
    "- 상태 관리와 체크포인트\n",
    "- 조건부 분기와 순환 구조\n",
    "- Human-in-the-Loop 지원\n",
    "- 도구 통합과 멀티 에이전트 시스템\n",
    "\"\"\"\n",
    "\n",
    "# Command 객체로 재개\n",
    "human_command = Command(resume={\"data\": human_response})\n",
    "\n",
    "print(f\"\\n💡 사람의 응답: {human_response[:100]}...\\n\")\n",
    "\n",
    "# 재개\n",
    "result = graph_hitl.invoke(human_command, config=config_hitl)\n",
    "print(f\"\\n[최종 응답]\\n{result['messages'][-1].content}\")"
   ]
  },
  {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
   "## 📚 Part 3-4 요약\n",
   "\n",
   "### Part 3: 메모리 추가\n",
   "1. **Checkpointer**: 대화 상태 영구 저장\n",
   "2. **Thread ID**: 세션별 대화 컨텍스트 관리\n",
   "3. **장기 기억**: `remember` 키워드로 중요 정보 저장\n",
   "4. **Store**: 사용자별 프로필 정보 관리\n",
   "\n",
   "### Part 4: Human-in-the-Loop\n",
   "1. **interrupt**: 실행 중단 및 인간 개입 대기\n",
   "2. **Command**: 승인/거부 후 재개 명령\n",
   "3. **도구 기반 승인**: `human_assistance` 도구로 구현\n",
   "4. **병렬 호출 제한**: interrupt 시 단일 도구만 호출\n",
   "\n",
   "### 실무 활용 시나리오\n",
   "- **고객 서비스 봇**: 고객 정보 기억 및 민감한 요청 승인\n",
   "- **의료 상담 봇**: 환자 이력 관리 및 처방 전 의사 승인\n",
   "- **금융 어시스턴트**: 거래 이력 저장 및 고액 거래 승인\n",
   "- **교육 도우미**: 학습 진도 추적 및 중요 결정 시 교사 승인\n",
   "\n",
   "### 주의사항\n",
   "- **개인정보 보호**: 민감한 정보는 암호화하여 저장\n",
   "- **승인 정책**: 명확한 승인 기준과 에스컬레이션 규칙 정의\n",
   "- **감사 추적**: 모든 승인/거부 이력 기록\n",
   "- **타임아웃 처리**: interrupt 상태에서 장시간 대기 시 처리 방안"
  ]
 }
],
"metadata": {
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": 3
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.11.0"
 }
},
"nbformat": 4,
"nbformat_minor": 4
}