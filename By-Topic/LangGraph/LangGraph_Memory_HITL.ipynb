{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: ë©”ëª¨ë¦¬ ì¶”ê°€ ğŸ’¾\n",
    "\n",
    "## ğŸ¯ ëª©ì \n",
    "ì„¸ì…˜ ê°„ì—ë„ ì‚¬ìš©ì ì •ë³´ë¥¼ ìœ ì§€í•˜ëŠ” **ì˜êµ¬ ìƒíƒœ ê´€ë¦¬**ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "### Before\n",
    "- ì„¸ì…˜ì´ ë°”ë€Œë©´ ì´ì „ ì •ë³´ ì†Œì‹¤\n",
    "\n",
    "### After\n",
    "- ì‚¬ìš©ìë³„ë¡œ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì €ì¥í•˜ê³  ì¬ì‚¬ìš©\n",
    "\n",
    "### í•µì‹¬ ê°œë…\n",
    "- **ğŸ’¾ Checkpointer**: ëŒ€í™” ìƒíƒœ ì €ì¥/ë³µì›\n",
    "- **ğŸ·ï¸ Thread ID**: ì„¸ì…˜ ì‹ë³„ì\n",
    "- **ğŸ—‚ï¸ Persistent State**: ëˆ„ì  ì´ë ¥ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "from typing import List, Optional\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë©”ëª¨ë¦¬ ì¶”ì¶œê¸° ì„¤ì •\n",
    "\n",
    "ëŒ€í™”ì—ì„œ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic ëª¨ë¸ ì •ì˜\n",
    "class MemoryItem(BaseModel):\n",
    "    \"\"\"ê°œë³„ ë©”ëª¨ë¦¬ ì•„ì´í…œ\"\"\"\n",
    "    key: str = Field(description=\"ë©”ëª¨ë¦¬ í‚¤ (ì˜ˆ: user_name, preference, fact)\")\n",
    "    value: str = Field(description=\"ë©”ëª¨ë¦¬ ê°’\")\n",
    "    category: str = Field(\n",
    "        description=\"ì¹´í…Œê³ ë¦¬ (personal_info, preference, interest, relationship, fact, etc.)\"\n",
    "    )\n",
    "    importance: int = Field(description=\"ì¤‘ìš”ë„ (1-5, 5ê°€ ê°€ì¥ ì¤‘ìš”)\", ge=1, le=5)\n",
    "    confidence: float = Field(description=\"ì¶”ì¶œ ì‹ ë¢°ë„ (0.0-1.0)\", ge=0.0, le=1.0)\n",
    "\n",
    "\n",
    "class ExtractedMemories(BaseModel):\n",
    "    \"\"\"ì¶”ì¶œëœ ë©”ëª¨ë¦¬ ì»¬ë ‰ì…˜\"\"\"\n",
    "    memories: List[MemoryItem] = Field(description=\"ì¶”ì¶œëœ ë©”ëª¨ë¦¬ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸\")\n",
    "    summary: str = Field(description=\"ëŒ€í™” ë‚´ìš© ìš”ì•½\")\n",
    "    timestamp: str = Field(\n",
    "        default_factory=lambda: datetime.now().isoformat(), \n",
    "        description=\"ì¶”ì¶œ ì‹œê°„\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_memory_extractor(model: str = \"gpt-4o\"):\n",
    "    \"\"\"ë©”ëª¨ë¦¬ ì¶”ì¶œê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    # Output Parser ìƒì„±\n",
    "    memory_parser = PydanticOutputParser(pydantic_object=ExtractedMemories)\n",
    "    \n",
    "    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
    "    system_prompt = \"\"\"You are an expert memory extraction assistant. \n",
    "    Extract important information from user conversations and convert them into structured key-value pairs.\n",
    "    \n",
    "    Extract ALL relevant information including:\n",
    "    - Personal information (name, age, location, occupation, etc.)\n",
    "    - Preferences and interests\n",
    "    - Important facts or events mentioned\n",
    "    - Goals and aspirations\"\"\"\n",
    "    \n",
    "    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "    template = f\"\"\"{system_prompt}\n",
    "    \n",
    "    User Input: {{input}}\n",
    "    \n",
    "    {{format_instructions}}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        template,\n",
    "        partial_variables={\n",
    "            \"format_instructions\": memory_parser.get_format_instructions()\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # ëª¨ë¸ ì„¤ì •\n",
    "    llm = ChatOpenAI(model=model, temperature=0)\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì¶”ì¶œ ì²´ì¸ ìƒì„±\n",
    "    memory_extractor = prompt | llm | memory_parser\n",
    "    \n",
    "    return memory_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì´ ìˆëŠ” ê·¸ë˜í”„ êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.store.base import BaseStore\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "memory_extractor = create_memory_extractor(model=\"gpt-4o\")\n",
    "\n",
    "\n",
    "def call_model(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    *,\n",
    "    store: BaseStore,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"LLM ëª¨ë¸ì„ í˜¸ì¶œí•˜ê³  ì‚¬ìš©ì ë©”ëª¨ë¦¬ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        state: í˜„ì¬ ìƒíƒœ (ë©”ì‹œì§€ í¬í•¨)\n",
    "        config: ì‹¤í–‰ ì„¤ì •\n",
    "        store: ë©”ëª¨ë¦¬ ì €ì¥ì†Œ\n",
    "    \"\"\"\n",
    "    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ì—ì„œ user_id ì¶”ì¶œ\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memories\", user_id)\n",
    "    \n",
    "    # ìœ ì €ì˜ ë©”ëª¨ë¦¬ ê²€ìƒ‰\n",
    "    memories = store.search(namespace, query=str(state[\"messages\"][-1].content))\n",
    "    info = \"\\n\".join([f\"{memory.key}: {memory.value}\" for memory in memories])\n",
    "    system_msg = f\"You are a helpful assistant. User info: {info}\" if info else \"You are a helpful assistant.\"\n",
    "    \n",
    "    # ì‚¬ìš©ìê°€ ê¸°ì–µ ìš”ì²­ ì‹œ ë©”ëª¨ë¦¬ ì €ì¥\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if \"remember\" in last_message.content.lower():\n",
    "        result = memory_extractor.invoke({\"input\": str(state[\"messages\"][-1].content)})\n",
    "        for memory in result.memories:\n",
    "            print(f\"ğŸ’¾ ì €ì¥: {memory.key} = {memory.value}\")\n",
    "            store.put(namespace, str(uuid.uuid4()), {memory.key: memory.value})\n",
    "    \n",
    "    # LLM í˜¸ì¶œ\n",
    "    response = model.invoke(\n",
    "        [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n",
    "    )\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# ê·¸ë˜í”„ ë¹Œë“œ\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„° ìƒì„±\n",
    "# ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” PostgresSaver ì‚¬ìš© ê¶Œì¥\n",
    "memory_saver = InMemorySaver()\n",
    "memory_store = InMemoryStore()\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph_with_memory = builder.compile(\n",
    "    checkpointer=memory_saver,\n",
    "    store=memory_store,\n",
    ")\n",
    "\n",
    "print(\"âœ… ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì´ ìˆëŠ” ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_graph(msg, thread_id=\"default\", user_id=\"default\"):\n",
    "    \"\"\"ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•˜ëŠ” í—¬í¼ í•¨ìˆ˜\"\"\"\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": thread_id + user_id,\n",
    "            \"user_id\": user_id,\n",
    "        }\n",
    "    }\n",
    "    print(f\"\\n[ìœ ì €ğŸ™‹] {msg}\")\n",
    "    \n",
    "    result = graph_with_memory.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": msg}]},\n",
    "        config=config,\n",
    "    )\n",
    "    \n",
    "    print(f\"[AIğŸ¤–] {result['messages'][-1].content}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬\n",
    "run_graph(\"ì•ˆë…•? ë‚´ ì´ë¦„ì€ í…Œë””ì•¼\", \"1\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°™ì€ threadì—ì„œ ëŒ€í™” ê³„ì†\n",
    "run_graph(\"ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?\", \"1\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ë¥¸ threadì—ì„œëŠ” ê¸°ì–µí•˜ì§€ ëª»í•¨\n",
    "run_graph(\"ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?\", \"2\", \"someone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§  ì¥ê¸° ê¸°ì–µ ì €ì¥: `remember` í‚¤ì›Œë“œ\n",
    "\n",
    "ë©”ì‹œì§€ì— `remember` ê°€ í¬í•¨ë˜ë©´ ì¤‘ìš” ì •ë³´ë¥¼ ì¥ê¸° ì €ì¥ì†Œì— ê¸°ë¡í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember í‚¤ì›Œë“œë¡œ ì¥ê¸° ê¸°ì–µì— ì €ì¥\n",
    "run_graph(\"ë‚´ ì´ë¦„ì´ í…Œë””ì•¼ remember\", \"2\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶”ê°€ ì •ë³´ ì €ì¥\n",
    "run_graph(\n",
    "    \"ë‚´ ì§ì—…ì€ AI Engineer ì•¼. ë‚´ ì·¨ë¯¸ëŠ” Netflix ë³´ê¸° ì•¼. remember\", \n",
    "    \"4\", \n",
    "    \"someone\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ë¥¸ ìŠ¤ë ˆë“œì—ì„œë„ ì¥ê¸° ê¸°ì–µì€ ìœ ì§€ë¨\n",
    "run_graph(\"ë‚´ ì´ë¦„, ì§ì—…, ì·¨ë¯¸ ì•Œë ¤ì¤˜\", \"100\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ë¥¸ user_idë¡œ ì‹¤í–‰í•œ ê²½ìš°ëŠ” ê¸°ì–µí•˜ì§€ ëª»í•¨\n",
    "run_graph(\"ë‚´ ì´ë¦„, ì§ì—…, ì·¨ë¯¸ ì•Œë ¤ì¤˜\", \"100\", \"other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š State í™•ì¸\n",
    "\n",
    "í˜„ì¬ ì €ì¥ëœ ìƒíƒœë¥¼ ì¡°íšŒí•´ ë©”ì‹œì§€ ì´ë ¥ê³¼ ì²´í¬í¬ì¸íŠ¸ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ì˜ì˜ Config ì„¤ì •\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"100\" + \"someone\",\n",
    "        \"user_id\": \"someone\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°\n",
    "snapshot = graph_with_memory.get_state(config)\n",
    "\n",
    "print(\"ğŸ“Š í˜„ì¬ ìƒíƒœ ì •ë³´:\")\n",
    "print(f\"- ë©”ì‹œì§€ ìˆ˜: {len(snapshot.values['messages'])}ê°œ\")\n",
    "print(f\"- ì²´í¬í¬ì¸íŠ¸ ID: {snapshot.config['configurable']['checkpoint_id']}\")\n",
    "\n",
    "# ìµœê·¼ ë©”ì‹œì§€ ëª‡ ê°œ í‘œì‹œ\n",
    "print(\"\\n[ìµœê·¼ ë©”ì‹œì§€]\")\n",
    "for msg in snapshot.values[\"messages\"][-3:]:\n",
    "    role = msg.type if hasattr(msg, \"type\") else \"unknown\"\n",
    "    content = msg.content if hasattr(msg, \"content\") else str(msg)\n",
    "    print(f\"  [{role}]: {content[:100]}...\" if len(content) > 100 else f\"  [{role}]: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Human-in-the-Loop ğŸ™‹\n",
    "\n",
    "## ğŸ¯ ëª©ì \n",
    "ê³ ìœ„í—˜/ì¤‘ìš” ì‘ì—…ì— ëŒ€í•´ AIê°€ ìŠ¤ìŠ¤ë¡œ ë©ˆì¶”ê³  ì¸ê°„ ìŠ¹ì¸ì„ ìš”ì²­í•˜ëŠ” ìŠ¹ì¸ ê¸°ë°˜ íë¦„ì„ ë„ì…í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì–¸ì œ ìŠ¹ì¸ì´ í•„ìš”í•œê°€\n",
    "- **ğŸ’° ê¸ˆìœµ ì²˜ë¦¬**: ê²°ì œ/ì´ì²´/íˆ¬ì\n",
    "- **ğŸ¥ ì˜ë£Œ ì¡°ì–¸**: ì²˜ë°©/ì¹˜ë£Œ ê¶Œê³ \n",
    "- **ğŸ“§ ëŒ€ì™¸ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜**: ê³µì§€/ë°œì†¡\n",
    "- **ğŸ” ë³´ì•ˆ ë³€ê²½**: ê¶Œí•œ/ì„¤ì •\n",
    "\n",
    "### í•µì‹¬ ê°œë…\n",
    "- **â¸ï¸ interrupt**: ì‹¤í–‰ ì¼ì‹œì •ì§€ ë° ìŠ¹ì¸ ëŒ€ê¸°\n",
    "- **ğŸ“‹ Command**: ìŠ¹ì¸/ê±°ë¶€ í›„ ì¬ê°œ ëª…ë ¹\n",
    "- **ğŸ’¡ Human Approval**: ìŠ¹ì¸ ì›Œí¬í”Œë¡œìš°(ê²€í†  â†’ ê²°ì • â†’ ì¬ê°œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "@tool\n",
    "def human_assistance(query: str) -> str:\n",
    "    \"\"\"Request assistance from an expert(human).\"\"\"\n",
    "    # interruptë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤í–‰ ì¼ì‹œ ì¤‘ì§€\n",
    "    # ì‚¬ëŒì˜ ì‘ë‹µì„ ê¸°ë‹¤ë¦¼\n",
    "    human_response = interrupt({\"query\": query})\n",
    "    \n",
    "    # ì‚¬ëŒì˜ ì‘ë‹µ ë°˜í™˜\n",
    "    return human_response[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ—ºï¸ HITL ê·¸ë˜í”„ êµ¬ì„±\n",
    "\n",
    "`human_assistance` ë„êµ¬ë¥¼ í†µí•´ ìŠ¹ì¸ ì§€ì ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "- **ì—­í• **: í•„ìš” ì‹œ interrupt ë¡œ ì¤‘ë‹¨ í›„ ì¸ê°„ ë‹µë³€ ëŒ€ê¸°\n",
    "- **íë¦„**: chatbot â†’ tools(`human_assistance`) â†’ chatbot â†’ END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# State ì •ì˜\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# ë„êµ¬ ë¦¬ìŠ¤íŠ¸\n",
    "tools_with_human = [human_assistance]\n",
    "\n",
    "# ìƒˆë¡œìš´ ê·¸ë˜í”„ êµ¬ì„±\n",
    "graph_builder_hitl = StateGraph(State)\n",
    "\n",
    "# LLMì— ë„êµ¬ ë°”ì¸ë”©\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "llm_with_human_tools = llm.bind_tools(tools_with_human)\n",
    "\n",
    "\n",
    "def chatbot_with_human(state: State):\n",
    "    \"\"\"Human Interruption ìš”ì²­í•  ìˆ˜ ìˆëŠ” ì±—ë´‡\"\"\"\n",
    "    message = llm_with_human_tools.invoke(state[\"messages\"])\n",
    "    \n",
    "    # interrupt ì¤‘ ë³‘ë ¬ ë„êµ¬ í˜¸ì¶œ ë°©ì§€\n",
    "    # (ì¬ê°œ ì‹œ ë„êµ¬ í˜¸ì¶œì´ ë°˜ë³µë˜ëŠ” ê²ƒì„ ë°©ì§€)\n",
    "    if hasattr(message, \"tool_calls\"):\n",
    "        assert (\n",
    "            len(message.tool_calls) <= 1\n",
    "        ), \"ë³‘ë ¬ ë„êµ¬ í˜¸ì¶œì€ interruptì™€ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"\n",
    "    \n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "graph_builder_hitl.add_node(\"chatbot_with_human\", chatbot_with_human)\n",
    "\n",
    "# ToolNode ì¶”ê°€\n",
    "tool_node_hitl = ToolNode(tools=tools_with_human)\n",
    "graph_builder_hitl.add_node(\"tools\", tool_node_hitl)\n",
    "\n",
    "# ì—£ì§€ ì¶”ê°€\n",
    "graph_builder_hitl.add_conditional_edges(\"chatbot_with_human\", tools_condition)\n",
    "graph_builder_hitl.add_edge(\"tools\", \"chatbot_with_human\")\n",
    "graph_builder_hitl.add_edge(START, \"chatbot_with_human\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ì™€ í•¨ê»˜ ì»´íŒŒì¼\n",
    "memory_hitl = InMemorySaver()\n",
    "graph_hitl = graph_builder_hitl.compile(checkpointer=memory_hitl)\n",
    "\n",
    "print(\"âœ… Human-in-the-Loop ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¬ HITL í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ì‚¬ëŒì—ê²Œ ì¡°ì–¸ì„ ìš”ì²­í•˜ëŠ” ì§ˆë¬¸ìœ¼ë¡œ interrupt ì™€ ì¬ê°œ íë¦„ì„ ê²€ì¦í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# ì¸ê°„ ì§€ì›ì„ ìš”ì²­í•˜ëŠ” ë©”ì‹œì§€\n",
    "user_input = \"LangGraphê°€ ë­ì•¼? ì‚¬ëŒí•œí…Œ ë“£ê³  ì‹¶ì–´.\"\n",
    "config_hitl = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "print(f\"User: {user_input}\\n\")\n",
    "\n",
    "# ì‹¤í–‰ (interruptì—ì„œ ì¤‘ë‹¨ë  ê²ƒì„)\n",
    "try:\n",
    "    result = graph_hitl.invoke(\n",
    "        {\"messages\": [HumanMessage(content=user_input)]},\n",
    "        config=config_hitl,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"â¸ï¸ ì‹¤í–‰ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒíƒœ í™•ì¸ - ì–´ëŠ ë…¸ë“œì—ì„œ ì¤‘ë‹¨ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "snapshot = graph_hitl.get_state(config_hitl)\n",
    "print(f\"\\nğŸ“Š í˜„ì¬ ìƒíƒœ:\")\n",
    "print(f\"  ë‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œ: {snapshot.next}\")\n",
    "print(f\"  ì²´í¬í¬ì¸íŠ¸ ID: {snapshot.config['configurable']['checkpoint_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¸ê°„ì˜ ì‘ë‹µìœ¼ë¡œ ì‹¤í–‰ ì¬ê°œ\n",
    "human_response = \"\"\"## ì „ë¬¸ê°€ì˜ ì¡°ì–¸:\n",
    "LangGraphëŠ” LangChain íŒ€ì—ì„œ ê°œë°œí•œ í”„ë ˆì„ì›Œí¬ë¡œ, ìƒíƒœ ê¸°ë°˜ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ \n",
    "ê·¸ë˜í”„ êµ¬ì¡°ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ì£¼ìš” íŠ¹ì§•:\n",
    "- ìƒíƒœ ê´€ë¦¬ì™€ ì²´í¬í¬ì¸íŠ¸\n",
    "- ì¡°ê±´ë¶€ ë¶„ê¸°ì™€ ìˆœí™˜ êµ¬ì¡°\n",
    "- Human-in-the-Loop ì§€ì›\n",
    "- ë„êµ¬ í†µí•©ê³¼ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ\n",
    "\"\"\"\n",
    "\n",
    "# Command ê°ì²´ë¡œ ì¬ê°œ\n",
    "human_command = Command(resume={\"data\": human_response})\n",
    "\n",
    "print(f\"\\nğŸ’¡ ì‚¬ëŒì˜ ì‘ë‹µ: {human_response[:100]}...\\n\")\n",
    "\n",
    "# ì¬ê°œ\n",
    "result = graph_hitl.invoke(human_command, config=config_hitl)\n",
    "print(f\"\\n[ìµœì¢… ì‘ë‹µ]\\n{result['messages'][-1].content}\")"
   ]
  },
  {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
   "## ğŸ“š Part 3-4 ìš”ì•½\n",
   "\n",
   "### Part 3: ë©”ëª¨ë¦¬ ì¶”ê°€\n",
   "1. **Checkpointer**: ëŒ€í™” ìƒíƒœ ì˜êµ¬ ì €ì¥\n",
   "2. **Thread ID**: ì„¸ì…˜ë³„ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬\n",
   "3. **ì¥ê¸° ê¸°ì–µ**: `remember` í‚¤ì›Œë“œë¡œ ì¤‘ìš” ì •ë³´ ì €ì¥\n",
   "4. **Store**: ì‚¬ìš©ìë³„ í”„ë¡œí•„ ì •ë³´ ê´€ë¦¬\n",
   "\n",
   "### Part 4: Human-in-the-Loop\n",
   "1. **interrupt**: ì‹¤í–‰ ì¤‘ë‹¨ ë° ì¸ê°„ ê°œì… ëŒ€ê¸°\n",
   "2. **Command**: ìŠ¹ì¸/ê±°ë¶€ í›„ ì¬ê°œ ëª…ë ¹\n",
   "3. **ë„êµ¬ ê¸°ë°˜ ìŠ¹ì¸**: `human_assistance` ë„êµ¬ë¡œ êµ¬í˜„\n",
   "4. **ë³‘ë ¬ í˜¸ì¶œ ì œí•œ**: interrupt ì‹œ ë‹¨ì¼ ë„êµ¬ë§Œ í˜¸ì¶œ\n",
   "\n",
   "### ì‹¤ë¬´ í™œìš© ì‹œë‚˜ë¦¬ì˜¤\n",
   "- **ê³ ê° ì„œë¹„ìŠ¤ ë´‡**: ê³ ê° ì •ë³´ ê¸°ì–µ ë° ë¯¼ê°í•œ ìš”ì²­ ìŠ¹ì¸\n",
   "- **ì˜ë£Œ ìƒë‹´ ë´‡**: í™˜ì ì´ë ¥ ê´€ë¦¬ ë° ì²˜ë°© ì „ ì˜ì‚¬ ìŠ¹ì¸\n",
   "- **ê¸ˆìœµ ì–´ì‹œìŠ¤í„´íŠ¸**: ê±°ë˜ ì´ë ¥ ì €ì¥ ë° ê³ ì•¡ ê±°ë˜ ìŠ¹ì¸\n",
   "- **êµìœ¡ ë„ìš°ë¯¸**: í•™ìŠµ ì§„ë„ ì¶”ì  ë° ì¤‘ìš” ê²°ì • ì‹œ êµì‚¬ ìŠ¹ì¸\n",
   "\n",
   "### ì£¼ì˜ì‚¬í•­\n",
   "- **ê°œì¸ì •ë³´ ë³´í˜¸**: ë¯¼ê°í•œ ì •ë³´ëŠ” ì•”í˜¸í™”í•˜ì—¬ ì €ì¥\n",
   "- **ìŠ¹ì¸ ì •ì±…**: ëª…í™•í•œ ìŠ¹ì¸ ê¸°ì¤€ê³¼ ì—ìŠ¤ì»¬ë ˆì´ì…˜ ê·œì¹™ ì •ì˜\n",
   "- **ê°ì‚¬ ì¶”ì **: ëª¨ë“  ìŠ¹ì¸/ê±°ë¶€ ì´ë ¥ ê¸°ë¡\n",
   "- **íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬**: interrupt ìƒíƒœì—ì„œ ì¥ì‹œê°„ ëŒ€ê¸° ì‹œ ì²˜ë¦¬ ë°©ì•ˆ"
  ]
 }
],
"metadata": {
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": 3
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.11.0"
 }
},
"nbformat": 4,
"nbformat_minor": 4
}