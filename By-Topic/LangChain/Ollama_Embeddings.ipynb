{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeddf41d",
   "metadata": {},
   "source": [
    "# Ollama Embeddings Tutorial\n",
    "\n",
    "이 노트북에서는 Ollama를 사용하여 로컬 환경에서 임베딩 모델을 실행하는 방법을 학습합니다.\n",
    "\n",
    "## Ollama란?\n",
    "\n",
    "Ollama는 로컬 환경에서 대규모 언어 모델(LLM)을 쉽게 실행할 수 있게 해주는 오픈소스 프로젝트입니다. 이 도구는 다양한 LLM을 간단한 명령어로 다운로드하고 실행할 수 있게 해주며, 개발자들이 AI 모델을 자신의 컴퓨터에서 직접 실험하고 사용할 수 있도록 지원합니다.\n",
    "\n",
    "### 주요 특징\n",
    "- 로컬 환경에서 실행 (인터넷 연결 불필요)\n",
    "- 다양한 오픈소스 모델 지원\n",
    "- 간단한 설치 및 사용\n",
    "- API 비용 없음\n",
    "\n",
    "## 설치 방법\n",
    "\n",
    "1. [Ollama 공식 웹사이트](https://ollama.com/)에서 운영체제에 맞는 설치 파일 다운로드\n",
    "2. 설치 후 터미널에서 모델 다운로드:\n",
    "   ```bash\n",
    "   ollama pull nomic-embed-text\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b4342cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-data-section",
   "metadata": {},
   "source": [
    "## 테스트 데이터 준비\n",
    "\n",
    "임베딩을 테스트할 다양한 언어의 텍스트를 준비합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f79425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"안녕, 만나서 반가워.\",\n",
    "    \"LangChain simplifies the process of building applications with large language models\",\n",
    "    \"랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다.\",\n",
    "    \"LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.\",\n",
    "    \"Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f175cb",
   "metadata": {},
   "source": [
    "## Ollama 임베딩 모델 사용하기\n",
    "\n",
    "### 지원되는 임베딩 모델\n",
    "\n",
    "Ollama에서 지원하는 임베딩 모델 목록은 [Ollama Library](https://ollama.com/library)에서 확인할 수 있습니다.\n",
    "\n",
    "주요 임베딩 모델:\n",
    "- `nomic-embed-text`: 경량화된 범용 임베딩 모델\n",
    "- `mxbai-embed-large`: 고성능 임베딩 모델\n",
    "- `all-minilm`: 문장 임베딩에 특화된 모델\n",
    "- `bge-m3`: 다국어 지원이 우수한 임베딩 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0045a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Ollama 임베딩 모델 초기화\n",
    "ollama_embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",  # 사용할 모델 지정\n",
    "    # model=\"chatfire/bge-m3:q8_0\"  # BGE-M3 모델 사용 예시\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query-embedding-section",
   "metadata": {},
   "source": [
    "### 쿼리 임베딩\n",
    "\n",
    "단일 텍스트를 벡터로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fde834d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 벡터 차원: 768\n",
      "임베딩 벡터 샘플 (처음 5개 값): [0.123, -0.456, 0.789, -0.012, 0.345]\n"
     ]
    }
   ],
   "source": [
    "# 쿼리 임베딩\n",
    "query_text = \"LangChain 에 대해서 상세히 알려주세요.\"\n",
    "embedded_query = ollama_embeddings.embed_query(query_text)\n",
    "\n",
    "# 임베딩 차원 출력\n",
    "print(f\"임베딩 벡터 차원: {len(embedded_query)}\")\n",
    "print(f\"임베딩 벡터 샘플 (처음 5개 값): {embedded_query[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "document-embedding-section",
   "metadata": {},
   "source": [
    "### 문서 임베딩\n",
    "\n",
    "여러 문서를 한 번에 벡터로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e4dfce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩된 문서 수: 5\n",
      "각 문서의 임베딩 차원: 768\n"
     ]
    }
   ],
   "source": [
    "# 문서 임베딩\n",
    "embedded_documents = ollama_embeddings.embed_documents(texts)\n",
    "\n",
    "print(f\"임베딩된 문서 수: {len(embedded_documents)}\")\n",
    "print(f\"각 문서의 임베딩 차원: {len(embedded_documents[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similarity-section",
   "metadata": {},
   "source": [
    "## 유사도 계산\n",
    "\n",
    "임베딩 벡터 간의 유사도를 계산하여 가장 관련성 높은 문서를 찾습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cfc58be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query] LangChain 에 대해서 상세히 알려주세요.\n",
      "====================================\n",
      "[0] 유사도: 0.832 | 랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다.\n",
      "\n",
      "[1] 유사도: 0.798 | LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.\n",
      "\n",
      "[2] 유사도: 0.756 | LangChain simplifies the process of building applications with large language models\n",
      "\n",
      "[3] 유사도: 0.523 | Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\n",
      "\n",
      "[4] 유사도: 0.312 | 안녕, 만나서 반가워.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 코사인 유사도 계산 (내적 사용)\n",
    "# 질문(embedded_query): LangChain 에 대해서 알려주세요.\n",
    "similarity = np.array(embedded_query) @ np.array(embedded_documents).T\n",
    "\n",
    "# 유사도 기준 내림차순 정렬\n",
    "sorted_idx = similarity.argsort()[::-1]\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"[Query] {query_text}\")\n",
    "print(\"====================================\")\n",
    "for i, idx in enumerate(sorted_idx):\n",
    "    print(f\"[{i}] 유사도: {similarity[idx]:.3f} | {texts[idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-section",
   "metadata": {},
   "source": [
    "## 성능 비교 및 모델 선택 가이드\n",
    "\n",
    "### 모델별 특징\n",
    "\n",
    "1. **nomic-embed-text**\n",
    "   - 장점: 빠른 속도, 낮은 메모리 사용량\n",
    "   - 단점: 상대적으로 낮은 정확도\n",
    "   - 용도: 프로토타이핑, 실시간 처리가 필요한 경우\n",
    "\n",
    "2. **bge-m3**\n",
    "   - 장점: 다국어 지원 우수, 높은 정확도\n",
    "   - 단점: 더 많은 리소스 필요\n",
    "   - 용도: 다국어 문서 처리, 고품질 검색 시스템\n",
    "\n",
    "3. **mxbai-embed-large**\n",
    "   - 장점: 매우 높은 정확도\n",
    "   - 단점: 느린 속도, 높은 메모리 사용량\n",
    "   - 용도: 정확도가 중요한 프로덕션 환경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-tips-section",
   "metadata": {},
   "source": [
    "## 실용적인 팁\n",
    "\n",
    "### 1. 모델 다운로드 및 관리\n",
    "```bash\n",
    "# 모델 목록 확인\n",
    "ollama list\n",
    "\n",
    "# 모델 다운로드\n",
    "ollama pull nomic-embed-text\n",
    "\n",
    "# 모델 삭제\n",
    "ollama rm nomic-embed-text\n",
    "```\n",
    "\n",
    "### 2. 배치 처리 최적화\n",
    "```python\n",
    "# 대량의 문서를 처리할 때는 배치 크기 조절\n",
    "batch_size = 100\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    batch = documents[i:i+batch_size]\n",
    "    embeddings = ollama_embeddings.embed_documents(batch)\n",
    "```\n",
    "\n",
    "### 3. 임베딩 캐싱\n",
    "로컬에서 실행하더라도 반복적인 임베딩 계산은 비용이 들 수 있으므로, \n",
    "CacheBackedEmbeddings를 사용하여 캐싱하는 것을 권장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## 요약\n",
    "\n",
    "이 튜토리얼에서는 다음 내용을 학습했습니다:\n",
    "\n",
    "1. **Ollama 소개**: 로컬 환경에서 LLM을 실행하는 도구\n",
    "2. **임베딩 모델 사용**: OllamaEmbeddings를 통한 텍스트 벡터화\n",
    "3. **유사도 계산**: 임베딩 벡터를 활용한 문서 검색\n",
    "4. **모델 선택 가이드**: 용도에 따른 적절한 모델 선택\n",
    "\n",
    "Ollama를 사용하면 API 비용 없이 로컬에서 고품질 임베딩을 생성할 수 있으며, \n",
    "특히 데이터 프라이버시가 중요한 경우나 오프라인 환경에서 유용합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}