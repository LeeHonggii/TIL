{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaParser로 고급 문서 파싱\n",
    " \n",
    "LlamaParse는 LlamaIndex에서 개발한 문서 파싱 서비스로, 대규모 언어 모델(LLM)을 위해 특별히 설계되었습니다. 주요 특징은 다음과 같습니다:\n",
    "\n",
    "- PDF, Word, PowerPoint, Excel 등 다양한 문서 형식 지원\n",
    "- 자연어 지시를 통한 맞춤형 출력 형식 제공\n",
    "- 복잡한 표와 이미지 추출 기능\n",
    "- JSON 모드 지원\n",
    "- 외국어 지원\n",
    "\n",
    "LlamaParse는 독립형 API로 제공되며, LlamaCloud 플랫폼의 일부로도 사용 가능합니다. 이 서비스는 문서를 파싱하고 정제하여 검색 증강 생성(RAG) 등 LLM 기반 애플리케이션의 성능을 향상시키는 것을 목표로 합니다.\n",
    "\n",
    "사용자는 무료로 하루 1,000페이지를 처리할 수 있으며, 유료 플랜을 통해 추가 용량을 확보할 수 있습니다. LlamaParse는 현재 공개 베타 버전으로 제공되고 있으며, 지속적으로 기능이 확장되고 있습니다.\n",
    "\n",
    "- 링크: https://cloud.llamaindex.ai\n",
    "\n",
    "**API 키 설정**\n",
    "- API 키를 발급 후 `.env` 파일에 `LLAMA_CLOUD_API_KEY` 에 설정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설치\n",
    "\n",
    "LlamaParse를 사용하기 위해 필요한 패키지들을 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설치\n",
    "# !pip install llama-index-core llama-parse llama-index-readers-file python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 비동기 처리를 위한 설정\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 파서 사용법\n",
    "\n",
    "LlamaParse의 기본 사용법을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 3edeadb0-31d2-4b1d-b5ff-4b0bd91aac61\n"
     ]
    }
   ],
   "source": [
    "#기본 파서 적용\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# 파서 설정\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",  # \"markdown\"과 \"text\" 사용 가능\n",
    "    num_workers=8,  # worker 수 (기본값: 4)\n",
    "    verbose=True,\n",
    "    language=\"ko\",\n",
    ")\n",
    "\n",
    "# SimpleDirectoryReader를 사용하여 파일 파싱\n",
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "# LlamaParse로 파일 파싱\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"../ADsP.pdf\"],\n",
    "    file_extractor=file_extractor,\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 페이지 수 확인\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='32c602f9-0883-4b76-bcb6-2a5f18f31f57', embedding=None, metadata={'file_path': '..\\\\ADsP.pdf', 'file_name': 'ADsP.pdf', 'file_type': 'application/pdf', 'file_size': 73587, 'creation_date': '2025-03-03', 'last_modified_date': '2024-11-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# ADsP 요약정리 및 오답노트\\n\\n# -1과목-\\n\\n# (객관식)\\n\\n데이터 마스킹 : 데이터의 속성은 유지한 채, 익명으로 생성\\n\\nCinematch -> 넷플릭스에서 개발한 알고리즘\\n\\n데이터마이닝 vs 머신러닝(딥러닝) 구분하기 다른거임\\n\\n트레이딩, 공급, 수요예측 -> 에너지 산업\\n\\nCRM -> 고객관계관리 데이터베이스 (기업내부)\\n\\nERP -> 기업 전체를 통합적으로 관리하고 경영의 효율화 목적\\n\\n빅데이터 가치측정 어려윤 이유 :\\n\\n1. 데이터 재사용, 재조합, 다목적용 개발\\n2. 새로운 가치 창출\\n3. 분석 기술 발전\\n\\ncf. 전문인력증가는 가치측정과 관련 없음\\n\\n사생활 침해를 막기 위한 개인정보 무작위 처리 (본래 목적 외에 사용 방지기술) -> 난수화\\n\\n유형분석 -> 특성에따라 분류할때 사용한다.\\n\\n핀테크(금융)분야에서 빅데이터 활용의 핵심분야 -> 신용평가\\n\\nK-NN 분석기법 - 딥러닝과 관련 x\\n\\nLSTM, Autoencoder, RNN -> 딥러닝과 관련 o\\n\\nCaffe, Tnsorflow, Theano -> 딥러닝 관련된 오픈소스\\n\\nAnaconda -> 머신러닝 관련된 오픈소스\\n\\n책임 원칙의 훼손 -> 일어나지도 않은 일을 예측해서 행동함 (범행 전에 체포, 신용불량 전에 대출금지)\\n\\n멀티미디어 등 복잡한 데이터베이스 관리 -> 객체지향 DBMS\\n\\ncf. 일반적으로 사용되는 테이블 기반 -> 관계형 DBMS\\n\\n데이터 시각화 -> 비즈니스 컨설팅 영역 (IT영역 X)', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파싱된 문서 확인\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Document로 변환\n",
    "\n",
    "LlamaIndex Document를 LangChain Document로 변환할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LlamaIndex -> LangChain Document 로 변환기제공\n",
    "docs = [doc.to_langchain_format() for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ADsP 요약정리 및 오답노트\n",
      "\n",
      "# -1과목-\n",
      "\n",
      "# (객관식)\n",
      "\n",
      "데이터 마스킹 : 데이터의 속성은 유지한 채, 익명으로 생성\n",
      "\n",
      "Cinematch -> 넷플릭스에서 개발한 알고리즘\n",
      "\n",
      "데이터마이닝 vs 머신러닝(딥러닝) 구분하기 다른거임\n",
      "\n",
      "트레이딩, 공급, 수요예측 -> 에너지 산업\n",
      "\n",
      "CRM -> 고객관계관리 데이터베이스 (기업내부)\n",
      "\n",
      "ERP -> 기업 전체를 통합적으로 관리하고 경영의 효율화 목적\n",
      "\n",
      "빅데이터 가치측정 어려윤 이유 :\n",
      "\n",
      "1. 데이터 재사용, 재조합, 다목적용 개발\n",
      "2. 새로운 가치 창출\n",
      "3. 분석 기술 발전\n",
      "\n",
      "cf. 전문인력증가는 가치측정과 관련 없음\n",
      "\n",
      "사생활 침해를 막기 위한 개인정보 무작위 처리 (본래 목적 외에 사용 방지기술) -> 난수화\n",
      "\n",
      "유형분석 -> 특성에따라 분류할때 사용한다.\n",
      "\n",
      "핀테크(금융)분야에서 빅데이터 활용의 핵심분야 -> 신용평가\n",
      "\n",
      "K-NN 분석기법 - 딥러닝과 관련 x\n",
      "\n",
      "LSTM, Autoencoder, RNN -> 딥러닝과 관련 o\n",
      "\n",
      "Caffe, Tnsorflow, Theano -> 딥러닝 관련된 오픈소스\n",
      "\n",
      "Anaconda -> 머신러닝 관련된 오픈소스\n",
      "\n",
      "책임 원칙의 훼손 -> 일어나지도 않은 일을 예측해서 행동함 (범행 전에 체포, 신용불량 전에 대출금지)\n",
      "\n",
      "멀티미디어 등 복잡한 데이터베이스 관리 -> 객체지향 DBMS\n",
      "\n",
      "cf. 일반적으로 사용되는 테이블 기반 -> 관계형 DBMS\n",
      "\n",
      "데이터 시각화 -> 비즈니스 컨설팅 영역 (IT영역 X)\n"
     ]
    }
   ],
   "source": [
    "# 페이지 내용 출력\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': '..\\\\ADsP.pdf',\n",
       " 'file_name': 'ADsP.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 73587,\n",
       " 'creation_date': '2025-03-03',\n",
       " 'last_modified_date': '2024-11-06'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metadata 출력\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiModal Model로 파싱\n",
    "\n",
    "LlamaParse는 멀티모달 모델을 활용하여 더 정확한 문서 파싱이 가능합니다.\n",
    "\n",
    "**주요 파라미터**\n",
    "\n",
    "- `use_vendor_multimodal_model`: 멀티모달 모델 사용 여부를 지정합니다. `True`로 설정하면 외부 벤더의 멀티모달 모델을 사용합니다.\n",
    "\n",
    "- `vendor_multimodal_model_name`: 사용할 멀티모달 모델의 이름을 지정합니다. 여기서는 \"openai-gpt4o\"를 사용하고 있습니다.\n",
    "\n",
    "- `vendor_multimodal_api_key`: 멀티모달 모델 API 키를 지정합니다. 환경 변수에서 OpenAI API 키를 가져옵니다.\n",
    "\n",
    "- `result_type`: 파싱 결과의 형식을 지정합니다. \"markdown\"으로 설정되어 있어 결과가 마크다운 형식으로 반환됩니다.\n",
    "\n",
    "- `language`: 파싱할 문서의 언어를 지정합니다. \"ko\"로 설정되어 한국어로 처리됩니다.\n",
    "\n",
    "- `skip_diagonal_text`: 대각선 텍스트를 건너뛸지 여부를 결정합니다.\n",
    "\n",
    "- `page_separator`: 페이지 구분자를 지정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티모달 모델을 사용한 파서 설정\n",
    "documents = LlamaParse(\n",
    "    use_vendor_multimodal_model=True,\n",
    "    vendor_multimodal_model_name=\"openai-gpt4o\",\n",
    "    vendor_multimodal_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    result_type=\"markdown\",\n",
    "    language=\"ko\",\n",
    "    # skip_diagonal_text=True,\n",
    "    # page_separator=\"\\n=================\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 770c66d6-ece1-4e9e-ab61-b9d2cb0e0b5a\n"
     ]
    }
   ],
   "source": [
    "# parsing 된 결과\n",
    "parsed_docs = documents.load_data(file_path=\"../ADsP.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain 도큐먼트로 변환\n",
    "docs = [doc.to_langchain_format() for doc in parsed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# <ADsP 요약정리 및 오답노트>\n",
      "\n",
      "## -1과목-\n",
      "\n",
      "### (객관식)\n",
      "\n",
      "- 데이터 마스킹 : 데이터의 속성은 유지한 채, 익명으로 생성\n",
      "\n",
      "- Cinematch -> 넷플릭스에서 개발한 알고리즘\n",
      "\n",
      "- 데이터마이닝 vs 머신러닝(딥러닝) 구분하기 다른개임\n",
      "\n",
      "- 트레이딩, 공급, 수요예측 -> 에너지 산업\n",
      "\n",
      "- CRM -> 고객관계관리 데이터베이스 (기업내부)\n",
      "\n",
      "- ERP -> 기업 전체를 통합적으로 관리하고 경영의 효율화 목적\n",
      "\n",
      "- 빅데이터 가치측정 어려운 이유 : \n",
      "  1. 데이터 재사용, 재조합, 다목적용 개발\n",
      "  2. 새로운 가치 창출\n",
      "  3. 분석 기술 발전\n",
      "\n",
      "  cf. 전문인력증가는 가치측정과 관련 없음\n",
      "\n",
      "- 사생활 침해를 막기 위한 개인정보 무작위 처리 (본래 목적 외에 사용 방지기술) -> 난수화\n",
      "\n",
      "- 유형분석 -> 특정에 따라 분류할때 사용한다.\n",
      "\n",
      "- 핀테크(금융분야)에서 빅데이터 활용의 핵심분야 -> 신용평가\n",
      "\n",
      "- K-NN 분석기법 - 딥러닝과 관련 X  \n",
      "  LSTM, Autoencoder, RNN -> 딥러닝과 관련 O\n",
      "\n",
      "- Caffe, Tnsorflow, Theano -> 딥러닝 관련된 오픈소스  \n",
      "  Anaconda -> 머신러닝 관련된 오픈소스\n",
      "\n",
      "- 책임 원칙의 훼손 -> 일어나지도 않은 일을 예측해서 행동함 (범행 전에 체포, 신용불량 전에 대출금지)\n",
      "\n",
      "- 멀티미디어 등 복잡한 데이터베이스 관리 -> 객체지향 DBMS  \n",
      "  cf. 일반적으로 사용되는 테이블 기반 -> 관계형 DBMS\n",
      "\n",
      "- 데이터 시작 -> 비즈니스 컨설팅 영역 (IT영역 X)\n"
     ]
    }
   ],
   "source": [
    "# 멀티모달 모델로 파싱된 결과 확인\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파싱 지시사항(Parsing Instruction) 활용\n",
    "\n",
    "특정 형식으로 문서를 파싱하도록 지시할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 7dc3e0a4-37b4-42cb-9b50-1ed76fe55e82\n"
     ]
    }
   ],
   "source": [
    "# parsing instruction 을 지정합니다.\n",
    "parsing_instruction = (\n",
    "    \"You are parsing a brief of ADsP Study note. Please extract tables in markdown format.\"\n",
    ")\n",
    "\n",
    "# LlamaParse 설정\n",
    "parser = LlamaParse(\n",
    "    use_vendor_multimodal_model=True,\n",
    "    vendor_multimodal_model_name=\"openai-gpt4o\",\n",
    "    vendor_multimodal_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    result_type=\"markdown\",\n",
    "    language=\"ko\",\n",
    "    parsing_instruction=parsing_instruction,\n",
    ")\n",
    "\n",
    "# parsing 된 결과\n",
    "parsed_docs = parser.load_data(file_path=\"../ADsP.pdf\")\n",
    "\n",
    "# langchain 도큐먼트로 변환\n",
    "docs = [doc.to_langchain_format() for doc in parsed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm unable to provide a detailed summary of the image, but I can help with specific questions or topics related to ADsP (Advanced Data Science Professional) study notes. Let me know how I can assist you!\n"
     ]
    }
   ],
   "source": [
    "# markdown 형식으로 추출된 테이블 확인\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다양한 파싱 옵션 활용\n",
    "\n",
    "### 1. JSON 모드로 파싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 형식으로 파싱\n",
    "json_parser = LlamaParse(\n",
    "    result_type=\"json\",  # JSON 형식으로 결과 반환\n",
    "    language=\"ko\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 파싱 실행\n",
    "json_docs = json_parser.load_data(file_path=\"../ADsP.pdf\")\n",
    "print(\"JSON 형식 결과:\")\n",
    "print(json_docs[0].text[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 페이지별 분할 파싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 페이지 구분자를 사용한 파싱\n",
    "page_parser = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    language=\"ko\",\n",
    "    page_separator=\"\\n\\n========== 페이지 구분선 ==========\\n\\n\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 여러 페이지 문서 파싱 시 유용\n",
    "page_docs = page_parser.load_data(file_path=\"../multi_page_document.pdf\")\n",
    "\n",
    "# 페이지별로 분리된 내용 확인\n",
    "pages = page_docs[0].text.split(\"========== 페이지 구분선 ==========\")\n",
    "print(f\"총 {len(pages)}개의 페이지로 분리됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 커스텀 파싱 전략"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 섹션만 추출하는 파싱 지시\n",
    "section_instruction = \"\"\"\n",
    "Extract only the following sections from the document:\n",
    "1. Definitions and key terms\n",
    "2. Important formulas or equations\n",
    "3. Summary points\n",
    "Format the output with clear headers for each section.\n",
    "\"\"\"\n",
    "\n",
    "section_parser = LlamaParse(\n",
    "    use_vendor_multimodal_model=True,\n",
    "    vendor_multimodal_model_name=\"openai-gpt4o\",\n",
    "    vendor_multimodal_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    result_type=\"markdown\",\n",
    "    language=\"ko\",\n",
    "    parsing_instruction=section_instruction,\n",
    ")\n",
    "\n",
    "section_docs = section_parser.load_data(file_path=\"../ADsP.pdf\")\n",
    "print(\"특정 섹션 추출 결과:\")\n",
    "print(section_docs[0].text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaParse와 RAG 통합\n",
    "\n",
    "파싱된 문서를 RAG 시스템에 활용하는 예제입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 텍스트 분할기 설정\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "# 문서 분할\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 임베딩 및 벡터 저장소 생성\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "\n",
    "# 검색 테스트\n",
    "query = \"빅데이터 가치측정이 어려운 이유\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"검색 쿼리: {query}\\n\")\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"결과 {i+1}:\")\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 고급 활용 팁\n",
    "\n",
    "### 1. 에러 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List\n",
    "\n",
    "def parse_with_retry(file_path: str, max_retries: int = 3) -> List:\n",
    "    \"\"\"\n",
    "    재시도 로직을 포함한 파싱 함수\n",
    "    \"\"\"\n",
    "    parser = LlamaParse(\n",
    "        result_type=\"markdown\",\n",
    "        language=\"ko\",\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"파싱 시도 {attempt + 1}/{max_retries}...\")\n",
    "            docs = parser.load_data(file_path=file_path)\n",
    "            print(\"파싱 성공!\")\n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            print(f\"파싱 실패: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt  # 지수 백오프\n",
    "                print(f\"{wait_time}초 후 재시도...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    return []\n",
    "\n",
    "# 사용 예시\n",
    "# docs = parse_with_retry(\"../problematic_document.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 배치 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def batch_parse_documents(directory: str, file_pattern: str = \"*.pdf\"):\n",
    "    \"\"\"\n",
    "    디렉토리 내 여러 문서를 일괄 파싱\n",
    "    \"\"\"\n",
    "    parser = LlamaParse(\n",
    "        result_type=\"markdown\",\n",
    "        language=\"ko\",\n",
    "        num_workers=8,  # 병렬 처리\n",
    "    )\n",
    "    \n",
    "    all_docs = []\n",
    "    pdf_files = list(Path(directory).glob(file_pattern))\n",
    "    \n",
    "    print(f\"{len(pdf_files)}개의 파일을 찾았습니다.\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\n파싱 중: {pdf_file.name}\")\n",
    "        try:\n",
    "            docs = parser.load_data(file_path=str(pdf_file))\n",
    "            # 파일명을 메타데이터에 추가\n",
    "            for doc in docs:\n",
    "                doc.metadata[\"source_file\"] = pdf_file.name\n",
    "            all_docs.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"에러 발생: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return all_docs\n",
    "\n",
    "# 사용 예시\n",
    "# all_documents = batch_parse_documents(\"../documents/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약\n",
    "\n",
    "LlamaParse는 고급 문서 파싱을 위한 강력한 도구입니다:\n",
    "\n",
    "1. **다양한 문서 형식 지원**: PDF, Word, PowerPoint, Excel 등\n",
    "2. **멀티모달 모델 활용**: GPT-4 등의 비전 모델을 활용한 정확한 문서 분석\n",
    "3. **유연한 출력 형식**: Markdown, Text, JSON 등 다양한 형식으로 출력 가능\n",
    "4. **맞춤형 파싱 지시**: 자연어 지시를 통한 특정 형식 추출\n",
    "5. **LangChain 통합**: LlamaIndex Document를 LangChain Document로 쉽게 변환\n",
    "\n",
    "### 주요 활용 사례\n",
    "- **RAG 시스템**: 복잡한 문서를 정확하게 파싱하여 검색 품질 향상\n",
    "- **표 추출**: 문서 내 표를 구조화된 형식으로 추출\n",
    "- **다국어 문서 처리**: 한국어 등 다양한 언어 지원\n",
    "- **이미지 기반 문서**: 스캔된 문서나 이미지 내 텍스트 추출\n",
    "\n",
    "LlamaParse는 기존의 단순한 텍스트 추출을 넘어 문서의 구조와 맥락을 이해하는 지능형 파싱을 제공합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 }
}