{
"cells": [
 {
  "cell_type": "markdown",
  "id": "title-cell",
  "metadata": {},
  "source": [
   "# ContextualCompressionRetriever로 검색 결과 압축하기\n",
   "\n",
   "이 튜토리얼에서는 LangChain의 `ContextualCompressionRetriever`를 사용하여 검색된 문서에서 질의와 관련된 정보만을 추출하는 방법을 학습합니다.\n",
   "\n",
   "## 목차\n",
   "1. [개요](#개요)\n",
   "2. [기본 Retriever 설정](#기본-retriever-설정)\n",
   "3. [맥락적 압축 (ContextualCompression)](#맥락적-압축-contextualcompression)\n",
   "4. [LLM을 활용한 문서 필터링](#llm을-활용한-문서-필터링)\n",
   "5. [파이프라인 생성](#파이프라인-생성압축기문서-변환기)\n",
   "6. [요약](#요약)"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "overview-cell",
  "metadata": {},
  "source": [
   "## 개요\n",
   "\n",
   "### 문맥 압축 검색기가 필요한 이유\n",
   "\n",
   "검색 시스템에서 직면하는 주요 문제점:\n",
   "- 데이터 수집 시점에 어떤 질의가 들어올지 예측 불가\n",
   "- 관련 정보가 무관한 텍스트에 묻혀있을 수 있음\n",
   "- 전체 문서 전달 시 비용 증가 및 응답 품질 저하\n",
   "\n",
   "### ContextualCompressionRetriever의 작동 원리\n",
   "\n",
   "1. **질의 전달**: Base retriever에 질의를 전달\n",
   "2. **문서 검색**: 초기 문서 집합 가져오기\n",
   "3. **문서 압축**: Document Compressor를 통해 관련 정보만 추출\n",
   "   - 개별 문서 내용 압축\n",
   "   - 관련 없는 문서 필터링\n",
   "\n",
   "> 💡 **핵심 아이디어**: 질의의 맥락을 활용하여 관련 정보만 선별적으로 반환"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 1,
  "id": "setup-cell",
  "metadata": {},
  "outputs": [
   {
    "data": {
     "text/plain": [
      "True"
     ]
    },
    "execution_count": 1,
    "metadata": {},
    "output_type": "execute_result"
   }
  ],
  "source": [
   "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
   "from dotenv import load_dotenv\n",
   "\n",
   "# API 키 정보 로드\n",
   "load_dotenv()"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 2,
  "id": "helper-function",
  "metadata": {},
  "outputs": [],
  "source": [
   "# 문서를 예쁘게 출력하기 위한 도우미 함수\n",
   "def pretty_print_docs(docs):\n",
   "    print(\n",
   "        f\"\\n{'-' * 100}\\n\".join(\n",
   "            [f\"문서 {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
   "        )\n",
   "    )"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "basic-retriever-section",
  "metadata": {},
  "source": [
   "## 기본 Retriever 설정\n",
   "\n",
   "먼저 일반적인 벡터 스토어 retriever를 설정하고, 압축 없이 검색했을 때의 결과를 확인해봅시다."
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 3,
  "id": "basic-retriever-setup",
  "metadata": {},
  "outputs": [
   {
    "name": "stdout",
    "output_type": "stream",
    "text": [
     "문서 1:\n",
     "\n",
     "Semantic Search\n",
     "\n",
     "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
     "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
     "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
     "\n",
     "Embedding\n",
     "----------------------------------------------------------------------------------------------------\n",
     "문서 2:\n",
     "\n",
     "정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.\n",
     "예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\n",
     "연관키워드: 검색 엔진, 데이터 검색, 정보 검색\n",
     "\n",
     "Page Rank\n",
     "----------------------------------------------------------------------------------------------------\n",
     "문서 3:\n",
     "\n",
     "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
     "예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
     "연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
     "\n",
     "Word2Vec\n",
     "----------------------------------------------------------------------------------------------------\n",
     "문서 4:\n",
     "\n",
     "정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.\n",
     "예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\n",
     "연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석\n",
     "\n",
     "데이터 마이닝\n"
    ]
   }
  ],
  "source": [
   "from langchain_community.document_loaders import TextLoader\n",
   "from langchain_community.vectorstores import FAISS\n",
   "from langchain_openai import OpenAIEmbeddings\n",
   "from langchain_text_splitters import CharacterTextSplitter\n",
   "\n",
   "# TextLoader를 사용하여 \"appendix-keywords.txt\" 파일에서 문서를 로드합니다.\n",
   "loader = TextLoader(\"../appendix-keywords.txt\")\n",
   "\n",
   "# CharacterTextSplitter를 사용하여 문서를 청크 크기 300자와 청크 간 중복 0으로 분할합니다.\n",
   "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
   "texts = loader.load_and_split(text_splitter)\n",
   "\n",
   "# OpenAIEmbeddings를 사용하여 FAISS 벡터 저장소를 생성하고 검색기로 변환합니다.\n",
   "retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()\n",
   "\n",
   "# 쿼리에 질문을 정의하고 관련 문서를 검색합니다.\n",
   "docs = retriever.invoke(\"Semantic Search 에 대해서 알려줘.\")\n",
   "\n",
   "# 검색된 문서를 예쁘게 출력합니다.\n",
   "pretty_print_docs(docs)"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "observation-1",
  "metadata": {},
  "source": [
   "### 관찰 결과\n",
   "\n",
   "기본 retriever는 4개의 문서를 반환했습니다:\n",
   "- ✅ **문서 1**: Semantic Search (직접적으로 관련)\n",
   "- ⚠️ **문서 2-4**: 키워드 검색, 크롤링, 페이지 랭크 (간접적이거나 관련 없음)\n",
   "\n",
   "이제 ContextualCompressionRetriever를 사용하여 관련성을 개선해봅시다."
  ]
 },
 {
  "cell_type": "markdown",
  "id": "contextual-compression-section",
  "metadata": {},
  "source": [
   "## 맥락적 압축 (ContextualCompression)\n",
   "\n",
   "### LLMChainExtractor\n",
   "\n",
   "`LLMChainExtractor`는 LLM을 사용하여 문서에서 질의와 관련된 부분만을 추출합니다."
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 4,
  "id": "llm-chain-extractor",
  "metadata": {},
  "outputs": [
   {
    "name": "stdout",
    "output_type": "stream",
    "text": [
     "문서 1:\n",
     "\n",
     "Semantic Search\n",
     "\n",
     "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
     "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
     "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
     "\n",
     "Embedding\n",
     "----------------------------------------------------------------------------------------------------\n",
     "문서 2:\n",
     "\n",
     "정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.\n",
     "예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\n",
     "연관키워드: 검색 엔진, 데이터 검색, 정보 검색\n",
     "\n",
     "Page Rank\n",
     "----------------------------------------------------------------------------------------------------\n",
     "문서 3:\n",
     "\n",
     "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
     "예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
     "연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
     "\n",
     "Word2Vec\n",
     "----------------------------------------------------------------------------------------------------\n",
     "문서 4:\n",
     "\n",
     "정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.\n",
     "예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\n",
     "연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석\n",
     "\n",
     "데이터 마이닝\n",
     "=========================================================\n",
     "============== LLMChainExtractor 적용 후 ==================\n",
     "문서 1:\n",
     "\n",
     "Semantic Search\n",
     "\n",
     "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
     "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n"
    ]
   }
  ],
  "source": [
   "from langchain_teddynote.document_compressors import LLMChainExtractor\n",
   "from langchain.retrievers import ContextualCompressionRetriever\n",
   "from langchain_openai import ChatOpenAI\n",
   "\n",
   "# OpenAI 언어 모델 초기화\n",
   "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
   "\n",
   "# LLM을 사용하여 문서 압축기 생성\n",
   "compressor = LLMChainExtractor.from_llm(llm)\n",
   "\n",
   "# 문서 압축기와 리트리버를 사용하여 컨텍스트 압축 리트리버 생성\n",
   "compression_retriever = ContextualCompressionRetriever(\n",
   "    base_compressor=compressor,\n",
   "    base_retriever=retriever,\n",
   ")\n",
   "\n",
   "# 압축 전 결과\n",
   "pretty_print_docs(retriever.invoke(\"Semantic Search 에 대해서 알려줘.\"))\n",
   "\n",
   "print(\"=========================================================\")\n",
   "print(\"============== LLMChainExtractor 적용 후 ==================\")\n",
   "\n",
   "# 압축 후 결과\n",
   "compressed_docs = compression_retriever.invoke(\n",
   "    \"Semantic Search 에 대해서 알려줘.\"\n",
   ")\n",
   "pretty_print_docs(compressed_docs)"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "llm-extractor-observation",
  "metadata": {},
  "source": [
   "### 결과 분석\n",
   "\n",
   "**LLMChainExtractor 효과:**\n",
   "- 4개 문서 → 1개 문서로 압축\n",
   "- 질의와 직접 관련된 Semantic Search 정보만 추출\n",
   "- 불필요한 정보 제거로 응답 품질 향상"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "llm-filtering-section",
  "metadata": {},
  "source": [
   "## LLM을 활용한 문서 필터링\n",
   "\n",
   "### LLMChainFilter\n",
   "\n",
   "`LLMChainFilter`는 문서 내용을 변경하지 않고 전체 문서를 선택적으로 반환합니다."
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 5,
  "id": "llm-chain-filter",
  "metadata": {},
  "outputs": [
   {
    "name": "stdout",
    "output_type": "stream",
    "text": [
     "문서 1:\n",
     "\n",
     "Semantic Search\n",
     "\n",
     "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
     "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
     "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
     "\n",
     "Embedding\n"
    ]
   }
  ],
  "source": [
   "from langchain_teddynote.document_compressors import LLMChainFilter\n",
   "\n",
   "# LLM을 사용하여 LLMChainFilter 객체를 생성합니다.\n",
   "_filter = LLMChainFilter.from_llm(llm)\n",
   "\n",
   "# LLMChainFilter와 retriever를 사용하여 ContextualCompressionRetriever 객체를 생성합니다.\n",
   "compression_retriever = ContextualCompressionRetriever(\n",
   "    base_compressor=_filter,\n",
   "    base_retriever=retriever,\n",
   ")\n",
   "\n",
   "compressed_docs = compression_retriever.invoke(\n",
   "    \"Semantic Search 에 대해서 알려줘.\"\n",
   ")\n",
   "pretty_print_docs(compressed_docs)"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "llm-filter-observation",
  "metadata": {},
  "source": [
   "### LLMChainFilter vs LLMChainExtractor\n",
   "\n",
   "| 특징 | LLMChainFilter | LLMChainExtractor |\n",
   "|------|----------------|-------------------|\n",
   "| 동작 방식 | 문서 전체 반환/제거 | 문서 내 관련 부분만 추출 |\n",
   "| 문서 내용 변경 | ❌ | ✅ |\n",
   "| 사용 시기 | 문서 단위 필터링 필요 시 | 문서 내 압축 필요 시 |"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "embeddings-filter-section",
  "metadata": {},
  "source": [
   "### EmbeddingsFilter\n",
   "\n",
   "`EmbeddingsFilter`는 임베딩 유사도를 기반으로 문서를 필터링합니다.\n",
   "\n",
   "**장점:**\n",
   "- ✅ LLM 호출 없이 작동 (빠르고 저렴)\n",
   "- ✅ 유사도 임계값으로 세밀한 제어 가능"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 6,
  "id": "embeddings-filter",
  "metadata": {},
  "outputs": [
   {
    "name": "stdout",
    "output_type": "stream",
    "text": [
     "문서 1:\n",
     "\n",
     "Semantic Search\n",
     "\n",
     "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
     "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
     "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
     "\n",
     "Embedding\n"
    ]
   }
  ],
  "source": [
   "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
   "from langchain_openai import OpenAIEmbeddings\n",
   "\n",
   "embeddings = OpenAIEmbeddings()\n",
   "\n",
   "# 유사도 임계값이 0.86인 EmbeddingsFilter 객체를 생성합니다.\n",
   "embeddings_filter = EmbeddingsFilter(\n",
   "    embeddings=embeddings, \n",
   "    similarity_threshold=0.86  # 0.86 이상의 유사도를 가진 문서만 반환\n",
   ")\n",
   "\n",
   "# 기본 압축기로 embeddings_filter를, 기본 검색기로 retriever를 사용\n",
   "compression_retriever = ContextualCompressionRetriever(\n",
   "    base_compressor=embeddings_filter, \n",
   "    base_retriever=retriever\n",
   ")\n",
   "\n",
   "compressed_docs = compression_retriever.invoke(\n",
   "    \"Semantic Search 에 대해서 알려줘.\"\n",
   ")\n",
   "pretty_print_docs(compressed_docs)"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "embeddings-observation",
  "metadata": {},
  "source": [
   "### 유사도 임계값 선택 가이드\n",
   "\n",
   "- **0.9 이상**: 매우 엄격한 필터링 (거의 완벽한 매칭)\n",
   "- **0.8 ~ 0.9**: 높은 관련성 요구\n",
   "- **0.7 ~ 0.8**: 중간 수준의 관련성\n",
   "- **0.7 이하**: 느슨한 필터링"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "pipeline-section",
  "metadata": {},
  "source": [
   "## 파이프라인 생성(압축기+문서 변환기)\n",
   "\n",
   "`DocumentCompressorPipeline`을 사용하면 여러 압축기와 변환기를 순차적으로 결합할 수 있습니다.\n",
   "\n",
   "### 파이프라인 구성 요소\n",
   "\n",
   "1. **CharacterTextSplitter**: 문서를 작은 청크로 분할\n",
   "2. **EmbeddingsRedundantFilter**: 중복 문서 제거 (유사도 0.95 이상)\n",
   "3. **EmbeddingsFilter**: 관련성 기준 필터링\n",
   "4. **LLMChainExtractor**: 최종 내용 압축"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 7,
  "id": "pipeline-setup",
  "metadata": {},
  "outputs": [],
  "source": [
   "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
   "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
   "from langchain_text_splitters import CharacterTextSplitter\n",
   "\n",
   "# 1. 문자 기반 텍스트 분할기 생성\n",
   "splitter = CharacterTextSplitter(\n",
   "    chunk_size=300,     # 각 청크의 최대 크기\n",
   "    chunk_overlap=0     # 청크 간 중복 없음\n",
   ")\n",
   "\n",
   "# 2. 중복 필터 생성 (임베딩 유사도 0.95 이상 제거)\n",
   "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
   "\n",
   "# 3. 관련성 필터 생성 (유사도 0.86 이상만 통과)\n",
   "relevant_filter = EmbeddingsFilter(\n",
   "    embeddings=embeddings, \n",
   "    similarity_threshold=0.86\n",
   ")\n",
   "\n",
   "# 4. 파이프라인 생성\n",
   "pipeline_compressor = DocumentCompressorPipeline(\n",
   "    transformers=[\n",
   "        splitter,           # 1단계: 문서 분할\n",
   "        redundant_filter,   # 2단계: 중복 제거\n",
   "        relevant_filter,    # 3단계: 관련성 필터링\n",
   "        LLMChainExtractor.from_llm(llm),  # 4단계: 내용 압축\n",
   "    ]\n",
   ")"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "pipeline-explanation",
  "metadata": {},
  "source": [
   "### 파이프라인 처리 흐름\n",
   "\n",
   "```\n",
   "원본 문서 → [분할] → 작은 청크들 → [중복 제거] → 고유 청크들 → [관련성 필터] → 관련 청크들 → [내용 압축] → 최종 결과\n",
   "```"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 8,
  "id": "pipeline-execution",
  "metadata": {},
  "outputs": [
   {
    "name": "stdout",
    "output_type": "stream",
    "text": [
     "문서 1:\n",
     "\n",
     "Semantic Search\n",
     "\n",
     "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
     "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n"
    ]
   }
  ],
  "source": [
   "# 파이프라인 압축기를 사용한 ContextualCompressionRetriever 생성\n",
   "compression_retriever = ContextualCompressionRetriever(\n",
   "    base_compressor=pipeline_compressor,\n",
   "    base_retriever=retriever,\n",
   ")\n",
   "\n",
   "# 파이프라인 실행\n",
   "compressed_docs = compression_retriever.invoke(\n",
   "    \"Semantic Search 에 대해서 알려줘.\"\n",
   ")\n",
   "\n",
   "# 결과 출력\n",
   "pretty_print_docs(compressed_docs)"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "summary-section",
  "metadata": {},
  "source": [
   "## 요약\n",
   "\n",
   "### 주요 학습 내용\n",
   "\n",
   "1. **ContextualCompressionRetriever의 필요성**\n",
   "   - 검색된 문서의 관련 정보만 추출\n",
   "   - LLM 호출 비용 절감 및 응답 품질 향상\n",
   "\n",
   "2. **다양한 압축 방법**\n",
   "   - **LLMChainExtractor**: 문서 내용 압축\n",
   "   - **LLMChainFilter**: 문서 전체 필터링\n",
   "   - **EmbeddingsFilter**: 임베딩 기반 빠른 필터링\n",
   "\n",
   "3. **파이프라인 구성**\n",
   "   - 여러 압축기와 변환기를 순차적으로 결합\n",
   "   - 분할 → 중복 제거 → 관련성 필터링 → 내용 압축\n",
   "\n",
   "### 사용 시나리오별 권장 사항\n",
   "\n",
   "| 시나리오 | 권장 방법 |\n",
   "|---------|----------|\n",
   "| 빠른 필터링 필요 | EmbeddingsFilter |\n",
   "| 정확한 내용 추출 | LLMChainExtractor |\n",
   "| 문서 단위 선택 | LLMChainFilter |\n",
   "| 복잡한 처리 | DocumentCompressorPipeline |\n",
   "\n",
   "### 성능 vs 품질 트레이드오프\n",
   "\n",
   "- **EmbeddingsFilter**: 빠르지만 덜 정확\n",
   "- **LLM 기반 방법**: 느리지만 더 정확\n",
   "- **파이프라인**: 균형잡힌 접근\n",
   "\n",
   "### 다음 단계\n",
   "\n",
   "- 다양한 유사도 임계값 실험\n",
   "- 커스텀 압축기 구현\n",
   "- 실제 애플리케이션에 통합"
  ]
 }
],
"metadata": {
 "kernelspec": {
  "display_name": "Python 3 (ipykernel)",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": 3
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.11.11"
 }
},
"nbformat": 4,
"nbformat_minor": 5
}