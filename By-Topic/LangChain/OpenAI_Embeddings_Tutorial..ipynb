{
"cells": [
 {
  "cell_type": "markdown",
  "id": "title",
  "metadata": {},
  "source": [
   "# OpenAI Embeddings Tutorial\n",
   "\n",
   "이 노트북에서는 LangChain의 OpenAIEmbeddings를 사용하여 텍스트를 벡터로 변환하고 유사도를 계산하는 방법을 학습합니다.\n",
   "\n",
   "## 학습 목표\n",
   "- OpenAI의 임베딩 모델 이해\n",
   "- 텍스트를 벡터로 변환하는 방법\n",
   "- 임베딩 차원 조절\n",
   "- 코사인 유사도를 통한 문장 간 유사도 계산"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "overview",
  "metadata": {},
  "source": [
   "## 1. 문서 임베딩 개요\n",
   "\n",
   "문서 임베딩은 문서의 내용을 수치적인 벡터로 변환하는 과정입니다. \n",
   "\n",
   "이 과정을 통해 문서의 의미를 수치화하고, 다양한 자연어 처리 작업에 활용할 수 있습니다. 대표적인 사전 학습된 언어 모델로는 BERT와 GPT가 있으며, 이러한 모델들은 문맥적 정보를 포착하여 문서의 의미를 인코딩합니다. \n",
   "\n",
   "문서 임베딩은 토큰화된 문서를 모델에 입력하여 임베딩 벡터를 생성하고, 이를 평균하여 전체 문서의 벡터를 생성합니다. 이 벡터는 문서 분류, 감성 분석, 문서 간 유사도 계산 등에 활용될 수 있습니다.\n",
   "\n",
   "[OpenAI Embeddings 공식 문서](https://platform.openai.com/docs/guides/embeddings/embedding-models)"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "model-comparison",
  "metadata": {},
  "source": [
   "## 2. OpenAI 임베딩 모델 비교\n",
   "\n",
   "| MODEL                  | PAGES PER DOLLAR | PERFORMANCE ON MTEB EVAL | MAX INPUT |\n",
   "|------------------------|------------------|---------------------------|-----------|\n",
   "| text-embedding-3-small | 62,500           | 62.3%                     | 8191      |\n",
   "| text-embedding-3-large | 9,615            | 64.6%                     | 8191      |\n",
   "| text-embedding-ada-002 | 12,500           | 61.0%                     | 8191      |\n",
   "\n",
   "- **text-embedding-3-small**: 비용 효율적이며 대부분의 사용 사례에 적합\n",
   "- **text-embedding-3-large**: 더 높은 성능이 필요한 경우 사용\n",
   "- **text-embedding-ada-002**: 이전 세대 모델"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "setup",
  "metadata": {},
  "source": [
   "## 3. 환경 설정"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 1,
  "id": "env-setup",
  "metadata": {},
  "outputs": [
   {
    "data": {
     "text/plain": [
      "True"
     ]
    },
    "execution_count": 1,
    "metadata": {},
    "output_type": "execute_result"
   }
  ],
  "source": [
   "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
   "from dotenv import load_dotenv\n",
   "\n",
   "# API 키 정보 로드\n",
   "load_dotenv()"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "basic-usage",
  "metadata": {},
  "source": [
   "## 4. OpenAIEmbeddings 기본 사용법"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 2,
  "id": "import-embeddings",
  "metadata": {},
  "outputs": [],
  "source": [
   "from langchain_openai import OpenAIEmbeddings\n",
   "\n",
   "# OpenAI의 \"text-embedding-3-small\" 모델을 사용하여 임베딩을 생성합니다.\n",
   "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 3,
  "id": "sample-text",
  "metadata": {},
  "outputs": [],
  "source": [
   "# 테스트용 샘플 텍스트\n",
   "text = \"임베딩 테스트를 하기 위한 샘플 문장입니다.\""
  ]
 },
 {
  "cell_type": "markdown",
  "id": "query-embedding",
  "metadata": {},
  "source": [
   "## 5. 쿼리 임베딩\n",
   "\n",
   "`embeddings.embed_query(text)`는 주어진 텍스트를 임베딩 벡터로 변환하는 함수입니다.\n",
   "\n",
   "이 함수는 텍스트를 벡터 공간에 매핑하여 의미적으로 유사한 텍스트를 찾거나 텍스트 간의 유사도를 계산하는 데 사용될 수 있습니다.\n",
   "\n",
   "**사용 시나리오:**\n",
   "- 검색 쿼리 벡터화\n",
   "- 단일 텍스트의 임베딩이 필요한 경우"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 4,
  "id": "query-result",
  "metadata": {},
  "outputs": [],
  "source": [
   "# 텍스트를 임베딩하여 쿼리 결과를 생성합니다.\n",
   "query_result = embeddings.embed_query(text)"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 5,
  "id": "query-preview",
  "metadata": {},
  "outputs": [
   {
    "data": {
     "text/plain": [
      "[-0.00776276458054781,\n",
      " 0.03680367395281792,\n",
      " 0.019545823335647583,\n",
      " -0.0196656696498394,\n",
      " 0.017203375697135925]"
     ]
    },
    "execution_count": 5,
    "metadata": {},
    "output_type": "execute_result"
   }
  ],
  "source": [
   "# 쿼리 결과의 처음 5개 항목을 확인합니다.\n",
   "query_result[:5]"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 6,
  "id": "query-dimension",
  "metadata": {},
  "outputs": [
   {
    "name": "stdout",
    "output_type": "stream",
    "text": [
     "기본 임베딩 차원: 1536\n"
    ]
   }
  ],
  "source": [
   "# 기본 임베딩 차원 확인\n",
   "print(f\"기본 임베딩 차원: {len(query_result)}\")"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "document-embedding",
  "metadata": {},
  "source": [
   "## 6. Document 임베딩\n",
   "\n",
   "`embeddings.embed_documents()` 함수를 사용하여 여러 텍스트 문서를 한 번에 임베딩합니다.\n",
   "\n",
   "**특징:**\n",
   "- 리스트 형태로 여러 문서를 한 번에 처리\n",
   "- 배치 처리로 효율적인 벡터화\n",
   "- 문서 검색 시스템 구축에 활용"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 7,
  "id": "doc-embedding",
  "metadata": {},
  "outputs": [],
  "source": [
   "# 여러 문서를 한 번에 임베딩\n",
   "doc_result = embeddings.embed_documents(\n",
   "    [text, text, text, text]\n",
   ")  # 동일한 텍스트를 4번 임베딩"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 8,
  "id": "doc-count",
  "metadata": {},
  "outputs": [
   {
    "data": {
     "text/plain": [
      "4"
     ]
    },
    "execution_count": 8,
    "metadata": {},
    "output_type": "execute_result"
   }
  ],
  "source": [
   "# 임베딩된 문서의 개수 확인\n",
   "len(doc_result)"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 9,
  "id": "doc-preview",
  "metadata": {},
  "outputs": [
   {
    "data": {
     "text/plain": [
      "[-0.00776276458054781,\n",
      " 0.03680367395281792,\n",
      " 0.019545823335647583,\n",
      " -0.0196656696498394,\n",
      " 0.017203375697135925]"
     ]
    },
    "execution_count": 9,
    "metadata": {},
    "output_type": "execute_result"
   }
  ],
  "source": [
   "# 첫 번째 문서의 임베딩 벡터 일부 확인\n",
   "doc_result[0][:5]"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 10,
  "id": "compare-embeddings",
  "metadata": {},
  "outputs": [
   {
    "name": "stdout",
    "output_type": "stream",
    "text": [
     "쿼리 임베딩과 문서 임베딩이 동일한가요? True\n"
    ]
   }
  ],
  "source": [
   "# 동일한 텍스트의 쿼리 임베딩과 문서 임베딩 비교\n",
   "print(f\"쿼리 임베딩과 문서 임베딩이 동일한가요? {query_result == doc_result[0]}\")"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "dimension-control",
  "metadata": {},
  "source": [
   "## 7. 임베딩 차원 조절\n",
   "\n",
   "`text-embedding-3` 모델 클래스를 사용하면 반환되는 임베딩의 크기를 지정할 수 있습니다.\n",
   "\n",
   "**차원 조절의 장점:**\n",
   "- 저장 공간 절약\n",
   "- 계산 속도 향상\n",
   "- 특정 작업에 최적화된 차원 선택 가능\n",
   "\n",
   "**주의사항:**\n",
   "- 차원을 줄이면 정보 손실이 발생할 수 있음\n",
   "- 작업의 복잡도에 따라 적절한 차원 선택 필요"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 11,
  "id": "custom-dimension",
  "metadata": {},
  "outputs": [],
  "source": [
   "# 1024차원의 임베딩을 생성하는 객체를 초기화합니다.\n",
   "embeddings_1024 = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=1024)"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 12,
  "id": "verify-dimension",
  "metadata": {},
  "outputs": [
   {
    "data": {
     "text/plain": [
      "1024"
     ]
    },
    "execution_count": 12,
    "metadata": {},
    "output_type": "execute_result"
   }
  ],
  "source": [
   "# 차원이 1024로 설정되었는지 확인\n",
   "len(embeddings_1024.embed_documents([text])[0])"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 13,
  "id": "compare-dimensions",
  "metadata": {},
  "outputs": [
   {
    "name": "stdout",
    "output_type": "stream",
    "text": [
     "512차원 임베딩 크기: 512\n",
     "1024차원 임베딩 크기: 1024\n",
     "1536차원 임베딩 크기(기본): 1536\n"
    ]
   }
  ],
  "source": [
   "# 다양한 차원으로 임베딩 생성 비교\n",
   "embeddings_512 = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=512)\n",
   "embeddings_1536 = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # 기본값\n",
   "\n",
   "print(f\"512차원 임베딩 크기: {len(embeddings_512.embed_query(text))}\")\n",
   "print(f\"1024차원 임베딩 크기: {len(embeddings_1024.embed_query(text))}\")\n",
   "print(f\"1536차원 임베딩 크기(기본): {len(embeddings_1536.embed_query(text))}\")"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "similarity-calculation",
  "metadata": {},
  "source": [
   "## 8. 유사도 계산\n",
   "\n",
   "임베딩 벡터를 사용하여 텍스트 간의 의미적 유사도를 계산할 수 있습니다.\n",
   "코사인 유사도는 두 벡터 간의 각도를 측정하여 -1에서 1 사이의 값을 반환합니다.\n",
   "\n",
   "**코사인 유사도 해석:**\n",
   "- 1에 가까울수록: 매우 유사함\n",
   "- 0에 가까울수록: 관련성이 낮음\n",
   "- -1에 가까울수록: 반대 의미 (실제로는 거의 발생하지 않음)"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 14,
  "id": "sample-sentences",
  "metadata": {},
  "outputs": [],
  "source": [
   "# 유사도 테스트를 위한 다양한 문장들\n",
   "sentence1 = \"안녕하세요? 반갑습니다.\"\n",
   "sentence2 = \"안녕하세요? 반갑습니다!\"  # 구두점만 다름\n",
   "sentence3 = \"안녕하세요? 만나서 반가워요.\"  # 유사한 의미\n",
   "sentence4 = \"Hi, nice to meet you.\"  # 영어로 같은 의미\n",
   "sentence5 = \"I like to eat apples.\"  # 전혀 다른 주제"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 15,
  "id": "embed-sentences",
  "metadata": {},
  "outputs": [],
  "source": [
   "from sklearn.metrics.pairwise import cosine_similarity\n",
   "\n",
   "# 모든 문장을 임베딩\n",
   "sentences = [sentence1, sentence2, sentence3, sentence4, sentence5]\n",
   "embedded_sentences = embeddings_1024.embed_documents(sentences)"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 16,
  "id": "similarity-function",
  "metadata": {},
  "outputs": [],
  "source": [
   "# 코사인 유사도 계산 함수\n",
   "def similarity(a, b):\n",
   "    \"\"\"두 임베딩 벡터 간의 코사인 유사도를 계산합니다.\"\"\"\n",
   "    return cosine_similarity([a], [b])[0][0]"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 17,
  "id": "calculate-similarities",
  "metadata": {},
  "outputs": [
   {
    "name": "stdout",
    "output_type": "stream",
    "text": [
     "[유사도 0.9644] 안녕하세요? 반갑습니다. \t <=====> \t 안녕하세요? 반갑습니다!\n",
     "[유사도 0.8376] 안녕하세요? 반갑습니다. \t <=====> \t 안녕하세요? 만나서 반가워요.\n",
     "[유사도 0.5042] 안녕하세요? 반갑습니다. \t <=====> \t Hi, nice to meet you.\n",
     "[유사도 0.1362] 안녕하세요? 반갑습니다. \t <=====> \t I like to eat apples.\n",
     "[유사도 0.8142] 안녕하세요? 반갑습니다! \t <=====> \t 안녕하세요? 만나서 반가워요.\n",
     "[유사도 0.4790] 안녕하세요? 반갑습니다! \t <=====> \t Hi, nice to meet you.\n",
     "[유사도 0.1318] 안녕하세요? 반갑습니다! \t <=====> \t I like to eat apples.\n",
     "[유사도 0.5127] 안녕하세요? 만나서 반가워요. \t <=====> \t Hi, nice to meet you.\n",
     "[유사도 0.1409] 안녕하세요? 만나서 반가워요. \t <=====> \t I like to eat apples.\n",
     "[유사도 0.2250] Hi, nice to meet you. \t <=====> \t I like to eat apples.\n"
    ]
   }
  ],
  "source": [
   "# 모든 문장 쌍의 유사도 계산 및 출력\n",
   "for i, sentence in enumerate(embedded_sentences):\n",
   "    for j, other_sentence in enumerate(embedded_sentences):\n",
   "        if i < j:  # 중복 계산 방지\n",
   "            print(\n",
   "                f\"[유사도 {similarity(sentence, other_sentence):.4f}] {sentences[i]} \\t <=====> \\t {sentences[j]}\"\n",
   "            )"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "similarity-analysis",
  "metadata": {},
  "source": [
   "## 9. 유사도 분석\n",
   "\n",
   "위 결과를 분석해보면:\n",
   "\n",
   "1. **구두점만 다른 경우 (0.9644)**: 거의 동일한 의미로 인식\n",
   "2. **같은 언어, 유사한 표현 (0.8376, 0.8142)**: 높은 유사도\n",
   "3. **다른 언어, 같은 의미 (0.5042, 0.4790, 0.5127)**: 중간 정도의 유사도\n",
   "4. **완전히 다른 주제 (0.1362, 0.1318, 0.1409)**: 낮은 유사도\n",
   "\n",
   "이를 통해 OpenAI 임베딩이 의미론적 유사성을 잘 포착함을 확인할 수 있습니다."
  ]
 },
 {
  "cell_type": "markdown",
  "id": "practical-example",
  "metadata": {},
  "source": [
   "## 10. 실용적인 예제: 간단한 문서 검색 시스템"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": 18,
  "id": "document-search",
  "metadata": {},
  "outputs": [
   {
    "name": "stdout",
    "output_type": "stream",
    "text": [
     "\n",
     "쿼리: '머신러닝을 배우고 싶어요'\n",
     "\n",
     "검색 결과:\n",
     "1. [유사도: 0.7893] 인공지능과 머신러닝은 현대 기술의 핵심입니다. 딥러닝은 머신러닝의 한 분야입니다.\n",
     "2. [유사도: 0.6543] 파이썬은 데이터 사이언스와 머신러닝에 가장 인기 있는 프로그래밍 언어입니다.\n",
     "3. [유사도: 0.4012] 데이터 분석은 비즈니스 의사결정에 중요한 역할을 합니다.\n"
    ]
   }
  ],
  "source": [
   "# 간단한 문서 데이터베이스\n",
   "documents = [\n",
   "    \"인공지능과 머신러닝은 현대 기술의 핵심입니다. 딥러닝은 머신러닝의 한 분야입니다.\",\n",
   "    \"파이썬은 데이터 사이언스와 머신러닝에 가장 인기 있는 프로그래밍 언어입니다.\",\n",
   "    \"자연어 처리는 컴퓨터가 인간의 언어를 이해하고 처리하는 기술입니다.\",\n",
   "    \"클라우드 컴퓨팅은 인터넷을 통해 컴퓨팅 자원을 제공하는 서비스입니다.\",\n",
   "    \"데이터 분석은 비즈니스 의사결정에 중요한 역할을 합니다.\"\n",
   "]\n",
   "\n",
   "# 문서 임베딩\n",
   "doc_embeddings = embeddings_1024.embed_documents(documents)\n",
   "\n",
   "# 검색 쿼리\n",
   "query = \"머신러닝을 배우고 싶어요\"\n",
   "query_embedding = embeddings_1024.embed_query(query)\n",
   "\n",
   "# 유사도 계산 및 정렬\n",
   "similarities = []\n",
   "for i, doc_embedding in enumerate(doc_embeddings):\n",
   "    sim = similarity(query_embedding, doc_embedding)\n",
   "    similarities.append((i, sim, documents[i]))\n",
   "\n",
   "# 유사도 기준으로 정렬\n",
   "similarities.sort(key=lambda x: x[1], reverse=True)\n",
   "\n",
   "# 상위 3개 결과 출력\n",
   "print(f\"\\n쿼리: '{query}'\\n\")\n",
   "print(\"검색 결과:\")\n",
   "for i, (idx, sim, doc) in enumerate(similarities[:3]):\n",
   "    print(f\"{i+1}. [유사도: {sim:.4f}] {doc}\")"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "best-practices",
  "metadata": {},
  "source": [
   "## 11. 임베딩 사용 시 주의사항 및 모범 사례\n",
   "\n",
   "### 주의사항\n",
   "1. **API 비용**: 대량의 텍스트를 임베딩할 때는 비용을 고려해야 합니다.\n",
   "2. **토큰 제한**: 최대 8191 토큰까지 처리 가능합니다.\n",
   "3. **차원 선택**: 용도에 맞는 적절한 차원을 선택하세요.\n",
   "\n",
   "### 모범 사례\n",
   "1. **배치 처리**: `embed_documents()`를 사용하여 여러 문서를 한 번에 처리\n",
   "2. **캐싱**: 자주 사용하는 임베딩은 저장하여 재사용\n",
   "3. **정규화**: 필요에 따라 텍스트 전처리 수행\n",
   "4. **모델 선택**: 성능과 비용의 균형을 고려하여 모델 선택"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "summary",
  "metadata": {},
  "source": [
   "## 12. 요약\n",
   "\n",
   "이 튜토리얼에서 학습한 내용:\n",
   "\n",
   "1. **OpenAI 임베딩 모델**의 종류와 특징\n",
   "2. **쿼리 임베딩**과 **문서 임베딩**의 차이점과 사용법\n",
   "3. **임베딩 차원 조절** 방법\n",
   "4. **코사인 유사도**를 사용한 텍스트 유사도 계산\n",
   "5. 간단한 **문서 검색 시스템** 구현\n",
   "\n",
   "OpenAI 임베딩은 다양한 NLP 작업의 기초가 되는 중요한 기술입니다. RAG(Retrieval-Augmented Generation), 시맨틱 검색, 문서 분류 등 다양한 응용 분야에서 활용할 수 있습니다."
  ]
 }
],
"metadata": {
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": 3
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.11.0"
 }
},
"nbformat": 4,
"nbformat_minor": 5
}