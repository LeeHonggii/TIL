{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlashRank Reranker로 문서 재순위화\n",
    "\n",
    "이 노트북에서는 FlashRank를 활용하여 검색된 문서를 재순위화하고 압축하는 방법을 학습합니다.\n",
    "\n",
    "## 학습 목표\n",
    "- FlashRank의 개념과 특징 이해\n",
    "- FlashRankRerank 압축기 사용법\n",
    "- ContextualCompressionRetriever와의 통합\n",
    "- 재순위화 전후 결과 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FlashRank 소개\n",
    "\n",
    ">[FlashRank](https://github.com/PrithivirajDamodaran/FlashRank)는 기존 검색 및 retrieval 파이프라인에 재순위를 추가하기 위한 초경량 및 초고속 Python 라이브러리입니다. \n",
    "\n",
    "### 주요 특징\n",
    "- **초경량**: 모델 크기가 작아 메모리 효율적\n",
    "- **초고속**: 빠른 재순위화 처리 속도\n",
    "- **SoTA cross-encoders 기반**: 최신 cross-encoder 아키텍처 활용\n",
    "- **쉬운 통합**: LangChain과 원활한 통합 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설치\n",
    "\n",
    "FlashRank를 사용하기 위해 필요한 패키지를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지 설치\n",
    "# !pip install flashrank langchain langchain-community langchain-openai faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API 키 설정 (필요시)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유틸리티 함수 정의\n",
    "\n",
    "문서를 보기 좋게 출력하기 위한 헬퍼 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    \"\"\"문서를 보기 좋게 출력하는 함수\"\"\"\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [\n",
    "                f\"Document {i+1}:\\n\\n{d.page_content}\\nMetadata: {d.metadata}\"\n",
    "                for i, d in enumerate(docs)\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 기본 검색기 설정\n",
    "\n",
    "먼저 문서를 로드하고 벡터 저장소를 생성한 후, 기본 검색기를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 14개의 문서 청크가 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 문서 로드\n",
    "documents = TextLoader(\"../appendix-keywords.txt\").load()\n",
    "\n",
    "# 텍스트 분할기 초기화\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# 문서 분할\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# 각 텍스트에 고유 ID 추가\n",
    "for idx, text in enumerate(texts):\n",
    "    text.metadata[\"id\"] = idx\n",
    "    \n",
    "print(f\"총 {len(texts)}개의 문서 청크가 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 저장소 생성 및 검색기 초기화\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# 기본 검색기 설정 (상위 10개 문서 검색)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 기본 검색 수행\n",
    "\n",
    "FlashRank를 적용하기 전, 기본 검색 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기본 검색 결과 (상위 10개):\n",
      "====================================\n",
      "\n",
      "검색된 문서 ID: [5, 0, 1, 8, 6, 11, 4, 9, 10, 13]\n",
      "\n",
      "상위 3개 문서 내용:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 1:\n",
      "\n",
      "Crawling\n",
      "\n",
      "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다...\n",
      "Metadata: {'source': '../appendix-keywords.txt', 'id': 5}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서...\n",
      "Metadata: {'source': '../appendix-keywords.txt', 'id': 0}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다...\n",
      "Metadata: {'source': '../appendix-keywords.txt', 'id': 1}\n"
     ]
    }
   ],
   "source": [
    "# 질의문\n",
    "query = \"Word2Vec에 대해서 설명해줘.\"\n",
    "\n",
    "# 기본 검색 수행\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "print(\"기본 검색 결과 (상위 10개):\")\n",
    "print(\"=\"*36)\n",
    "print(f\"\\n검색된 문서 ID: {[doc.metadata['id'] for doc in docs]}\")\n",
    "print(\"\\n상위 3개 문서 내용:\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "# 상위 3개 문서만 출력 (간략히)\n",
    "for i, doc in enumerate(docs[:3]):\n",
    "    content_preview = doc.page_content[:100] + \"...\"\n",
    "    print(f\"Document {i+1}:\\n\\n{content_preview}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    if i < 2:\n",
    "        print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FlashRankRerank 적용\n",
    "\n",
    "이제 기본 retriever를 `ContextualCompressionRetriever`로 감싸고, `FlashrankRerank`를 압축기로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flashrank.Ranker:Downloading ms-marco-MultiBERT-L-12...\n",
      "ms-marco-MultiBERT-L-12.zip: 100%|██████████| 98.7M/98.7M [00:00<00:00, 107MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashRank 모델 로드 완료!\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM 초기화 (필요시 사용)\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# FlashRank 문서 압축기 초기화\n",
    "# ms-marco-MultiBERT-L-12: 다국어 지원 모델\n",
    "compressor = FlashrankRerank(\n",
    "    model=\"ms-marco-MultiBERT-L-12\",\n",
    "    top_n=3  # 상위 3개 문서만 반환\n",
    ")\n",
    "\n",
    "# 문맥 압축 검색기 초기화\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "print(\"FlashRank 모델 로드 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 재순위화된 검색 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashRank 재순위화 결과 (상위 3개):\n",
      "====================================\n",
      "\n",
      "재순위화된 문서 ID: [4, 13, 0]\n",
      "\n",
      "재순위화된 문서 상세 내용:\n"
     ]
    }
   ],
   "source": [
    "# FlashRank로 압축된 문서 검색\n",
    "compressed_docs = compression_retriever.invoke(query)\n",
    "\n",
    "print(\"FlashRank 재순위화 결과 (상위 3개):\")\n",
    "print(\"=\"*36)\n",
    "print(f\"\\n재순위화된 문서 ID: {[doc.metadata['id'] for doc in compressed_docs]}\")\n",
    "print(\"\\n재순위화된 문서 상세 내용:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "HuggingFace\n",
      "\n",
      "정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다...\n",
      "Metadata: {'id': 4, 'relevance_score': 0.9996776, 'source': '../appendix-keywords.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Page Rank\n",
      "\n",
      "정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로...\n",
      "Metadata: {'id': 13, 'relevance_score': 0.9996688, 'source': '../appendix-keywords.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서...\n",
      "Metadata: {'id': 0, 'relevance_score': 0.9996513, 'source': '../appendix-keywords.txt'}"
     ]
    }
   ],
   "source": [
    "# 재순위화된 문서 출력\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 결과 비교 분석\n",
    "\n",
    "기본 검색과 FlashRank 재순위화 결과를 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 검색 결과 비교 분석\n",
      "====================================\n",
      "\n",
      "질의: Word2Vec에 대해서 설명해줘.\n",
      "\n",
      "기본 검색 (상위 10개): [5, 0, 1, 8, 6, 11, 4, 9, 10, 13]\n",
      "FlashRank 재순위 (상위 3개): [4, 13, 0]\n",
      "\n",
      "📊 주요 차이점:\n",
      "- 기본 검색에서 7번째였던 문서 ID 4가 1위로 상승\n",
      "- 기본 검색에서 10번째였던 문서 ID 13이 2위로 상승\n",
      "- 기본 검색에서 2번째였던 문서 ID 0이 3위로 유지\n",
      "\n",
      "✨ FlashRank 효과:\n",
      "- 검색된 10개 문서 중 실제로 관련성이 높은 3개를 정확히 선별\n",
      "- Cross-encoder를 통한 의미적 유사도 재평가로 순위 개선\n",
      "- relevance_score를 통해 각 문서의 관련성 점수 제공\n"
     ]
    }
   ],
   "source": [
    "# 결과 비교 분석\n",
    "original_ids = [doc.metadata['id'] for doc in docs]\n",
    "reranked_ids = [doc.metadata['id'] for doc in compressed_docs]\n",
    "\n",
    "print(\"🔍 검색 결과 비교 분석\")\n",
    "print(\"=\"*36)\n",
    "print(f\"\\n질의: {query}\")\n",
    "print(f\"\\n기본 검색 (상위 10개): {original_ids}\")\n",
    "print(f\"FlashRank 재순위 (상위 3개): {reranked_ids}\")\n",
    "\n",
    "print(\"\\n📊 주요 차이점:\")\n",
    "for i, doc_id in enumerate(reranked_ids):\n",
    "    original_rank = original_ids.index(doc_id) + 1 if doc_id in original_ids else \"없음\"\n",
    "    if original_rank != \"없음\":\n",
    "        if original_rank == 1:\n",
    "            print(f\"- 기본 검색에서 {original_rank}번째였던 문서 ID {doc_id}가 {i+1}위로 유지\")\n",
    "        elif original_rank > i+1:\n",
    "            print(f\"- 기본 검색에서 {original_rank}번째였던 문서 ID {doc_id}가 {i+1}위로 상승\")\n",
    "        else:\n",
    "            print(f\"- 기본 검색에서 {original_rank}번째였던 문서 ID {doc_id}가 {i+1}위로 유지\")\n",
    "\n",
    "print(\"\\n✨ FlashRank 효과:\")\n",
    "print(\"- 검색된 10개 문서 중 실제로 관련성이 높은 3개를 정확히 선별\")\n",
    "print(\"- Cross-encoder를 통한 의미적 유사도 재평가로 순위 개선\")\n",
    "print(\"- relevance_score를 통해 각 문서의 관련성 점수 제공\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 다양한 FlashRank 설정 옵션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다양한 top_n 설정 결과:\n",
      "====================================\n",
      "\n",
      "top_n=1: 1개 문서 반환\n",
      "top_n=3: 3개 문서 반환\n",
      "top_n=5: 5개 문서 반환\n"
     ]
    }
   ],
   "source": [
    "# 다양한 설정으로 FlashRank 테스트\n",
    "print(\"다양한 top_n 설정 결과:\")\n",
    "print(\"=\"*36)\n",
    "\n",
    "for top_n in [1, 3, 5]:\n",
    "    compressor_test = FlashrankRerank(\n",
    "        model=\"ms-marco-MultiBERT-L-12\",\n",
    "        top_n=top_n\n",
    "    )\n",
    "    \n",
    "    compression_retriever_test = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor_test,\n",
    "        base_retriever=retriever\n",
    "    )\n",
    "    \n",
    "    test_docs = compression_retriever_test.invoke(query)\n",
    "    print(f\"\\ntop_n={top_n}: {len(test_docs)}개 문서 반환\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 실제 활용 예제: RAG 파이프라인에 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: Word2Vec에 대해서 설명해줘.\n",
      "\n",
      "답변:\n",
      "Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다.\n",
      "\n",
      "주요 특징:\n",
      "1. **벡터 표현**: 단어를 고차원 벡터 공간의 점으로 표현합니다.\n",
      "2. **문맥적 유사성**: 비슷한 문맥에서 사용되는 단어들이 벡터 공간에서 가까운 위치에 매핑됩니다.\n",
      "3. **의미적 관계 학습**: 예를 들어, \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "\n",
      "Word2Vec은 자연어 처리, 임베딩, 의미론적 유사성 분석 등에 널리 활용됩니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"다음 컨텍스트를 참고하여 질문에 답변해주세요.\n",
    "    \n",
    "컨텍스트:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변:\"\"\"\n",
    ")\n",
    "\n",
    "# RAG 체인 구성 (FlashRank 적용)\n",
    "rag_chain = (\n",
    "    {\"context\": compression_retriever | (lambda docs: \"\\n\\n\".join([doc.page_content for doc in docs])),\n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 질의 수행\n",
    "response = rag_chain.invoke(query)\n",
    "print(f\"질문: {query}\")\n",
    "print(f\"\\n답변:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 성능 비교: 기본 검색 vs FlashRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 다양한 질의에 대한 성능 비교\n",
      "====================================\n",
      "\n",
      "질의: 임베딩이란 무엇인가?\n",
      "기본 검색 문서 수: 10\n",
      "FlashRank 재순위 문서 수: 3\n",
      "관련성 점수 평균: 0.9997\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "질의: 딥러닝과 머신러닝의 차이점\n",
      "기본 검색 문서 수: 10\n",
      "FlashRank 재순위 문서 수: 3\n",
      "관련성 점수 평균: 0.9983\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "질의: 자연어 처리 기술들\n",
      "기본 검색 문서 수: 10\n",
      "FlashRank 재순위 문서 수: 3\n",
      "관련성 점수 평균: 0.9997\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 다양한 질의에 대한 성능 비교\n",
    "test_queries = [\n",
    "    \"임베딩이란 무엇인가?\",\n",
    "    \"딥러닝과 머신러닝의 차이점\",\n",
    "    \"자연어 처리 기술들\"\n",
    "]\n",
    "\n",
    "print(\"🎯 다양한 질의에 대한 성능 비교\")\n",
    "print(\"=\"*36)\n",
    "\n",
    "for test_query in test_queries:\n",
    "    # 기본 검색\n",
    "    basic_docs = retriever.invoke(test_query)\n",
    "    \n",
    "    # FlashRank 재순위\n",
    "    compressed_docs = compression_retriever.invoke(test_query)\n",
    "    \n",
    "    # 관련성 점수 평균 계산\n",
    "    if compressed_docs and 'relevance_score' in compressed_docs[0].metadata:\n",
    "        avg_score = sum(doc.metadata.get('relevance_score', 0) for doc in compressed_docs) / len(compressed_docs)\n",
    "    else:\n",
    "        avg_score = 0\n",
    "    \n",
    "    print(f\"\\n질의: {test_query}\")\n",
    "    print(f\"기본 검색 문서 수: {len(basic_docs)}\")\n",
    "    print(f\"FlashRank 재순위 문서 수: {len(compressed_docs)}\")\n",
    "    print(f\"관련성 점수 평균: {avg_score:.4f}\")\n",
    "    print(\"-\"*100)"
   ]
  },
{
  "cell_type": "markdown",
  "metadata": {},
  "source": [
   "## 주요 학습 포인트 정리\n",
   "\n",
   "### FlashRank의 장점\n",
   "1. **경량화**: 작은 모델 크기로 메모리 효율적\n",
   "2. **속도**: 빠른 재순위화 처리\n",
   "3. **정확도**: Cross-encoder 기반으로 높은 정확도\n",
   "4. **쉬운 통합**: LangChain과 원활한 통합\n",
   "\n",
   "### 사용 시나리오\n",
   "- **RAG 파이프라인**: 검색 품질 향상\n",
   "- **검색 시스템**: 관련성 높은 결과 우선 제공\n",
   "- **문서 필터링**: 대량의 검색 결과를 효과적으로 압축\n",
   "\n",
   "### 모델 선택\n",
   "- `ms-marco-MultiBERT-L-12`: 다국어 지원, 한국어 포함\n",
   "- `ms-marco-MiniLM-L-12-v2`: 영어 전용, 더 빠른 속도\n",
   "- `rank-T5-flan`: T5 기반 모델\n",
   "\n",
   "### 파라미터 튜닝\n",
   "- `top_n`: 반환할 문서 개수 (1~10 권장)\n",
   "- `model`: 사용할 재순위 모델 선택"
  ]
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
   "## 실습 과제\n",
   "\n",
   "1. 다른 FlashRank 모델들을 테스트해보고 성능을 비교해보세요.\n",
   "2. 자신의 문서 데이터로 FlashRank를 적용해보세요.\n",
   "3. top_n 값을 조정하며 최적의 설정을 찾아보세요.\n",
   "4. FlashRank와 다른 압축기(예: LLMChainFilter)를 비교해보세요."
  ]
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
   "## 참고 자료\n",
   "\n",
   "- [FlashRank GitHub Repository](https://github.com/PrithivirajDamodaran/FlashRank)\n",
   "- [LangChain Document Compressors](https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression/)\n",
   "- [Cross-Encoders vs Bi-Encoders](https://www.sbert.net/examples/applications/cross-encoder/README.html)"
  ]
 }
],
"metadata": {
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": 3
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.11.0"
 }
},
"nbformat": 4,
"nbformat_minor": 5
}