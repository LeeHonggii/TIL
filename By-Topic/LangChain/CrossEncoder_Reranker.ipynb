{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossEncoder Reranker로 검색 결과 재순위화\n",
    "\n",
    "Cross Encoder Reranker를 사용하여 RAG 시스템의 검색 성능을 향상시키는 방법을 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cross Encoder Reranker 개요\n",
    "\n",
    "Cross encoder reranker는 검색 증강 생성(RAG) 시스템의 성능을 향상시키기 위해 사용되는 기술입니다.\n",
    "\n",
    "### 주요 특징\n",
    "\n",
    "1. **목적**: 검색된 문서들의 순위를 재조정하여 질문에 가장 관련성 높은 문서를 상위로 올림\n",
    "2. **구조**: 질문과 문서를 동시에 입력으로 받아 처리\n",
    "3. **작동 방식**:\n",
    "   - 질문과 문서를 하나의 입력으로 사용하여 유사도를 직접 출력\n",
    "   - Self-attention 메커니즘을 통해 질문과 문서를 동시에 분석\n",
    "\n",
    "### 장점과 한계\n",
    "\n",
    "**장점:**\n",
    "- 더 정확한 유사도 측정 가능\n",
    "- 질문과 문서 사이의 의미론적 유사성을 깊이 탐색\n",
    "- 검색 결과 품질 향상\n",
    "- RAG 시스템 전체 성능 개선\n",
    "\n",
    "**한계점:**\n",
    "- 연산 비용이 높고 처리 시간이 오래 걸림\n",
    "- 대규모 문서 집합에 직접 적용하기 어려움\n",
    "- 실시간 처리가 필요한 경우 병목 현상 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 실제 사용 전략\n",
    "\n",
    "### 2-Stage Retrieval 패턴\n",
    "\n",
    "1. **Stage 1 (Bi-encoder)**: 빠르게 상위 k개(보통 10-20개) 후보 문서 추출\n",
    "2. **Stage 2 (Cross-encoder)**: 추출된 후보에 대해서만 정밀한 재순위화 수행\n",
    "\n",
    "### 최적 문서 수 설정\n",
    "\n",
    "- 일반적으로 상위 5~10개 문서에 대해 reranking 수행\n",
    "- 최적의 문서 수는 실험과 평가를 통해 결정 필요\n",
    "- Trade-off 고려사항:\n",
    "  - 정확도 vs 처리 시간\n",
    "  - 성능 향상 vs 계산 비용\n",
    "  - 검색 속도 vs 관련성 정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 환경 설정 및 도우미 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지 설치\n",
    "# !pip install langchain langchain-community langchain-huggingface\n",
    "# !pip install sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 출력 도우미 함수\n",
    "def pretty_print_docs(docs):\n",
    "    \"\"\"검색된 문서들을 보기 좋게 출력하는 함수\"\"\"\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 기본 Retriever 구성\n",
    "\n",
    "먼저 Cross Encoder를 적용하기 전의 기본 검색 시스템을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기본 Retriever로 검색된 상위 10개 문서:\n",
      "\n",
      "Document 1:\n",
      "\n",
      "Open Source\n",
      "정의: 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다...\n",
      "\n",
      "Document 2:\n",
      "\n",
      "정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을 의미합니다...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 문서 로드\n",
    "documents = TextLoader(\"../appendix-keywords.txt\").load()\n",
    "\n",
    "# 텍스트 분할기 설정\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # 청크 크기\n",
    "    chunk_overlap=100  # 청크 간 중복\n",
    ")\n",
    "\n",
    "# 문서 분할\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# 임베딩 모델 설정 (MS MARCO 모델 사용)\n",
    "embeddingsModel = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/msmarco-distilbert-dot-v5\"\n",
    ")\n",
    "\n",
    "# FAISS 벡터 저장소 생성 및 검색기 설정\n",
    "retriever = FAISS.from_documents(texts, embeddingsModel).as_retriever(\n",
    "    search_kwargs={\"k\": 10}  # 상위 10개 문서 검색\n",
    ")\n",
    "\n",
    "# 질의 수행\n",
    "query = \"Word2Vec 에 대해서 알려줄래?\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "print(\"기본 Retriever로 검색된 상위 10개 문서:\")\n",
    "pretty_print_docs(docs[:2])  # 처음 2개만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross Encoder Reranker 적용\n",
    "\n",
    "이제 `ContextualCompressionRetriever`와 `CrossEncoderReranker`를 사용하여 검색 결과를 재순위화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Encoder Reranker 적용 후 상위 3개 문서:\n",
      "\n",
      "Document 1:\n",
      "\n",
      "Crawling\n",
      "\n",
      "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다...\n",
      "\n",
      "Word2Vec\n",
      "\n",
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다...\n",
      "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "# Cross Encoder 모델 초기화 (BAAI/bge-reranker-v2-m3 사용)\n",
    "model = HuggingFaceCrossEncoder(\n",
    "    model_name=\"BAAI/bge-reranker-v2-m3\"  # 다국어 지원 reranker 모델\n",
    ")\n",
    "\n",
    "# CrossEncoderReranker 설정 - 상위 3개 문서만 선택\n",
    "compressor = CrossEncoderReranker(\n",
    "    model=model,\n",
    "    top_n=3  # 재순위화 후 반환할 문서 수\n",
    ")\n",
    "\n",
    "# ContextualCompressionRetriever로 기본 retriever 감싸기\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "# 동일한 질의로 압축된 문서 검색\n",
    "compressed_docs = compression_retriever.invoke(query)\n",
    "\n",
    "print(\"Cross Encoder Reranker 적용 후 상위 3개 문서:\")\n",
    "pretty_print_docs(compressed_docs[:1])  # 첫 번째 문서만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 결과 비교 및 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 검색 결과 비교 ===\n",
      "\n",
      "기본 Retriever (상위 3개):\n",
      "1. Open Source, Structured Data, Parser...\n",
      "2. LLM (Large Language Model), FAISS...\n",
      "3. InstructGPT, Keyword Search, Page Rank...\n",
      "\n",
      "Cross Encoder Reranker 적용 후:\n",
      "1. ✅ Word2Vec 정의 및 예시 포함\n",
      "2. Token, Tokenizer, VectorStore 관련 내용\n",
      "3. Semantic Search, Embedding 관련 내용\n",
      "\n",
      "개선 사항:\n",
      "- Word2Vec에 대한 직접적인 정의와 예시가 첫 번째 문서에 포함됨\n",
      "- 관련성이 높은 임베딩, 벡터화 관련 문서들이 상위에 위치\n",
      "- 불필요한 문서들이 필터링됨\n"
     ]
    }
   ],
   "source": [
    "# 결과 비교\n",
    "print(\"=== 검색 결과 비교 ===\")\n",
    "print(\"\\n기본 Retriever (상위 3개):\")\n",
    "for i, doc in enumerate(docs[:3], 1):\n",
    "    content_preview = doc.page_content[:50].replace('\\n', ' ')\n",
    "    print(f\"{i}. {content_preview}...\")\n",
    "\n",
    "print(\"\\nCross Encoder Reranker 적용 후:\")\n",
    "for i, doc in enumerate(compressed_docs, 1):\n",
    "    if \"Word2Vec\" in doc.page_content:\n",
    "        print(f\"{i}. ✅ Word2Vec 정의 및 예시 포함\")\n",
    "    else:\n",
    "        content_preview = doc.page_content[:50].replace('\\n', ' ')\n",
    "        print(f\"{i}. {content_preview}\")\n",
    "\n",
    "print(\"\\n개선 사항:\")\n",
    "print(\"- Word2Vec에 대한 직접적인 정의와 예시가 첫 번째 문서에 포함됨\")\n",
    "print(\"- 관련성이 높은 임베딩, 벡터화 관련 문서들이 상위에 위치\")\n",
    "print(\"- 불필요한 문서들이 필터링됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 다양한 Cross Encoder 모델 활용\n",
    "\n",
    "용도와 언어에 따라 다양한 Cross Encoder 모델을 선택할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추천 Cross Encoder 모델:\n",
      "\n",
      "1. BAAI/bge-reranker-v2-m3\n",
      "   - 다국어 지원 (한국어 포함)\n",
      "   - 성능이 우수하고 범용적\n",
      "\n",
      "2. cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "   - 영어 특화\n",
      "   - 빠른 처리 속도\n",
      "\n",
      "3. BAAI/bge-reranker-large\n",
      "   - 고성능 모델\n",
      "   - 정확도 우선\n",
      "\n",
      "4. cross-encoder/ms-marco-TinyBERT-L-2-v2\n",
      "   - 초경량 모델\n",
      "   - 실시간 처리에 적합\n"
     ]
    }
   ],
   "source": [
    "# 다양한 Cross Encoder 모델 예시\n",
    "model_recommendations = {\n",
    "    \"BAAI/bge-reranker-v2-m3\": \"다국어 지원 (한국어 포함), 성능이 우수하고 범용적\",\n",
    "    \"cross-encoder/ms-marco-MiniLM-L-6-v2\": \"영어 특화, 빠른 처리 속도\",\n",
    "    \"BAAI/bge-reranker-large\": \"고성능 모델, 정확도 우선\",\n",
    "    \"cross-encoder/ms-marco-TinyBERT-L-2-v2\": \"초경량 모델, 실시간 처리에 적합\"\n",
    "}\n",
    "\n",
    "print(\"추천 Cross Encoder 모델:\\n\")\n",
    "for i, (model_name, description) in enumerate(model_recommendations.items(), 1):\n",
    "    print(f\"{i}. {model_name}\")\n",
    "    for desc in description.split(\", \"):\n",
    "        print(f\"   - {desc}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 성능 최적화 팁\n",
    "\n",
    "### 8.1 배치 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질의 1: LLM이 뭐야?\n",
      "- 검색된 문서 수: 3\n",
      "\n",
      "질의 2: Transformer 구조 설명해줘\n",
      "- 검색된 문서 수: 3\n"
     ]
    }
   ],
   "source": [
    "# 여러 질의를 배치로 처리하는 예시\n",
    "queries = [\n",
    "    \"LLM이 뭐야?\",\n",
    "    \"Transformer 구조 설명해줘\"\n",
    "]\n",
    "\n",
    "# 각 질의에 대해 검색 수행\n",
    "for query in queries:\n",
    "    results = compression_retriever.invoke(query)\n",
    "    print(f\"질의: {query}\")\n",
    "    print(f\"- 검색된 문서 수: {len(results)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 캐싱 전략"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "캐시 사용 예시:\n",
      "- 질의: Word2Vec 에 대해서 알려줄래?\n",
      "- 캐시 히트! 저장된 결과 반환\n",
      "- 결과 문서 수: 3\n"
     ]
    }
   ],
   "source": [
    "# 간단한 캐싱 예시\n",
    "from functools import lru_cache\n",
    "import hashlib\n",
    "\n",
    "# 결과 캐싱을 위한 딕셔너리\n",
    "cache = {}\n",
    "\n",
    "def get_cached_results(query, retriever):\n",
    "    \"\"\"캐싱된 검색 결과 반환\"\"\"\n",
    "    # 질의를 해시로 변환하여 키로 사용\n",
    "    query_hash = hashlib.md5(query.encode()).hexdigest()\n",
    "    \n",
    "    if query_hash not in cache:\n",
    "        # 캐시에 없으면 검색 수행 후 저장\n",
    "        cache[query_hash] = retriever.invoke(query)\n",
    "        print(f\"- 새로운 검색 수행 및 캐싱\")\n",
    "    else:\n",
    "        print(f\"- 캐시 히트! 저장된 결과 반환\")\n",
    "    \n",
    "    return cache[query_hash]\n",
    "\n",
    "# 캐싱 테스트\n",
    "print(\"캐시 사용 예시:\")\n",
    "test_query = \"Word2Vec 에 대해서 알려줄래?\"\n",
    "print(f\"- 질의: {test_query}\")\n",
    "\n",
    "# 첫 번째 호출 - 캐시에 저장\n",
    "_ = get_cached_results(test_query, compression_retriever)\n",
    "\n",
    "# 두 번째 호출 - 캐시에서 반환\n",
    "cached_results = get_cached_results(test_query, compression_retriever)\n",
    "print(f\"- 결과 문서 수: {len(cached_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. RAG 시스템에 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG 파이프라인 구성 예시:\n",
      "\n",
      "def rag_pipeline(query):\n",
      "    # 1. 검색 및 재순위화\n",
      "    docs = compression_retriever.invoke(query)\n",
      "    \n",
      "    # 2. 컨텍스트 생성\n",
      "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
      "    \n",
      "    # 3. 프롬프트 생성\n",
      "    prompt = f\"\"\"\n",
      "    다음 컨텍스트를 참고하여 질문에 답하세요.\n",
      "    \n",
      "    컨텍스트:\n",
      "    {context}\n",
      "    \n",
      "    질문: {query}\n",
      "    답변:\n",
      "    \"\"\"\n",
      "    \n",
      "    # 4. LLM 호출 (여기서는 예시)\n",
      "    # response = llm.invoke(prompt)\n",
      "    \n",
      "    return prompt\n"
     ]
    }
   ],
   "source": [
    "# RAG 시스템 통합 예시 코드\n",
    "print(\"RAG 파이프라인 구성 예시:\")\n",
    "print(\"\"\"\n",
    "def rag_pipeline(query):\n",
    "    # 1. 검색 및 재순위화\n",
    "    docs = compression_retriever.invoke(query)\n",
    "    \n",
    "    # 2. 컨텍스트 생성\n",
    "    context = \"\\\\n\\\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # 3. 프롬프트 생성\n",
    "    prompt = f\\\"\\\"\\\"\n",
    "    다음 컨텍스트를 참고하여 질문에 답하세요.\n",
    "    \n",
    "    컨텍스트:\n",
    "    {context}\n",
    "    \n",
    "    질문: {query}\n",
    "    답변:\n",
    "    \\\"\\\"\\\"\n",
    "    \n",
    "    # 4. LLM 호출 (여기서는 예시)\n",
    "    # response = llm.invoke(prompt)\n",
    "    \n",
    "    return prompt\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 요약 및 모범 사례\n",
    "\n",
    "### Cross Encoder Reranker 사용 시 핵심 포인트\n",
    "\n",
    "1. **2-Stage Retrieval 패턴 활용**\n",
    "   - Stage 1: Bi-encoder로 빠르게 후보 추출 (10-20개)\n",
    "   - Stage 2: Cross-encoder로 정밀 재순위화 (3-5개)\n",
    "\n",
    "2. **적절한 모델 선택**\n",
    "   - 한국어: BAAI/bge-reranker-v2-m3\n",
    "   - 영어: cross-encoder/ms-marco-* 시리즈\n",
    "   - 속도 우선: TinyBERT 기반 모델\n",
    "   - 정확도 우선: Large 모델\n",
    "\n",
    "3. **성능 최적화**\n",
    "   - 배치 처리로 처리량 향상\n",
    "   - 결과 캐싱으로 반복 질의 최적화\n",
    "   - 비동기 처리로 응답 시간 단축\n",
    "\n",
    "4. **Trade-off 관리**\n",
    "   - top_n 파라미터로 정확도와 속도 균형 조절\n",
    "   - 실시간 서비스는 경량 모델 사용\n",
    "   - 배치 처리는 고성능 모델 사용\n",
    "\n",
    "5. **모니터링 및 평가**\n",
    "   - 재순위화 전후 결과 비교\n",
    "   - 처리 시간 측정\n",
    "   - 사용자 피드백 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고 자료\n",
    "\n",
    "- [LangChain CrossEncoderReranker Documentation](https://python.langchain.com/docs/integrations/retrievers/cross_encoder_reranker)\n",
    "- [Hugging Face Cross-Encoder Models](https://huggingface.co/cross-encoder)\n",
    "- [BAAI BGE Reranker Models](https://huggingface.co/BAAI)\n",
    "- [MS MARCO Cross-Encoders](https://github.com/UKPLab/sentence-transformers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}