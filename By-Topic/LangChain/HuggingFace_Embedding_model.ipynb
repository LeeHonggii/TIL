{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c1c0f77",
   "metadata": {},
   "source": [
    "# HuggingFace Embeddings Tutorial\n",
    "\n",
    "이 튜토리얼에서는 LangChain과 HuggingFace를 활용한 다양한 임베딩 방법을 다룹니다:\n",
    "- HuggingFace Endpoint Embeddings\n",
    "- HuggingFace Local Embeddings\n",
    "- BGE-M3 임베딩\n",
    "- FlagEmbedding (Dense, Sparse, Multi-Vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9015268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()\n",
    "\n",
    "# 경고 무시\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ./cache/ 경로에 다운로드 받도록 설정\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-data-section",
   "metadata": {},
   "source": [
    "## 테스트 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "039ea623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다국어 테스트 텍스트\n",
    "texts = [\n",
    "    \"안녕, 만나서 반가워.\",\n",
    "    \"LangChain simplifies the process of building applications with large language models\",\n",
    "    \"랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다. \",\n",
    "    \"LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.\",\n",
    "    \"Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6750eb",
   "metadata": {},
   "source": [
    "## 1. HuggingFace Endpoint Embedding\n",
    "\n",
    "`HuggingFaceEndpointEmbeddings`는 HuggingFace의 추론 API를 사용하여 임베딩을 생성합니다. 로컬에 모델을 다운로드할 필요 없이 API를 통해 임베딩을 계산할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bddd50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "\n",
    "hf_embeddings = HuggingFaceEndpointEmbeddings(\n",
    "    model=model_name,\n",
    "    task=\"feature-extraction\",\n",
    "    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37690f28",
   "metadata": {},
   "source": [
    "### Document 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6d0c37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \t\tintfloat/multilingual-e5-large-instruct\n",
      "Dimension: \t1024\n"
     ]
    }
   ],
   "source": [
    "# Document Embedding 수행\n",
    "embedded_documents = hf_embeddings.embed_documents(texts)\n",
    "\n",
    "print(f\"Model: \\t\\t{model_name}\")\n",
    "print(f\"Dimension: \\t{len(embedded_documents[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query-embedding-section",
   "metadata": {},
   "source": [
    "### Query 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ebf9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "# Query Embedding 수행\n",
    "embedded_query = hf_embeddings.embed_query(\"LangChain 에 대해서 알려주세요.\")\n",
    "print(f\"Query embedding dimension: {len(embedded_query)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ed26b",
   "metadata": {},
   "source": [
    "## 유사도 계산\n",
    "\n",
    "### 벡터 내적을 통한 유사도 계산\n",
    "\n",
    "벡터 내적(dot product)을 사용하여 유사도를 계산합니다.\n",
    "\n",
    "**수학적 배경:**\n",
    "\n",
    "1. **벡터 내적 정의**\n",
    "   $$ \\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i $$\n",
    "\n",
    "2. **코사인 유사도와의 관계**\n",
    "   $$ \\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos \\theta $$\n",
    "\n",
    "3. **유사도 해석**\n",
    "   - 내적 값이 클수록 두 벡터가 유사\n",
    "   - 두 벡터가 같은 방향을 가리킬수록 높은 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "addfdb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사도 점수: [0.842  0.865  0.865  0.896  0.768]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 질문(embedded_query): LangChain 에 대해서 알려주세요.\n",
    "similarities = np.array(embedded_query) @ np.array(embedded_documents).T\n",
    "print(f\"유사도 점수: {similarities.round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9590ff84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query] LangChain 에 대해서 알려주세요.\n",
      "====================================\n",
      "[0] LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.\n",
      "[1] LangChain simplifies the process of building applications with large language models\n",
      "[2] 랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다. \n",
      "[3] 안녕, 만나서 반가워.\n",
      "[4] Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\n"
     ]
    }
   ],
   "source": [
    "# 유사도 순으로 정렬\n",
    "sorted_idx = similarities.argsort()[::-1]\n",
    "\n",
    "print(\"[Query] LangChain 에 대해서 알려주세요.\\n====================================\")\n",
    "for i, idx in enumerate(sorted_idx):\n",
    "    print(f\"[{i}] {texts[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8157e3ac",
   "metadata": {},
   "source": [
    "## 2. HuggingFace Local Embeddings\n",
    "\n",
    "`HuggingFaceEmbeddings`는 모델을 로컬에 다운로드하여 사용합니다. API 호출 없이 로컬에서 임베딩을 생성할 수 있어 더 빠른 처리가 가능합니다.\n",
    "\n",
    "### 사용 가능한 모델들\n",
    "- [intfloat/multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct)\n",
    "- [intfloat/multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d9c7b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "\n",
    "hf_local_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": \"cpu\"},  # cuda, cpu, mps(Mac)\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # 정규화된 임베딩 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bebd5a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \t\tintfloat/multilingual-e5-large-instruct\n",
      "Dimension: \t1024\n"
     ]
    }
   ],
   "source": [
    "# Document Embedding\n",
    "embedded_documents_local = hf_local_embeddings.embed_documents(texts)\n",
    "\n",
    "print(f\"Model: \\t\\t{model_name}\")\n",
    "print(f\"Dimension: \\t{len(embedded_documents_local[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45093d",
   "metadata": {},
   "source": [
    "## 3. BGE-M3 임베딩\n",
    "\n",
    "BGE-M3는 다국어 지원이 우수한 임베딩 모델입니다.\n",
    "\n",
    "### 디바이스 옵션\n",
    "- `{\"device\": \"cpu\"}`: CPU를 사용하여 임베딩 계산 (모든 사용자)\n",
    "- `{\"device\": \"cuda\"}`: GPU를 사용하여 임베딩 계산 (CUDA 설치 필요)\n",
    "- `{\"device\": \"mps\"}`: Apple Silicon GPU 사용 (Mac 사용자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "727eac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "bge_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, \n",
    "    model_kwargs=model_kwargs, \n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19e090a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \t\tBAAI/bge-m3\n",
      "Dimension: \t1024\n"
     ]
    }
   ],
   "source": [
    "# Document Embedding\n",
    "bge_embedded_documents = bge_embeddings.embed_documents(texts)\n",
    "\n",
    "print(f\"Model: \\t\\t{model_name}\")\n",
    "print(f\"Dimension: \\t{len(bge_embedded_documents[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49278179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query] LangChain 에 대해서 알려주세요.\n",
      "====================================\n",
      "[0] LangChain simplifies the process of building applications with large language models\n",
      "[1] LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.\n",
      "[2] 랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다. \n",
      "[3] 안녕, 만나서 반가워.\n",
      "[4] Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\n"
     ]
    }
   ],
   "source": [
    "# BGE-M3로 유사도 검색\n",
    "bge_query = bge_embeddings.embed_query(\"LangChain 에 대해서 알려주세요.\")\n",
    "bge_similarities = np.array(bge_query) @ np.array(bge_embedded_documents).T\n",
    "bge_sorted_idx = bge_similarities.argsort()[::-1]\n",
    "\n",
    "print(\"[Query] LangChain 에 대해서 알려주세요.\\n====================================\")\n",
    "for i, idx in enumerate(bge_sorted_idx):\n",
    "    print(f\"[{i}] {texts[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a46d097",
   "metadata": {},
   "source": [
    "## 4. FlagEmbedding을 활용한 고급 임베딩\n",
    "\n",
    "FlagEmbedding은 BGE-M3 모델의 고급 기능을 활용할 수 있는 라이브러리입니다. 세 가지 접근법을 제공합니다:\n",
    "\n",
    "1. **Dense Vector**: 기본적인 dense embedding\n",
    "2. **Sparse Embedding**: Lexical weight를 활용한 sparse embedding\n",
    "3. **Multi-Vector (ColBERT)**: 토큰 수준의 세밀한 매칭\n",
    "\n",
    "### 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5d80213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FlagEmbedding 설치\n",
    "!pip install -qU FlagEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-vector-section",
   "metadata": {},
   "source": [
    "### 4.1 Dense Vector Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f74db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "bge_flagmodel = BGEM3FlagModel(\n",
    "    model_name, \n",
    "    use_fp16=True  # FP16 사용으로 메모리 효율성 향상\n",
    ")\n",
    "\n",
    "# Dense embedding 생성\n",
    "bge_encoded = bge_flagmodel.encode(\n",
    "    texts,\n",
    "    batch_size=12,\n",
    "    max_length=8192,  # 긴 문서 지원\n",
    "    return_dense=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5c0c19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense vector shape: (5, 1024)\n"
     ]
    }
   ],
   "source": [
    "# 결과 출력(행, 열)\n",
    "print(f\"Dense vector shape: {bge_encoded['dense_vecs'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d74eb",
   "metadata": {},
   "source": [
    "### 4.2 Sparse Embedding (Lexical Weight)\n",
    "\n",
    "Sparse embedding은 단어의 중요도를 직접적으로 반영하는 방식입니다.\n",
    "\n",
    "**특징:**\n",
    "- 대부분의 값이 0인 고차원 벡터\n",
    "- 특정 단어나 구문을 정확히 매칭\n",
    "- TF-IDF나 BM25와 유사한 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba35f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse embedding 생성\n",
    "bge_sparse_encoded = bge_flagmodel.encode(\n",
    "    texts, \n",
    "    return_sparse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a7be2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical matching score (0 <-> 0): 1.0\n",
      "Lexical matching score (0 <-> 1): 0.0\n"
     ]
    }
   ],
   "source": [
    "# Lexical matching score 계산\n",
    "lexical_scores1 = bge_flagmodel.compute_lexical_matching_score(\n",
    "    bge_sparse_encoded[\"lexical_weights\"][0], \n",
    "    bge_sparse_encoded[\"lexical_weights\"][0]\n",
    ")\n",
    "lexical_scores2 = bge_flagmodel.compute_lexical_matching_score(\n",
    "    bge_sparse_encoded[\"lexical_weights\"][0], \n",
    "    bge_sparse_encoded[\"lexical_weights\"][1]\n",
    ")\n",
    "\n",
    "print(f\"Lexical matching score (0 <-> 0): {lexical_scores1}\")\n",
    "print(f\"Lexical matching score (0 <-> 1): {lexical_scores2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88420a00",
   "metadata": {},
   "source": [
    "### 4.3 Multi-Vector (ColBERT)\n",
    "\n",
    "ColBERT(Contextualized Late Interaction over BERT)는 토큰 수준의 세밀한 매칭을 수행합니다.\n",
    "\n",
    "**작동 방식:**\n",
    "1. 문서의 각 토큰에 대해 별도의 벡터 생성\n",
    "2. 쿼리의 각 토큰과 문서의 모든 토큰 간 유사도 계산\n",
    "3. MaxSim 연산으로 최종 점수 계산\n",
    "\n",
    "**장점:**\n",
    "- 토큰 수준의 정밀한 매칭\n",
    "- 문맥을 고려한 임베딩\n",
    "- 긴 문서에 효과적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2243360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColBERT vectors 생성\n",
    "bge_colbert_encoded = bge_flagmodel.encode(\n",
    "    texts, \n",
    "    return_colbert_vecs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bb92de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColBERT score (0 <-> 0): 1.0\n",
      "ColBERT score (0 <-> 1): 0.65\n"
     ]
    }
   ],
   "source": [
    "# ColBERT score 계산\n",
    "colbert_scores1 = bge_flagmodel.colbert_score(\n",
    "    bge_colbert_encoded[\"colbert_vecs\"][0], \n",
    "    bge_colbert_encoded[\"colbert_vecs\"][0]\n",
    ")\n",
    "colbert_scores2 = bge_flagmodel.colbert_score(\n",
    "    bge_colbert_encoded[\"colbert_vecs\"][0], \n",
    "    bge_colbert_encoded[\"colbert_vecs\"][1]\n",
    ")\n",
    "\n",
    "print(f\"ColBERT score (0 <-> 0): {colbert_scores1:.2f}\")\n",
    "print(f\"ColBERT score (0 <-> 1): {colbert_scores2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## 요약\n",
    "\n",
    "이 튜토리얼에서는 다양한 HuggingFace 임베딩 방법을 다루었습니다:\n",
    "\n",
    "1. **HuggingFace Endpoint Embeddings**\n",
    "   - API를 통한 임베딩 생성\n",
    "   - 로컬 설치 불필요\n",
    "   - API 토큰 필요\n",
    "\n",
    "2. **HuggingFace Local Embeddings**\n",
    "   - 로컬에서 모델 실행\n",
    "   - 더 빠른 처리 속도\n",
    "   - 오프라인 사용 가능\n",
    "\n",
    "3. **BGE-M3 Embeddings**\n",
    "   - 우수한 다국어 지원\n",
    "   - 높은 성능\n",
    "   - 다양한 디바이스 옵션\n",
    "\n",
    "4. **FlagEmbedding 고급 기능**\n",
    "   - Dense Vector: 기본 임베딩\n",
    "   - Sparse Embedding: 정확한 단어 매칭\n",
    "   - ColBERT: 토큰 수준 매칭\n",
    "\n",
    "각 방법은 사용 사례에 따라 장단점이 있으므로, 프로젝트의 요구사항에 맞게 선택하여 사용하시기 바랍니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}