   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create_documents() 메서드 사용\n",
    "\n",
    "- `text_splitter`를 사용하여 `file` 텍스트를 문서 단위로 분할합니다.\n",
    "- 분할된 문서는 `texts` 리스트에 저장됩니다.\n",
    "- 각 문서는 `page_content` 속성을 가진 Document 객체입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding'\n",
      "============================================================\n",
      "page_content='Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token'\n"
     ]
    }
   ],
   "source": [
    "# text_splitter를 사용하여 file 텍스트를 문서로 분할합니다.\n",
    "texts = text_splitter.create_documents([file])\n",
    "print(texts[0])  # 분할된 문서의 첫 번째 문서를 출력합니다.\n",
    "print(\"===\" * 20)\n",
    "print(texts[1])  # 분할된 문서의 두 번째 문서를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split_text() 메서드 사용\n",
    "\n",
    "`split_text()` 메서드는 텍스트를 분할하여 문자열 리스트로 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Semantic Search\\n\\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\\n\\nEmbedding',\n",
       " 'Embedding\\n\\n정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트를 분할하고 분할된 텍스트의 처음 2개 요소를 반환합니다.\n",
    "text_splitter.split_text(file)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 분할 결과 분석\n",
    "\n",
    "### 관찰 포인트:\n",
    "1. **청크 크기**: 각 청크는 약 250자 이내로 제한됨\n",
    "2. **중복 부분**: 첫 번째 청크의 끝부분 \"Embedding\"이 두 번째 청크의 시작 부분에도 나타남 (50자 중복)\n",
    "3. **의미적 단위 유지**: 각 키워드의 정의, 예시, 연관키워드가 가능한 한 같은 청크에 유지됨\n",
    "\n",
    "### 실무 활용 팁:\n",
    "- **chunk_size**: 임베딩 모델의 토큰 제한과 검색 정확도를 고려하여 설정 (일반적으로 500-1000)\n",
    "- **chunk_overlap**: 문맥 손실을 방지하기 위해 적절한 중복 설정 (일반적으로 chunk_size의 10-20%)\n",
    "- **separators**: 문서 특성에 맞는 구분자 커스터마이징 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 고급 설정 예제\n",
    "\n",
    "다양한 설정으로 텍스트 분할기를 사용하는 예제입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "커스텀 구분자를 사용한 분할 결과:\n",
      "청크 1 길이: 326\n",
      "청크 2 길이: 259\n"
     ]
    }
   ],
   "source": [
    "# 커스텀 구분자를 사용한 텍스트 분할기\n",
    "custom_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # 커스텀 구분자 설정\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "custom_splits = custom_text_splitter.split_text(file)\n",
    "print(\"커스텀 구분자를 사용한 분할 결과:\")\n",
    "print(f\"청크 1 길이: {len(custom_splits[0])}\")\n",
    "print(f\"청크 2 길이: {len(custom_splits[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 요약\n",
    "\n",
    "### RecursiveCharacterTextSplitter의 장점:\n",
    "1. **의미적 일관성**: 단락 > 문장 > 단어 순서로 분할하여 의미적 단위 보존\n",
    "2. **유연성**: chunk_size, overlap, separators 등 다양한 설정 가능\n",
    "3. **효율성**: 대용량 텍스트도 효과적으로 처리\n",
    "\n",
    "### 사용 시나리오:\n",
    "- RAG (Retrieval-Augmented Generation) 시스템 구축\n",
    "- 문서 임베딩 및 벡터 데이터베이스 저장\n",
    "- 긴 문서의 청크 단위 처리\n",
    "- LLM의 컨텍스트 윈도우 제한 극복\n",
    "\n",
    "### 다음 단계:\n",
    "- 다른 텍스트 분할기 (CharacterTextSplitter, TokenTextSplitter 등) 탐색\n",
    "- 분할된 텍스트를 임베딩하여 벡터 데이터베이스에 저장\n",
    "- RAG 파이프라인 구축"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RecursiveCharacterTextSplitter Tutorial\n",
    "\n",
    "이 노트북에서는 LangChain의 RecursiveCharacterTextSplitter를 사용하여 텍스트를 의미론적으로 연관된 청크로 분할하는 방법을 학습합니다.\n",
    "\n",
    "## 📚 학습 목표\n",
    "- RecursiveCharacterTextSplitter의 작동 원리 이해\n",
    "- 텍스트 분할 시 청크 크기와 중복 설정 방법 학습\n",
    "- 실제 텍스트 파일을 사용한 분할 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RecursiveCharacterTextSplitter 개요\n",
    "\n",
    "RecursiveCharacterTextSplitter는 일반적인 텍스트에 권장되는 텍스트 분할 방식입니다.\n",
    "\n",
    "### 주요 특징:\n",
    "- 문자 목록을 매개변수로 받아 동작\n",
    "- 청크가 충분히 작아질 때까지 주어진 문자 목록 순서대로 텍스트를 분할\n",
    "- 기본 문자 목록: `[\"\\n\\n\", \"\\n\", \" \", \"\"]`\n",
    "\n",
    "### 분할 순서:\n",
    "**단락** → **문장** → **단어** 순서로 재귀적으로 분할\n",
    "\n",
    "이는 단락(그 다음으로 문장, 단어) 단위가 의미적으로 가장 강하게 연관된 텍스트 조각으로 간주되므로, 가능한 한 함께 유지하려는 효과가 있습니다.\n",
    "\n",
    "### 핵심 개념:\n",
    "1. **텍스트 분할 방식**: 문자 목록(`[\"\\n\\n\", \"\\n\", \" \", \"\"]`)에 의해 분할\n",
    "2. **청크 크기 측정**: 문자 수에 의해 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 샘플 데이터 로드\n",
    "\n",
    "텍스트 분할을 실습하기 위해 샘플 텍스트 파일을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n",
    "with open(\"../appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "source": [
    "# 파일으로부터 읽은 내용을 일부 출력합니다.\n",
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RecursiveCharacterTextSplitter 설정 및 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 텍스트 분할기 설정\n",
    "\n",
    "`RecursiveCharacterTextSplitter`를 사용하여 텍스트를 작은 청크로 분할하는 예제입니다.\n",
    "\n",
    "#### 주요 매개변수:\n",
    "- `chunk_size`: 각 청크의 최대 크기 (문자 수)\n",
    "- `chunk_overlap`: 인접한 청크 간의 중복 문자 수\n",
    "- `length_function`: 텍스트 길이를 계산하는 함수\n",
    "- `is_separator_regex`: 구분자로 정규식 사용 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    