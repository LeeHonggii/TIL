{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatOllama를 사용한 로컬 LLM 활용 가이드\n",
    "\n",
    "이 노트북은 Ollama를 사용하여 로컬 환경에서 LLM을 구동하고 활용하는 방법을 다룹니다.\n",
    "\n",
    "## 주요 내용\n",
    "- Ollama 설치 및 설정\n",
    "- 한국어 모델 (EEVE-Korean) 사용\n",
    "- JSON 형식 출력 지원\n",
    "- 멀티모달 모델 (LLaVA) 활용\n",
    "- Modelfile을 통한 커스텀 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ollama 설치 및 설정\n",
    "\n",
    "### 프로그램 설치\n",
    "Ollama를 지원되는 플랫폼(Mac / Linux / Windows)에 다운로드하고 설치하세요.\n",
    "\n",
    "- 설치주소: [https://ollama.com/](https://ollama.com/)\n",
    "\n",
    "### 모델 다운로드\n",
    "\n",
    "#### 허깅페이스 모델\n",
    "허깅페이스(HuggingFace)에서 오픈모델을 다운로드 받습니다 (.gguf 확장자)\n",
    "\n",
    "#### Ollama 제공 모델\n",
    "`ollama pull <name-of-model>` 명령을 사용하여 사용 가능한 LLM 모델을 가져오세요.\n",
    "- 예: `ollama pull gemma:7b`\n",
    "\n",
    "아래의 경로에 모델의 기본 태그 버전이 다운로드됩니다.\n",
    "- Mac: `~/.ollama/models`\n",
    "- Linux/WSL: `/usr/share/ollama/.ollama/models`\n",
    "\n",
    "### 주요 명령어\n",
    "- `ollama list` - 가져온 모든 모델 확인\n",
    "- `ollama run <name-of-model>` - 명령줄에서 모델과 직접 채팅\n",
    "- `ollama pull <name-of-model>` - 모델 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 한국어 모델 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# Ollama 한국어 모델을 불러옵니다.\n",
    "llm = ChatOllama(model=\"EEVE-Korean-10.8B:latest\")\n",
    "\n",
    "# 프롬프트 생성\n",
    "prompt = ChatPromptTemplate.from_template(\"{topic} 에 대하여 간략히 설명해 줘.\")\n",
    "\n",
    "# 체인 생성\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 스트리밍 실행\n",
    "answer = chain.stream({\"topic\": \"deep learning\"})\n",
    "\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 비동기 스트리밍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비동기적으로 체인을 실행하여 청크 단위로 결과를 반환\n",
    "async for chunks in chain.astream(\n",
    "    {\"topic\": \"구글\"}\n",
    "):\n",
    "    print(chunks, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. JSON 형식 출력\n",
    "\n",
    "일부 모델(예: gemma)은 JSON 형식 출력을 지원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# JSON 형식 출력을 지원하는 모델 설정\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma:7b\",\n",
    "    format=\"json\",  # 입출력 형식을 JSON으로 설정\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# JSON 형식의 답변을 요구하는 프롬프트\n",
    "prompt = \"유럽 여행지 10곳을 알려주세요. key: `places`. response in JSON format.\"\n",
    "\n",
    "# 실행\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelfile을 통한 커스텀 모델 생성\n",
    "\n",
    "### Modelfile 구성 요소\n",
    "```\n",
    "FROM [모델파일명]              # 사용할 GGUF 파일 지정\n",
    "TEMPLATE \"\"\"[프롬프트 템플릿]\"\"\" # 입출력 형식 정의\n",
    "SYSTEM \"\"\"[시스템 프롬프트]\"\"\"   # AI 역할 정의\n",
    "PARAMETER [파라미터 설정]        # 추론 관련 설정\n",
    "```\n",
    "\n",
    "### 모델 생성 및 실행\n",
    "```bash\n",
    "# 모델 생성\n",
    "ollama create [모델별칭] -f Modelfile\n",
    "\n",
    "# 모델 실행\n",
    "ollama run [모델별칭]\n",
    "```\n",
    "\n",
    "### 주요 파라미터\n",
    "```\n",
    "PARAMETER temperature 0      # 출력의 무작위성 (0: 결정적, 1: 무작위)\n",
    "PARAMETER num_predict 3000   # 최대 토큰 생성 수\n",
    "PARAMETER num_ctx 4096      # 컨텍스트 윈도우 크기\n",
    "PARAMETER stop <s>          # 생성 중단 토큰\n",
    "PARAMETER stop </s>         # 생성 중단 토큰\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 멀티모달 모델 활용 (LLaVA)\n",
    "\n",
    "Ollama는 bakllava와 llava와 같은 멀티모달 LLM을 지원합니다.\n",
    "\n",
    "### 설치\n",
    "```bash\n",
    "ollama pull llava:7b\n",
    "# 또는\n",
    "ollama pull bakllava\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 처리를 위한 유틸리티 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from IPython.display import HTML, display\n",
    "from PIL import Image\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def convert_to_base64(pil_image):\n",
    "    \"\"\"\n",
    "    PIL 이미지를 Base64로 인코딩된 문자열로 변환합니다.\n",
    "    \"\"\"\n",
    "    buffered = BytesIO()\n",
    "    pil_image.save(buffered, format=\"JPEG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return img_str\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"\n",
    "    Base64로 인코딩된 문자열을 이미지로 표시합니다.\n",
    "    \"\"\"\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    display(HTML(image_html))\n",
    "\n",
    "def prompt_func(data):\n",
    "    \"\"\"\n",
    "    이미지와 텍스트를 포함하는 프롬프트를 생성합니다.\n",
    "    \"\"\"\n",
    "    text = data[\"text\"]\n",
    "    image = data[\"image\"]\n",
    "\n",
    "    image_part = {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{image}\",\n",
    "    }\n",
    "\n",
    "    text_part = {\"type\": \"text\", \"text\": text}\n",
    "\n",
    "    return [HumanMessage(content=[image_part, text_part])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 멀티모달 모델 사용 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 이미지 로드 및 변환\n",
    "file_path = \"../image/jeju-beach.jpg\"\n",
    "pil_image = Image.open(file_path)\n",
    "image_b64 = convert_to_base64(pil_image)\n",
    "\n",
    "# 이미지 표시\n",
    "plt_img_base64(image_b64)\n",
    "\n",
    "# 멀티모달 모델 설정\n",
    "llm = ChatOllama(model=\"llava:7b\", temperature=0)\n",
    "\n",
    "# 체인 생성\n",
    "chain = prompt_func | llm | StrOutputParser()\n",
    "\n",
    "# 이미지 설명 요청\n",
    "result = chain.invoke(\n",
    "    {\"text\": \"Describe a picture in bullet points\", \"image\": image_b64}\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 실제 활용시 주의사항\n",
    "\n",
    "1. **파일 위치**: GGUF 파일과 Modelfile이 같은 디렉토리에 있어야 함\n",
    "2. **파일명 정확성**: 파일명이 정확히 일치해야 함\n",
    "3. **양자화 레벨**: Q4_0, Q8_0 등 양자화 레벨 확인 필요\n",
    "4. **시스템 리소스**: RAM, GPU 고려 필요\n",
    "5. **모델 크기**: 모델에 따라 필요한 메모리가 다름\n",
    "\n",
    "### 권장 사양\n",
    "- 7B 모델: 8GB RAM 이상\n",
    "- 13B 모델: 16GB RAM 이상\n",
    "- 70B 모델: 32GB RAM 이상 + GPU 권장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 요약\n",
    "\n",
    "이 노트북에서는 다음 내용을 학습했습니다:\n",
    "\n",
    "1. **Ollama 설치 및 설정** - 로컬 환경에서 LLM 구동\n",
    "2. **한국어 모델 활용** - EEVE-Korean 모델 사용법\n",
    "3. **JSON 출력** - 구조화된 데이터 생성\n",
    "4. **커스텀 모델 생성** - Modelfile을 통한 설정\n",
    "5. **멀티모달 모델** - 이미지와 텍스트 함께 처리\n",
    "\n",
    "Ollama를 사용하면 클라우드 API 없이도 강력한 LLM을 로컬에서 활용할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}