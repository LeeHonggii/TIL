{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95259f7a",
   "metadata": {},
   "source": [
    "# 파이토치(PyTorch)로 딥러닝 첫걸음\n",
    "\n",
    "이 노트북에서는 파이토치(PyTorch)를 활용하여 딥러닝의 핵심 개념과 실습 방법을 학습합니다. 파이토치는 연구와 실무 모두에서 널리 사용되는 딥러닝 프레임워크로, 텐서(Tensor) 연산, 데이터 전처리, 모델 설계 및 학습 등 다양한 기능을 제공합니다.\n",
    "\n",
    "## 학습 목표\n",
    "- 파이토치의 주요 구성 요소(텐서, 데이터셋, 모델 등)를 이해하고 활용할 수 있다.\n",
    "- 텐서의 구조와 연산 방법을 실제 코드로 구현해본다.\n",
    "- 파이토치 환경을 직접 설치하고, 실습을 위한 준비를 마친다.\n",
    "- 파이토치의 데이터 처리 및 모델 빌드의 기본 원리를 익힌다.\n",
    "\n",
    "## 전체 흐름 소개\n",
    "1. 파이토치의 필요성과 주요 기능을 이해합니다.\n",
    "2. 텐서의 구조와 연산을 실습합니다.\n",
    "3. 파이토치 설치 방법을 익히고 실습 환경을 준비합니다.\n",
    "4. 이후 단계에서는 데이터셋, 모델 빌드, 학습 및 추론 등 딥러닝의 전체 흐름을 따라가며 실습을 진행할 예정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543b414c",
   "metadata": {},
   "source": [
    "## 왜 파이토치(PyTorch)가 필요한가?\n",
    "\n",
    "딥러닝 모델은 수많은 데이터와 파라미터를 다루며, 이 과정에서 대규모 행렬(텐서) 연산이 필수적입니다. 파이토치는 다음과 같은 이유로 널리 사용됩니다:\n",
    "\n",
    "- **고성능 텐서 연산**: CPU와 GPU 모두에서 대규모 연산을 빠르고 효율적으로 처리할 수 있습니다.\n",
    "- **편리한 데이터 처리**: 데이터셋(Dataset)과 데이터로더(DataLoader)를 통해 대용량 데이터를 쉽게 다루고, 미니배치 학습이 가능합니다.\n",
    "- **유연한 모델 설계**: 자주 쓰는 레이어를 간단히 불러오거나, 직접 커스텀 레이어를 만들어 다양한 모델을 설계할 수 있습니다.\n",
    "- **자동 미분(Autograd)과 학습 지원**: 복잡한 역전파(Backpropagation)를 자동으로 처리하며, 다양한 옵티마이저(Optimizer)와 학습 도구를 제공합니다.\n",
    "- **모델 추론 및 평가**: 학습된 모델의 예측, 평가 지표 계산 등 실전 활용에 필요한 기능을 제공합니다.\n",
    "\n",
    "이제 파이토치의 가장 기본이 되는 텐서(Tensor) 연산부터 자세히 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c87669a",
   "metadata": {},
   "source": [
    "## 텐서(Tensor) 연산이란?\n",
    "\n",
    "딥러닝에서 텐서는 데이터를 표현하는 기본 단위입니다. 텐서는 스칼라(0차원), 벡터(1차원), 행렬(2차원), 그리고 그 이상의 고차원 배열까지 모두 포함하는 개념입니다. 왜 텐서 연산이 중요한지 살펴보면:\n",
    "\n",
    "- **효율적 데이터 표현**: 이미지, 텍스트, 시계열 등 다양한 데이터를 텐서로 표현할 수 있습니다.\n",
    "- **병렬 연산 최적화**: 텐서 연산은 GPU에서 병렬로 처리되어 대규모 데이터 학습이 가능합니다.\n",
    "- **모델 파라미터 관리**: 딥러닝 모델의 가중치와 편향 등 모든 파라미터도 텐서로 저장되고 연산됩니다.\n",
    "\n",
    "이제 실제로 텐서의 구조(Shape)와 차원에 대해 예시를 통해 알아보겠습니다. 다음 코드 셀에서는 다양한 차원의 텐서가 어떻게 표현되는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a4f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2행 3열의 2차원 텐서(행렬)를 예로 들어봅니다.\n",
    "# 이 텐서는 2개의 행(row)과 3개의 열(column)로 구성되어 있습니다.\n",
    "tensor_shape = (2, 3)  # (행, 열) 형태로 텐서의 크기를 정의\n",
    "print(tensor_shape)  # 텐서의 shape(모양)을 출력\n",
    "\n",
    "# 다음: 3차원 텐서(예: 컬러 이미지 등)의 구조를 살펴봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70036f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 면의 길이가 3인 3차원 텐서(정육면체 형태)를 예로 들어봅니다.\n",
    "# 예를 들어, (채널, 높이, 너비) 구조의 컬러 이미지 데이터 등에서 사용됩니다.\n",
    "tensor_shape_3d = (3, 3, 3)  # 3차원 텐서의 크기 정의\n",
    "print(tensor_shape_3d)  # 텐서의 shape(모양)을 출력\n",
    "\n",
    "# 다음: 파이토치 설치 방법을 안내합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a276eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치(PyTorch) 설치 명령어입니다.\n",
    "# Colab, Jupyter 등 환경에서 실행하여 PyTorch를 설치할 수 있습니다.\n",
    "!pip install torch==2.8.0  # torch의 특정 버전을 설치\n",
    "\n",
    "# 다음: 파이토치가 정상적으로 설치되었는지 확인하는 방법을 안내합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e1a78c",
   "metadata": {},
   "source": [
    "## 파이토치(PyTorch) 시작하기\n",
    "\n",
    "딥러닝을 구현할 때 파이토치(PyTorch)는 매우 널리 사용되는 프레임워크입니다. 파이토치는 유연한 텐서(Tensor) 연산과 직관적인 신경망 모델링 기능을 제공합니다.\n",
    "\n",
    "### 왜 파이토치를 사용할까요?\n",
    "- **동적 계산 그래프**: 실행 시점에 그래프가 만들어져, 디버깅과 실험이 쉽습니다.\n",
    "- **직관적인 문법**: 파이썬과 유사한 문법으로 빠르게 코드를 작성할 수 있습니다.\n",
    "- **강력한 GPU 지원**: 대규모 데이터와 복잡한 모델도 빠르게 처리할 수 있습니다.\n",
    "\n",
    "이제 파이토치를 불러오고, 텐서(Tensor)를 다루는 기본적인 방법부터 살펴보겠습니다.\n",
    "\n",
    "다음 코드 셀에서는 파이토치 모듈을 임포트하고, 텐서를 생성하는 기본적인 예시를 실습해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치 모듈을 임포트합니다\n",
    "import torch  # torch는 파이토치의 핵심 패키지입니다\n",
    "\n",
    "# 다음: 다양한 방식으로 텐서를 생성하는 방법을 실습합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0fb37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1차원 텐서(Tensor) 생성 예시입니다\n",
    "# torch.tensor() 함수는 파이썬 리스트나 넘파이 배열로부터 텐서를 만듭니다\n",
    "tensor1 = torch.tensor([1, 2, 3])  # 정수 1, 2, 3으로 구성된 텐서 생성\n",
    "print(tensor1)  # 생성된 텐서를 출력합니다\n",
    "\n",
    "# 다음: 다양한 초기값을 갖는 텐서(0, 1, 랜덤, 특정값) 생성 방법을 알아봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7072c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 초기값을 갖는 텐서 생성 방법을 살펴봅니다\n",
    "\n",
    "# 0으로 채운 3x3 텐서 생성\n",
    "zero_tensor = torch.zeros(3, 3)  # 모든 원소가 0인 3행 3열 텐서\n",
    "print(zero_tensor)  # 0으로 채워진 텐서 출력\n",
    "\n",
    "# 1로 채운 3x3 텐서 생성\n",
    "ones_tensor = torch.ones(3, 3)  # 모든 원소가 1인 3행 3열 텐서\n",
    "print(ones_tensor)  # 1로 채워진 텐서 출력\n",
    "\n",
    "# 0~1 사이의 난수로 채운 3x3 텐서 생성\n",
    "random_tensor = torch.rand(3, 3)  # 각 원소가 0~1 사이의 랜덤값\n",
    "print(random_tensor)  # 랜덤값으로 채워진 텐서 출력\n",
    "\n",
    "# 모든 원소가 -0.7인 3x3 텐서 생성\n",
    "target_value = -0.7  # 특정 값 지정\n",
    "float_tensor = torch.full((3, 3), target_value)  # 지정한 값으로 채움\n",
    "print(float_tensor)  # 특정 값으로 채워진 텐서 출력\n",
    "\n",
    "# 다음: 파이토치에서 신경망 모델을 만드는 두 가지 방법(Sequential, Custom)에 대해 이론적으로 살펴봅니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c76633",
   "metadata": {},
   "source": [
    "## 파이토치에서 신경망 모델 만들기\n",
    "\n",
    "딥러닝 모델을 구현할 때, 파이토치는 두 가지 주요 방식을 제공합니다.\n",
    "\n",
    "- **Sequential API(순차적 API)**: 여러 층을 순서대로 쌓는 간단한 구조에 적합합니다. 복잡한 연결이나 조건이 필요 없는 경우에 유용합니다.\n",
    "- **Custom API(사용자 정의 API)**: nn.Module을 상속받아 직접 클래스를 정의합니다. 복잡한 구조, 다양한 입력/출력, 조건부 연산 등이 필요한 경우에 사용합니다.\n",
    "\n",
    "이 두 방식은 각각 장단점이 있으므로, 문제의 복잡도와 목적에 따라 적절히 선택해야 합니다.\n",
    "\n",
    "이제 먼저 Sequential API의 개념과 특징을 자세히 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a7064",
   "metadata": {},
   "source": [
    "### Sequential API란 무엇인가?\n",
    "\n",
    "Sequential API는 신경망의 각 층(Layer)을 순서대로 쌓아올리는 방식입니다. 이 방식은 다음과 같은 상황에서 매우 유용합니다:\n",
    "\n",
    "- **구조가 단순한 모델**: 입력 → 은닉층 → 출력층처럼, 층이 일렬로 연결될 때 적합합니다.\n",
    "- **빠른 프로토타이핑**: 복잡한 커스텀 로직이 필요 없을 때, 빠르게 실험할 수 있습니다.\n",
    "\n",
    "#### 왜 Sequential API가 필요할까요?\n",
    "복잡한 신경망을 처음부터 클래스로 정의하는 것은 번거롭고, 실험 속도도 느려질 수 있습니다. Sequential API를 사용하면, 단 몇 줄의 코드로 신경망 구조를 쉽게 정의하고, 다양한 조합을 빠르게 테스트할 수 있습니다.\n",
    "\n",
    "> 예를 들어, 입력층-은닉층-출력층으로 이어지는 다층 퍼셉트론(MLP, Multi-Layer Perceptron) 구조를 손쉽게 구현할 수 있습니다.\n",
    "\n",
    "이제 다음 코드 셀에서는 실제로 Sequential API를 사용해 간단한 신경망 모델을 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331ac14",
   "metadata": {},
   "source": [
    "## 파이토치(Pytorch) 모델 구성의 실제 예시\n",
    "\n",
    "앞서 파이토치의 신경망 모듈을 활용해 모델을 만드는 기본적인 방법을 살펴보았습니다. 이제 실제로 `nn.Sequential`(순차 컨테이너)을 이용해 간단한 신경망 모델을 정의하는 과정을 구체적으로 살펴보겠습니다.\n",
    "\n",
    "### 왜 nn.Sequential을 사용할까?\n",
    "- **빠른 프로토타이핑**: 여러 층을 순서대로 쌓는 단순한 구조의 모델을 빠르게 만들 수 있습니다.\n",
    "- **코드 간결성**: 각 층을 일일이 변수로 선언하지 않고, 리스트처럼 나열만 하면 되므로 코드가 짧아집니다.\n",
    "- **실험의 용이성**: 여러 층을 쉽게 추가/삭제하며 실험할 수 있습니다.\n",
    "\n",
    "이제 실제로 입력 차원 10, 은닉층 5, 출력층 2로 구성된 신경망을 만들어보겠습니다. 다음 코드 셀에서는 각 층의 역할과 함께 모델을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be349796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치 신경망 모듈(nn.Module)에서 Sequential(순차 컨테이너)를 사용하여 모델을 정의합니다.\n",
    "import torch.nn as nn  # 신경망 관련 모듈 임포트\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 5),   # 입력층: 입력 10차원 → 은닉층 5차원으로 변환 (가중치 10x5 + 편향 5)\n",
    "    nn.ReLU(),          # 활성화 함수: ReLU(Rectified Linear Unit)로 비선형성 부여\n",
    "    nn.Linear(5, 2)     # 출력층: 은닉층 5차원 → 출력 2차원으로 변환\n",
    ")\n",
    "\n",
    "# 다음 단계: 모델 구조를 시각적으로 요약해주는 torch-summary 패키지를 설치하고, 모델의 세부 구조를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c1c59a",
   "metadata": {},
   "source": [
    "### 모델 구조 요약을 위한 도구 설치\n",
    "\n",
    "신경망을 설계할 때, 각 층의 파라미터(매개변수) 수, 출력 크기 등을 한눈에 파악하는 것이 매우 중요합니다. 이를 위해 `torch-summary`라는 외부 패키지를 활용할 수 있습니다.\n",
    "\n",
    "- **torch-summary란?**\n",
    "  - 케라스(Keras)의 `model.summary()`와 유사하게, 파이토치 모델의 구조와 파라미터 개수를 표 형태로 보여줍니다.\n",
    "  - 복잡한 모델일수록 구조를 빠르게 파악하는 데 큰 도움이 됩니다.\n",
    "\n",
    "이제 다음 코드 셀에서 해당 패키지를 설치해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ff82f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-summary  # torch-summary 패키지 설치 (모델 구조 요약용)\n",
    "\n",
    "# 다음 단계: 설치한 torch-summary를 이용해 방금 만든 모델의 구조와 각 층의 파라미터 수를 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc7c481",
   "metadata": {},
   "source": [
    "### torch-summary로 모델 구조 확인하기\n",
    "\n",
    "패키지 설치가 끝났다면, 이제 실제로 `summary` 함수를 이용해 모델의 구조를 출력해보겠습니다.\n",
    "\n",
    "- **summary 함수의 역할**\n",
    "  - 각 층의 이름, 출력 크기, 파라미터 개수 등을 표로 정리해줍니다.\n",
    "  - 입력 데이터의 크기를 지정해주면, 각 층을 통과할 때 데이터의 크기가 어떻게 변하는지 알 수 있습니다.\n",
    "\n",
    "이제 다음 코드 셀에서 `summary` 함수를 사용해 모델 구조를 살펴봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c137444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary  # summary 함수 임포트\n",
    "\n",
    "# summary 함수로 모델 구조를 출력합니다.\n",
    "# input_size는 배치 크기를 제외한 입력 데이터의 크기입니다. (여기서는 1x10)\n",
    "summary(model, input_size=(1, 10))\n",
    "\n",
    "# 출력 결과:\n",
    "# - 각 층의 이름과 출력 크기, 파라미터(가중치+편향) 개수가 표로 나타납니다.\n",
    "# 다음 단계: nn.Module을 상속받아 직접 커스텀 신경망 클래스를 만드는 방법을 알아봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f2cbb0",
   "metadata": {},
   "source": [
    "## nn.Module을 활용한 커스텀 신경망 모델 설계\n",
    "\n",
    "지금까지는 `nn.Sequential`을 사용해 간단한 모델을 만들었습니다. 하지만 실제 연구나 실무에서는 더 복잡한 구조(예: 여러 입력, 조건부 분기, 다양한 연산 등)가 필요할 수 있습니다.\n",
    "\n",
    "이럴 때는 파이토치의 모든 신경망의 기반이 되는 `nn.Module`(모듈) 클래스를 상속받아 직접 클래스를 정의해야 합니다.\n",
    "\n",
    "### 왜 nn.Module을 직접 상속해서 모델을 만들까?\n",
    "- **유연성**: 복잡한 구조, 다양한 연산, 조건문, 반복문 등 자유롭게 구현 가능\n",
    "- **재사용성**: 여러 층을 모듈화하여 다른 모델에서도 재사용할 수 있음\n",
    "- **확장성**: 새로운 층이나 연산을 직접 만들어 추가할 수 있음\n",
    "\n",
    "이제 다음 코드 셀에서는 nn.Module을 상속받아 직접 신경망 클래스를 만드는 기본 구조를 예시로 보여드리겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a4f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # 텐서 연산을 위한 torch 임포트\n",
    "import torch.nn as nn  # 신경망 모듈 임포트\n",
    "\n",
    "# nn.Module을 상속받아 커스텀 신경망 클래스를 정의합니다.\n",
    "class CustomNet(nn.Module):  # 클래스 이름은 자유롭게 지정 가능\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):  # 생성자: 입력, 은닉, 출력 차원 지정\n",
    "        super().__init__()  # 부모 클래스 초기화 (필수)\n",
    "        # 입력층 정의: 입력 차원 → 은닉층 차원\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        # 은닉층 정의: 은닉층 차원 → 은닉층 차원 (예시)\n",
    "        self.hidden_layer = nn.ReLU()  # 활성화 함수 (비선형성)\n",
    "        # 출력층 정의: 은닉층 차원 → 출력 차원\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):  # 순전파 함수: 입력 x가 들어왔을 때 연산 정의\n",
    "        x = self.input_layer(x)      # 입력층 통과\n",
    "        x = self.hidden_layer(x)     # 은닉층(활성화 함수) 통과\n",
    "        x = self.output_layer(x)     # 출력층 통과\n",
    "        return x  # 최종 결과 반환\n",
    "\n",
    "# 위 구조는 입력, 은닉, 출력층을 가진 기본적인 신경망의 뼈대입니다.\n",
    "# 다음 단계: 실제로 이 클래스를 인스턴스화하고, 임의의 입력 데이터를 넣어 결과를 확인해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd25297",
   "metadata": {},
   "source": [
    "## 모델 구성 요소와 추론(순전파) 과정의 자동화\n",
    "\n",
    "딥러닝 프레임워크(예: PyTorch)는 모델의 각 구성 요소(레이어, 활성화 함수 등)와 순전파(forward) 과정을 정의하기만 하면, 내부적으로 계산 그래프(computational graph)를 자동으로 생성합니다. 이 그래프는 입력 데이터가 모델을 통과할 때 각 연산의 흐름을 추적하며, 이후 역전파(backpropagation) 시 자동 미분(autograd)을 통해 파라미터의 기울기를 계산할 수 있게 해줍니다.\n",
    "\n",
    "### 왜 이런 구조가 필요한가?\n",
    "- **복잡한 모델 구조**: 수많은 레이어와 연산이 연결된 복잡한 신경망에서, 수동으로 미분을 계산하는 것은 비효율적이고 오류가 많을 수 있습니다.\n",
    "- **유연한 실험**: 다양한 모델 구조를 쉽게 실험할 수 있으며, 순전파만 정의하면 역전파는 자동으로 처리됩니다.\n",
    "\n",
    "이제 실제로 PyTorch에서 신경망 클래스를 어떻게 정의하는지 살펴보겠습니다. 다음 코드 셀에서는 간단한 다층 퍼셉트론(Multilayer Perceptron, MLP) 구조를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a2dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망(Neural Network) 클래스를 정의합니다\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # nn.Module의 초기화 메서드 호출\n",
    "        # 입력 이미지를 1차원 벡터로 변환 (28x28 이미지를 784차원 벡터로)\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 선형(Linear) 레이어와 ReLU 활성화 함수를 쌓은 구조 정의\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),  # 첫 번째 은닉층: 입력 784 → 512\n",
    "            nn.ReLU(),              # 비선형성 추가\n",
    "            nn.Linear(512, 512),    # 두 번째 은닉층: 512 → 512\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),     # 출력층: 512 → 10 (클래스 개수)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력 x를 1차원으로 평탄화\n",
    "        x = self.flatten(x)\n",
    "        # 선형-활성화 스택을 통과시켜 최종 출력(logits) 계산\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits  # 각 클래스별 점수 반환\n",
    "\n",
    "# 다음: 자연어 처리에서 자주 사용하는 pad_sequence와 임베딩 레이어의 실제 사용 사례를 살펴봅니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02a4d7e",
   "metadata": {},
   "source": [
    "## Pad_Sequence와 임베딩(Embedding) 레이어의 실제 활용\n",
    "\n",
    "자연어 처리(NLP)에서는 문장마다 단어(토큰) 개수가 다르기 때문에, 딥러닝 모델에 입력하기 전에 길이를 맞춰주는 작업이 필요합니다. 이때 사용하는 것이 **pad_sequence** 함수입니다. 또한, 각 단어를 고정된 차원의 벡터로 변환하는 **임베딩(Embedding) 레이어**도 필수적입니다.\n",
    "\n",
    "### 왜 pad_sequence와 임베딩이 필요한가?\n",
    "- **pad_sequence**: 미니배치 학습을 위해 입력 데이터의 크기를 통일해야 합니다. 길이가 짧은 문장은 패딩(padding) 토큰(예: 0)으로 채워집니다.\n",
    "- **임베딩 레이어**: 단어를 고정된 크기의 실수 벡터로 변환하여, 신경망이 단어 간 의미적 유사성을 학습할 수 있게 합니다.\n",
    "\n",
    "이제 pad_sequence와 임베딩 레이어를 실제로 어떻게 사용하는지 코드로 살펴보겠습니다. 먼저, 간단한 문장 리스트를 토큰화하고, 인덱스 시퀀스로 변환한 뒤, pad_sequence로 길이를 맞추는 과정을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed32601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 예시 문장 리스트 정의\n",
    "sentences = [\n",
    "    \"I love banana.\",\n",
    "    \"I also love NLP\",\n",
    "    \"NLP is interesting\",\n",
    "    \"I am studing NLP\",\n",
    "    \"NLP is fun\"\n",
    "]\n",
    "\n",
    "# 각 문장을 공백 기준으로 토큰화\n",
    "tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "\n",
    "# 단어 사전(vocab) 생성: 각 단어에 고유 인덱스 부여 (0은 패딩 전용)\n",
    "vocab = {}\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab) + 1  # 0은 패딩, 1부터 단어 인덱스 할당\n",
    "\n",
    "# 각 문장을 인덱스 시퀀스로 변환\n",
    "indexed_sentences = [[vocab[word] for word in sentence] for sentence in tokenized_sentences]\n",
    "\n",
    "print(\"Indexed Sentences:\", indexed_sentences)\n",
    "\n",
    "# 인덱스 시퀀스를 LongTensor로 변환\n",
    "tensor_sentences = [torch.tensor(sentence, dtype=torch.long) for sentence in indexed_sentences]\n",
    "\n",
    "# pad_sequence: 문장 길이를 맞추기 위해 패딩(0) 추가\n",
    "padded_sentences = pad_sequence(tensor_sentences, batch_first=True, padding_value=0)\n",
    "\n",
    "print(\"Padded Sentences Tensor:\")\n",
    "print(padded_sentences)\n",
    "\n",
    "# 다음: pad_sequence로 패딩된 문장에 임베딩 레이어를 적용하여, 각 단어를 고정된 벡터로 변환하는 과정을 살펴봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 8  # 임베딩 벡터의 차원 수 (각 단어를 8차원 벡터로 표현)\n",
    "vocab_size = len(vocab) + 1  # 단어 사전 크기 (패딩 토큰 0 포함)\n",
    "\n",
    "# 임베딩 레이어 생성: 각 단어 인덱스를 embedding_dim 차원의 벡터로 변환\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)  # padding_idx=0은 패딩 토큰 무시\n",
    "\n",
    "# 패딩된 문장 텐서를 임베딩 레이어에 통과시켜 임베딩 벡터로 변환\n",
    "embedded_sentences = embedding_layer(padded_sentences)\n",
    "\n",
    "print(\"Embedded Sentences Tensor:\")\n",
    "print(embedded_sentences.shape)  # (배치 크기, 문장 길이, 임베딩 차원)\n",
    "print(embedded_sentences)\n",
    "\n",
    "# 다음: 임베딩 벡터를 활용하여 문장 표현을 만드는 방법(예: 평균, 합, RNN 입력 등)을 알아봅니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ef174",
   "metadata": {},
   "source": [
    "## Optimizer(최적화 함수)\n",
    "\n",
    "머신러닝과 딥러닝 모델을 학습할 때, 모델의 파라미터(가중치와 바이어스)를 어떻게 조정할지 결정하는 것이 매우 중요합니다. 이때 사용하는 것이 바로 **최적화 함수(Optimizer)**입니다. \n",
    "\n",
    "### 왜 Optimizer가 필요한가?\n",
    "- 모델이 예측한 값과 실제 값의 차이(손실, Loss)를 줄이기 위해, 파라미터를 반복적으로 업데이트해야 합니다.\n",
    "- 이때, 손실 함수의 값을 최소화하는 방향으로 파라미터를 조정하는 알고리즘이 Optimizer입니다.\n",
    "- 대표적으로 경사 하강법(Gradient Descent)과 그 변형들이 널리 사용됩니다.\n",
    "\n",
    "### 이번 셀에서 할 일\n",
    "- 간단한 함수의 최솟값을 찾기 위해 경사 하강법(Gradient Descent)이 어떻게 작동하는지 시각적으로 살펴보겠습니다.\n",
    "- 이를 통해 Optimizer의 원리를 직관적으로 이해할 수 있습니다.\n",
    "\n",
    "이제, 경사 하강법을 이용해 함수의 최소값을 찾아가는 과정을 Python 코드로 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74b1562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import numpy as np  # 수치 계산을 위한 넘파이\n",
    "import matplotlib.pyplot as plt  # 시각화를 위한 matplotlib\n",
    "from IPython.display import clear_output  # 반복 시각화에서 이전 그림 지우기\n",
    "\n",
    "# 최적화할 함수 정의: f(x) = x^4 - 3x^3\n",
    "def f(x):\n",
    "    return x**4 - 3 * x**3\n",
    "\n",
    "# 함수의 도함수(기울기) 정의: f'(x) = 4x^3 - 9x^2\n",
    "def f_prime(x):\n",
    "    return 4 * x**3 - 9 * x**2\n",
    "\n",
    "# 경사 하강법 초기값 설정\n",
    "x_old = 0  # 이전 x값 (초기값, 비교용)\n",
    "x_new = 6  # 시작점 (최적화 시작 위치)\n",
    "eps = 0.01  # 학습률(learning rate): 한 번에 이동하는 크기\n",
    "precision = 0.00001  # 반복 종료 조건: x값 변화가 이보다 작으면 종료\n",
    "\n",
    "# 시각화를 위한 x, y 값 생성\n",
    "x = np.linspace(-1, 4, 400)  # -1부터 4까지 400개 점 생성\n",
    "y = f(x)  # 각 x에 대한 함수값 계산\n",
    "\n",
    "# 경사 하강법 반복문: 최소값을 찾을 때까지 실행\n",
    "while abs(x_new - x_old) > precision:\n",
    "    x_old = x_new  # 이전 x값 저장\n",
    "    # 경사 하강법 공식: x_new = x_old - 학습률 * 기울기\n",
    "    x_new = x_old - eps * f_prime(x_old)\n",
    "    \n",
    "    # 반복 시각화를 위해 이전 그림 삭제\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # 함수 그래프와 현재 위치 시각화\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x, y, label=r\"$f(x) = x^4 - 3x^3$\")  # 함수 그래프\n",
    "    plt.scatter(x_new, f(x_new), color=\"red\", label=rf\"Current x = {x_new:.4f}\")  # 현재 x 위치\n",
    "    plt.title(r\"Gradient Descent Optimization of $f(x) = x^4 - 3x^3$\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(r\"$f(x)$\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# 다음: 실제 딥러닝에서 손실 함수(Loss Function)가 왜 중요한지, 그리고 대표적인 손실 함수의 예시를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1104e05c",
   "metadata": {},
   "source": [
    "## Loss Function(손실 함수)\n",
    "\n",
    "모델이 학습을 잘 하고 있는지, 즉 예측이 얼마나 실제 값과 가까운지를 수치로 평가해야 합니다. 이때 사용하는 것이 바로 **손실 함수(Loss Function)**입니다.\n",
    "\n",
    "### 왜 손실 함수가 중요한가?\n",
    "- 손실 함수는 모델의 예측값과 실제 값 사이의 차이를 수치로 나타냅니다.\n",
    "- 이 값이 작을수록 모델의 예측이 실제와 더 가깝다는 의미입니다.\n",
    "- Optimizer(최적화 함수)는 이 손실 값을 최소화하는 방향으로 파라미터를 업데이트합니다.\n",
    "- 이 과정에서 손실 함수의 미분값(기울기)을 사용하여, 파라미터를 얼마나, 어떤 방향으로 조정할지 결정합니다. 이 전체 과정을 **역전파(Backpropagation)**라고 부릅니다.\n",
    "\n",
    "### 다음 단계 안내\n",
    "- 이제 대표적인 손실 함수 중 하나인 MSE(평균 제곱 오차, Mean Squared Error)를 회귀 문제에 적용하는 예시를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff9c39",
   "metadata": {},
   "source": [
    "### 회귀(Regression) 문제에서의 손실 함수: MSE(평균 제곱 오차)\n",
    "\n",
    "회귀 문제에서는 모델이 연속적인 값을 예측합니다. 이때, 예측값과 실제 값의 차이를 평가하는 대표적인 손실 함수가 **MSE(Mean Squared Error, 평균 제곱 오차)**입니다.\n",
    "\n",
    "#### MSE의 원리\n",
    "- 각 데이터 샘플에 대해 예측값과 실제 값의 차이를 구합니다.\n",
    "- 이 차이를 제곱하여, 음수와 양수의 차이가 상쇄되는 것을 방지합니다.\n",
    "- 모든 데이터 샘플에 대해 제곱 오차를 평균내어 전체 손실 값을 계산합니다.\n",
    "\n",
    "MSE는 값이 클수록 예측이 실제와 많이 다르다는 것을 의미하며, Optimizer는 이 값을 줄이도록 파라미터를 조정합니다.\n",
    "\n",
    "이제 PyTorch를 사용하여 MSE 손실 함수를 계산하는 예시 코드를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee33922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch를 사용하여 MSE(평균 제곱 오차) 손실 함수 계산 예시\n",
    "import torch  # 파이토치 임포트\n",
    "import torch.nn as nn  # 신경망 관련 모듈 임포트\n",
    "\n",
    "# 예측값 텐서 (예: 모델이 예측한 값)\n",
    "prediction = torch.tensor([[2.0], [3.0], [4.0]])\n",
    "# 실제값 텐서 (예: 정답 레이블)\n",
    "ground_truth = torch.tensor([[2.5], [3.5], [5.0]])\n",
    "\n",
    "# MSE 손실 함수 객체 생성\n",
    "criterion = nn.MSELoss()\n",
    "# 예측값과 실제값 사이의 손실 값 계산\n",
    "loss = criterion(prediction, ground_truth)\n",
    "\n",
    "# 손실 값 출력\n",
    "print(f'손실 값: {loss.item()}')  # .item()은 파이토치 텐서를 파이썬 숫자로 변환\n",
    "\n",
    "# 다음: 분류(Classification) 문제에서 자주 사용하는 손실 함수(Cross Entropy 등)도 살펴볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abe7d5d",
   "metadata": {},
   "source": [
    "## 분류(Classification) 문제란 무엇이며, 왜 중요한가?\n",
    "\n",
    "머신러닝과 딥러닝에서 **분류(Classification)** 문제는 매우 널리 사용되는 기본 과제입니다. 분류란, 주어진 입력 데이터(예: 이미지, 텍스트 등)가 미리 정의된 여러 클래스(범주) 중 어디에 속하는지 예측하는 작업입니다.\n",
    "\n",
    "### 왜 분류가 중요한가?\n",
    "- **현실 세계 문제 대부분이 분류로 귀결**: 예를 들어, 이메일이 스팸인지 아닌지, 사진 속 동물이 강아지인지 고양이인지, 환자의 진단 결과가 양성인지 음성인지 등 다양한 문제를 분류로 모델링할 수 있습니다.\n",
    "- **확률 예측의 의미**: 분류 모델은 각 클래스에 속할 확률을 출력합니다. 이 확률은 모델의 '확신' 정도를 나타내며, 실제로는 이 확률을 바탕으로 손실 함수(loss function)를 계산하여 모델을 학습시킵니다.\n",
    "- **손실 함수의 역할**: 예측 확률과 실제 정답(레이블) 사이의 차이를 수치적으로 측정하여, 모델이 점점 더 정확한 예측을 하도록 만듭니다.\n",
    "\n",
    "### 예시\n",
    "예를 들어, 강아지와 고양이 이미지를 분류하는 문제에서는, 모델이 각 이미지에 대해 '강아지일 확률', '고양이일 확률'을 각각 출력합니다. 이 확률값을 이용해 실제 정답과의 차이를 계산하고, 이를 줄이도록 모델을 학습합니다.\n",
    "\n",
    "이제 실제로 분류 문제에서 손실 함수를 어떻게 계산하는지 예시 코드를 통해 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch(파이토치) 라이브러리 임포트\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 예시 예측값: 각 샘플에 대해 [강아지일 확률, 고양이일 확률]을 의미\n",
    "# 3개의 샘플에 대해 예측값을 준비합니다.\n",
    "predictions = torch.tensor([[0.8, 0.2],   # 첫 번째 샘플: 강아지 80%, 고양이 20%\n",
    "                           [0.4, 0.6],   # 두 번째 샘플: 강아지 40%, 고양이 60%\n",
    "                           [0.9, 0.1]])  # 세 번째 샘플: 강아지 90%, 고양이 10%\n",
    "\n",
    "# 실제 레이블(정답): 0은 강아지, 1은 고양이\n",
    "labels = torch.tensor([0, 1, 0])\n",
    "\n",
    "# 손실 함수로 CrossEntropyLoss(교차 엔트로피 손실) 사용\n",
    "# 이 함수는 분류 문제에서 가장 널리 사용되는 손실 함수입니다.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 손실 값 계산\n",
    "loss = criterion(predictions, labels)\n",
    "\n",
    "# 손실 값 출력 (값이 작을수록 모델의 예측이 정답에 가까움을 의미)\n",
    "print(f'손실 값: {loss.item()}')\n",
    "\n",
    "# 다음: 실제 분류 모델(VGGNet16)을 직접 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475153cd",
   "metadata": {},
   "source": [
    "## VGGNet16 구조 구현: 왜 이렇게 설계되었는가?\n",
    "\n",
    "딥러닝에서 이미지 분류를 위한 대표적인 모델 중 하나가 **VGGNet16**입니다. VGGNet16은 2014년 ILSVRC 대회에서 제안된 구조로, 다음과 같은 특징이 있습니다:\n",
    "\n",
    "- **단순한 구조**: 3x3의 작은 커널(kernel)과 2x2의 맥스풀링(max pooling)만을 반복적으로 사용합니다.\n",
    "- **깊은 네트워크**: 16개의 계층(컨볼루션+풀링+완전연결)으로 구성되어, 깊이가 깊을수록 더 복잡한 패턴을 학습할 수 있습니다.\n",
    "- **계층적 특징 추출**: 처음에는 저수준(엣지, 색상 등) 특징을, 깊은 층에서는 고수준(형태, 객체 등) 특징을 추출합니다.\n",
    "\n",
    "### 왜 VGGNet16 구조를 직접 구현해보는가?\n",
    "- **네트워크 설계 원리 이해**: 직접 구현함으로써 각 계층의 역할과 파라미터 설정의 의미를 배울 수 있습니다.\n",
    "- **블록 구조의 반복성**: VGG는 동일한 구조의 블록이 반복되는 전형적인 CNN 구조이므로, 딥러닝 모델 설계의 기본기를 익히기에 적합합니다.\n",
    "\n",
    "이제 VGGNet16의 주요 계층(컨볼루션, ReLU, 맥스풀링, 완전연결)을 모두 포함한 전체 구조를 PyTorch로 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879c1fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# VGGNet16 모델 클래스 정의\n",
    "class VGGNet16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 첫 번째 블록: 입력 채널 3(컬러 이미지), 출력 채널 64\n",
    "        self.conv1_1 = nn.Conv2d(\n",
    "            in_channels=3,      # 입력 채널: RGB 3채널\n",
    "            out_channels=64,    # 출력 채널: 64개 특징맵\n",
    "            kernel_size=3,      # 커널 크기: 3x3\n",
    "            padding=1           # 입력과 출력 크기 동일하게 유지\n",
    "        )\n",
    "        self.relu1_1 = nn.ReLU() # 비선형 활성화 함수\n",
    "        self.conv1_2 = nn.Conv2d(\n",
    "            in_channels=64,      # 이전 층 출력 채널\n",
    "            out_channels=64,    # 출력 채널: 64\n",
    "            kernel_size=3,      # 커널 크기: 3x3\n",
    "            padding=1           # 패딩 적용\n",
    "        )\n",
    "        self.relu1_2 = nn.ReLU()\n",
    "        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2) # 맥스풀링: 크기 절반 감소\n",
    "\n",
    "        # 두 번째 블록: 채널 수 128로 증가\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.relu2_1 = nn.ReLU()\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.relu2_2 = nn.ReLU()\n",
    "        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 세 번째 블록: 채널 수 256\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.relu3_1 = nn.ReLU()\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.relu3_2 = nn.ReLU()\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.relu3_3 = nn.ReLU()\n",
    "        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 네 번째 블록: 채널 수 512\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.relu4_1 = nn.ReLU()\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu4_2 = nn.ReLU()\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu4_3 = nn.ReLU()\n",
    "        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 다섯 번째 블록: 채널 수 512 (네 번째와 동일)\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu5_1 = nn.ReLU()\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu5_2 = nn.ReLU()\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu5_3 = nn.ReLU()\n",
    "        self.Maxpool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 완전연결층(FC): 이미지 특징을 최종 분류로 변환\n",
    "        self.fc1 = nn.Linear(512 * 7 * 7, 4096) # 224x224 입력 기준, 마지막 특징맵 크기 7x7\n",
    "        self.relu_fc1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.relu_fc2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(4096, 1000) # 최종 클래스 수(예: ImageNet 1000개)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 각 블록별로 컨볼루션, 활성화, 풀링 순서로 진행\n",
    "        x = self.relu1_1(self.conv1_1(x))\n",
    "        x = self.relu1_2(self.conv1_2(x))\n",
    "        x = self.Maxpool1(x)\n",
    "\n",
    "        x = self.relu2_1(self.conv2_1(x))\n",
    "        x = self.relu2_2(self.conv2_2(x))\n",
    "        x = self.Maxpool2(x)\n",
    "\n",
    "        x = self.relu3_1(self.conv3_1(x))\n",
    "        x = self.relu3_2(self.conv3_2(x))\n",
    "        x = self.relu3_3(self.conv3_3(x))\n",
    "        x = self.Maxpool3(x)\n",
    "\n",
    "        x = self.relu4_1(self.conv4_1(x))\n",
    "        x = self.relu4_2(self.conv4_2(x))\n",
    "        x = self.relu4_3(self.conv4_3(x))\n",
    "        x = self.Maxpool4(x)\n",
    "\n",
    "        x = self.relu5_1(self.conv5_1(x))\n",
    "        x = self.relu5_2(self.conv5_2(x))\n",
    "        x = self.relu5_3(self.conv5_3(x))\n",
    "        x = self.Maxpool5(x)\n",
    "\n",
    "        # 특징맵을 1차원 벡터로 변환 (Flatten)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu_fc1(self.fc1(x))\n",
    "        x = self.relu_fc2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 다음: 모델 객체 생성 및 손실 함수, 옵티마이저 설정 방법을 살펴봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c767c6",
   "metadata": {},
   "source": [
    "## VGGNet16 모델 객체 생성 및 학습 준비\n",
    "\n",
    "VGGNet16 구조를 정의한 후, 실제로 학습을 진행하려면 다음과 같은 준비가 필요합니다.\n",
    "\n",
    "1. **모델 객체 생성**: 우리가 정의한 VGGNet16 클래스를 인스턴스화합니다.\n",
    "2. **손실 함수(CrossEntropyLoss) 설정**: 분류 문제에서 정답과 예측 확률의 차이를 계산하는 함수입니다.\n",
    "3. **옵티마이저(Optimizer) 설정**: 모델의 파라미터를 업데이트하는 알고리즘입니다. 여기서는 확률적 경사 하강법(SGD, Stochastic Gradient Descent)을 사용합니다.\n",
    "\n",
    "이 과정을 통해 학습에 필요한 기본 환경을 갖추게 됩니다.\n",
    "\n",
    "이제 실제로 코드를 통해 모델, 손실 함수, 옵티마이저를 준비해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ed42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGGNet16 모델 객체 생성\n",
    "model = VGGNet16()\n",
    "\n",
    "# 옵티마이저(Optimizer) 설정: 확률적 경사 하강법(SGD) 사용, 학습률 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 손실 함수(CrossEntropyLoss) 설정: 분류 문제에 적합\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 다음: 모델 구조를 요약(summary)하여 각 계층의 입출력 크기와 파라미터 수를 확인해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f006e272",
   "metadata": {},
   "source": [
    "## 모델 구조 요약(summary) 시 주의점과 오류 해결\n",
    "\n",
    "모델을 구현한 후에는 각 계층의 입출력 크기, 파라미터 수 등을 요약해서 확인하는 것이 매우 중요합니다. 이를 위해 PyTorch에서는 `torchsummary` 패키지의 `summary` 함수를 자주 사용합니다.\n",
    "\n",
    "### 왜 summary가 필요한가?\n",
    "- **구조 검증**: 모델이 의도한 대로 설계되었는지, 각 계층의 입출력 크기가 올바른지 확인할 수 있습니다.\n",
    "- **파라미터 수 파악**: 모델의 복잡도와 학습 가능성을 미리 점검할 수 있습니다.\n",
    "\n",
    "### 오류 발생 원인과 해결 방법\n",
    "- `summary(model, (3, 224, 224))` 실행 시 `NotImplementedError` 또는 기타 오류가 발생할 수 있습니다.\n",
    "- 이는 모델이 GPU에 올라가 있지 않거나, `torchsummary` 패키지가 제대로 설치되지 않았거나, forward 메서드에서 예상치 못한 동작이 있을 때 발생합니다.\n",
    "\n",
    "#### 올바른 사용법\n",
    "1. 모델을 GPU 또는 CPU에 명시적으로 올려야 합니다.\n",
    "2. `torchsummary` 패키지가 설치되어 있어야 합니다.\n",
    "\n",
    "이제 올바른 방법으로 모델 구조를 요약하는 코드를 작성해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchsummary 패키지 임포트 (설치가 안 되어 있다면 먼저 설치 필요)\n",
    "from torchsummary import summary\n",
    "\n",
    "# 모델을 CPU에 올림 (또는 GPU 사용 시 .cuda()로 변경)\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# summary 함수로 모델 구조 요약 출력\n",
    "summary(model, (3, 224, 224))  # 입력 이미지 크기: 3채널(RGB), 224x224\n",
    "\n",
    "# 다음: 실제 데이터로 모델을 학습시키는 과정을 단계별로 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670099b6",
   "metadata": {},
   "source": [
    "## 신경망(Neural Network) 클래스 구현\n",
    "\n",
    "앞서 데이터 전처리와 모델 학습 환경을 준비했습니다. 이제 실제로 이미지를 분류할 신경망(Neural Network) 모델을 직접 구현해보겠습니다.\n",
    "\n",
    "### 왜 직접 신경망 클래스를 구현해야 할까요?\n",
    "- **유연성**: PyTorch의 `nn.Module`을 상속받아 직접 클래스를 정의하면, 다양한 구조의 신경망을 자유롭게 설계할 수 있습니다.\n",
    "- **직관적 구조**: 모델의 각 계층(layer)과 연산 과정을 명확하게 파악할 수 있어, 신경망의 동작 원리를 깊이 이해할 수 있습니다.\n",
    "- **확장성**: 추후에 더 복잡한 모델(예: 합성곱 신경망, 트랜스포머 등)로 확장할 때도 동일한 구조를 사용할 수 있습니다.\n",
    "\n",
    "### 모델 구조 설명\n",
    "- **입력층**: 28x28 크기의 이미지를 1차원 벡터(784차원)로 변환합니다.\n",
    "- **은닉층**: 두 개의 완전연결층(Linear)과 ReLU 활성화 함수로 구성되어 있습니다. 각 은닉층은 512개의 뉴런을 가집니다.\n",
    "- **출력층**: 10개의 클래스(숫자 0~9)에 대한 예측값을 출력합니다.\n",
    "\n",
    "이제 아래 코드에서 실제로 이 신경망 클래스를 구현하는 방법을 살펴보겠습니다.\n",
    "\n",
    "> 다음 코드 셀에서는 `NeuralNetwork` 클래스를 정의하고, 각 계층의 역할과 forward 연산 과정을 자세히 주석으로 설명합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babbeb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch의 nn.Module을 상속받아 신경망(Neural Network) 클래스를 정의합니다\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # 부모 클래스(nn.Module)의 초기화 메서드 호출\n",
    "        # 이미지를 1차원 벡터로 변환하는 계층 (예: 28x28 -> 784)\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 신경망의 주요 계층을 nn.Sequential로 묶어 한 번에 정의\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),  # 첫 번째 완전연결층: 입력 784차원, 출력 512차원\n",
    "            nn.ReLU(),              # 비선형성 추가를 위한 ReLU 활성화 함수\n",
    "            nn.Linear(512, 512),    # 두 번째 완전연결층: 입력 512, 출력 512\n",
    "            nn.ReLU(),              # 다시 ReLU 활성화 함수\n",
    "            nn.Linear(512, 10),     # 출력층: 10개의 클래스(숫자 0~9)로 매핑\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력 이미지를 1차원 벡터로 변환\n",
    "        x = self.flatten(x)\n",
    "        # 순차적으로 정의된 계층을 통과시켜 최종 예측값(logits) 계산\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits  # 각 클래스에 대한 점수(logits) 반환\n",
    "\n",
    "# 다음 단계: 이 신경망 모델을 실제로 생성하고, 입력 데이터를 넣어 예측 결과를 확인해보겠습니다."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
