{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64733c91",
   "metadata": {},
   "source": [
    "# ìì—°ì–´ì²˜ë¦¬(NLP) ì…ë¬¸ê³¼ ì‹¤ìŠµ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ìì—°ì–´ì²˜ë¦¬(Natural Language Processing, NLP)ì˜ í•µì‹¬ ê°œë…ê³¼ ì‹¤ì œ ë°ì´í„° í™œìš© ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ í•™ìŠµí•©ë‹ˆë‹¤. ë‹¨ìˆœíˆ ì´ë¡ ì„ ë‚˜ì—´í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì™œ NLPê°€ ì¤‘ìš”í•œì§€, ì‹¤ì œë¡œ ì–´ë–»ê²Œ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ”ì§€, ê·¸ë¦¬ê³  ê° ë‹¨ê³„ì—ì„œ ì–´ë–¤ ê³ ë¯¼ì´ í•„ìš”í•œì§€ê¹Œì§€ ê¹Šì´ ìˆê²Œ ë‹¤ë£¹ë‹ˆë‹¤.\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- ìì—°ì–´ì²˜ë¦¬(NLP)ì˜ ì£¼ìš” ëª©ì ê³¼ í•„ìš”ì„±ì„ ì´í•´í•œë‹¤.\n",
    "- NLPì˜ ëŒ€í‘œì  ì‘ì—…ì¸ ìì—°ì–´ì´í•´(NLU)ì™€ ìì—°ì–´ìƒì„±(NLG)ì˜ ì°¨ì´ì™€ ì˜ˆì‹œë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.\n",
    "- ì‹¤ì œë¡œ NLP ë°ì´í„°ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° í™˜ê²½ êµ¬ì„±ì„ í•  ìˆ˜ ìˆë‹¤.\n",
    "- ë°ì´í„°ì…‹(datasets) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ì‹¤ì œ ìì—°ì–´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  í™•ì¸í•˜ëŠ” ë°©ë²•ì„ ìµíŒë‹¤.\n",
    "\n",
    "## ì „ì²´ íë¦„ ì†Œê°œ\n",
    "1. **NLPì˜ ê°œë…ê³¼ í•„ìš”ì„±**: ì™œ ìì—°ì–´ì²˜ë¦¬ê°€ ì¤‘ìš”í•œì§€, ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ì§€ ì‚´í´ë´…ë‹ˆë‹¤.\n",
    "2. **NLP ì‘ì—…ì˜ ë¶„ë¥˜**: ìì—°ì–´ì´í•´(NLU)ì™€ ìì—°ì–´ìƒì„±(NLG)ì˜ ì°¨ì´ì™€ ì‹¤ì œ ì ìš© ì˜ˆì‹œë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "3. **ì‹¤ìŠµ í™˜ê²½ ì¤€ë¹„**: ë°ì´í„°ì…‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ë¶ˆëŸ¬ì˜¤ê¸° ê³¼ì •ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤.\n",
    "4. **ë°ì´í„° í™•ì¸**: ì‹¤ì œ ìì—°ì–´ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ë‹¤ë£¨ëŠ”ì§€ ë‹¨ê³„ë³„ë¡œ ì‚´í´ë´…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ ìì—°ì–´ì²˜ë¦¬ì˜ ê¸°ë³¸ ê°œë…ë¶€í„° ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dace20",
   "metadata": {},
   "source": [
    "## ìì—°ì–´ì²˜ë¦¬(NLP)ë€ ë¬´ì—‡ì¸ê°€?\n",
    "\n",
    "ìì—°ì–´ì²˜ë¦¬(Natural Language Processing, NLP)ëŠ” ì¸ê°„ì´ ì¼ìƒì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì–¸ì–´(ìì—°ì–´)ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•˜ê³ , í•´ì„í•˜ë©°, ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì¸ê³µì§€ëŠ¥(AI) ë¶„ì•¼ì…ë‹ˆë‹¤.\n",
    "\n",
    "### ì™œ NLPê°€ í•„ìš”í•œê°€?\n",
    "- ì¸ê°„ì˜ ì–¸ì–´ëŠ” êµ¬ì¡°ê°€ ë³µì¡í•˜ê³ , ë§¥ë½ì— ë”°ë¼ ì˜ë¯¸ê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì»´í“¨í„°ëŠ” ìˆ«ìë‚˜ ëª…í™•í•œ ê·œì¹™ì—ëŠ” ê°•í•˜ì§€ë§Œ, ìì—°ì–´ì²˜ëŸ¼ ëª¨í˜¸í•˜ê³  ë‹¤ì–‘í•œ í‘œí˜„ì„ ì´í•´í•˜ëŠ” ë°ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.\n",
    "- NLPëŠ” ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ì—¬, ê²€ìƒ‰ì—”ì§„, ì±—ë´‡, ë²ˆì—­ê¸°, ê°ì • ë¶„ì„ ë“± ì‹¤ìƒí™œì—ì„œ ë§¤ìš° ë‹¤ì–‘í•œ ì„œë¹„ìŠ¤ì˜ ê¸°ë°˜ì´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ NLPì˜ ëŒ€í‘œì ì¸ ì‘ì—…ë“¤ì„ ë¶„ë¥˜í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26eb5e9",
   "metadata": {},
   "source": [
    "## NLP ì‘ì—…ì˜ ë¶„ë¥˜: NLUì™€ NLG\n",
    "\n",
    "NLPì—ì„œ ìˆ˜í–‰í•˜ëŠ” ì‘ì—…ì€ í¬ê²Œ ë‘ ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- **ìì—°ì–´ì´í•´(NLU, Natural Language Understanding)**: ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì½ê³ , ê·¸ ì˜ë¯¸ì™€ ì˜ë„ë¥¼ íŒŒì•…í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë¬¸ì¥ì˜ ê°ì •ì„ ë¶„ì„í•˜ê±°ë‚˜, ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì°¾ëŠ” ì‘ì—…ì´ ì—¬ê¸°ì— í•´ë‹¹í•©ë‹ˆë‹¤.\n",
    "    - *ì™œ ì¤‘ìš”í•œê°€?* ì»´í“¨í„°ê°€ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ ì œëŒ€ë¡œ ì´í•´í•´ì•¼ë§Œ, ì ì ˆí•œ ë‹µë³€ì´ë‚˜ ì²˜ë¦¬ë¥¼ í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "- **ìì—°ì–´ìƒì„±(NLG, Natural Language Generation)**: ì»´í“¨í„°ê°€ ì˜ë¯¸ ìˆëŠ” ë¬¸ì¥ì´ë‚˜ í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ë§Œë“¤ì–´ë‚´ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë²ˆì—­ê¸°ì—ì„œ ìƒˆë¡œìš´ ì–¸ì–´ë¡œ ë¬¸ì¥ì„ ìƒì„±í•˜ê±°ë‚˜, ì±—ë´‡ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ë§Œë“¤ì–´ë‚´ëŠ” ê²ƒì´ ì—¬ê¸°ì— í•´ë‹¹í•©ë‹ˆë‹¤.\n",
    "    - *ì™œ ì¤‘ìš”í•œê°€?* ì‚¬ìš©ìê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì–¸ì–´ë¡œ ì •ë³´ë¥¼ ì œê³µí•´ì•¼, ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œ í™œìš©ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ ì‹¤ì œë¡œ NLP ë°ì´í„°ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ í•„ìš”í•œ í™˜ê²½ì„ ì¤€ë¹„í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d992bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets  # ğŸ¤– NLP ë°ì´í„°ì…‹ì„ ì‰½ê²Œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬(datasets) ì„¤ì¹˜\n",
    "# 'datasets'ëŠ” ë‹¤ì–‘í•œ ìì—°ì–´ì²˜ë¦¬ ë°ì´í„°ì…‹ì„ ê°„í¸í•˜ê²Œ ë‹¤ìš´ë¡œë“œí•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤\n",
    "# ë‹¤ìŒ: ì„¤ì¹˜í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‹¤ì œë¡œ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš©í•  ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets  # ë°ì´í„°ì…‹(datasets) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤\n",
    "# ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” NLP ì‹¤ìŠµì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ì„ ì‰½ê²Œ ë¶ˆëŸ¬ì˜¤ê³  ê´€ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤\n",
    "# ì£¼ì˜: ì‹¤í–‰ í™˜ê²½ì— ë”°ë¼ ê²½ê³  ë©”ì‹œì§€ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì‹¤ìŠµì—ëŠ” í° ì˜í–¥ì´ ì—†ìŠµë‹ˆë‹¤\n",
    "# ë‹¤ìŒ: ë°ì´í„°ì…‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ì‹¤ì œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  í™•ì¸í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16538e19",
   "metadata": {},
   "source": [
    "### ë°ì´í„° í™•ì¸ ë‹¨ê³„ ì•ˆë‚´\n",
    "\n",
    "ì´ì œ ë°ì´í„°ì…‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ì‹¤ì œ ìì—°ì–´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , ê·¸ êµ¬ì¡°ì™€ ë‚´ìš©ì„ í™•ì¸í•˜ëŠ” ë°©ë²•ì„ ì‹¤ìŠµí•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒ ë‹¨ê³„ì—ì„œëŠ” ëŒ€í‘œì ì¸ NLP ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê³ , ë°ì´í„°ì˜ ì˜ˆì‹œë¥¼ ì§ì ‘ ì‚´í´ë³¼ ì˜ˆì •ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8974ea96",
   "metadata": {},
   "source": [
    "## AG News ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°ì™€ êµ¬ì¡° ì´í•´\n",
    "\n",
    "ë¨¸ì‹ ëŸ¬ë‹, íŠ¹íˆ ìì—°ì–´ ì²˜ë¦¬(NLP)ì—ì„œëŠ” ì‹¤ì œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì´ìš©í•´ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ê·¸ ì¤‘ AG News ë°ì´í„°ì…‹ì€ ë‰´ìŠ¤ ê¸°ì‚¬ í…ìŠ¤íŠ¸ì™€ í•´ë‹¹ ê¸°ì‚¬ ì¹´í…Œê³ ë¦¬(ë¼ë²¨)ê°€ í¬í•¨ëœ ëŒ€í‘œì ì¸ ë¶„ë¥˜ìš© ë°ì´í„°ì…‹ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ë©´ í…ìŠ¤íŠ¸ ë¶„ë¥˜(Text Classification) ë¬¸ì œë¥¼ ì‹¤ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì—¬ê¸°ì„œëŠ” Hugging Faceì˜ `datasets` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ AG News ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê² ìŠµë‹ˆë‹¤. ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì„ ì†ì‰½ê²Œ ë‹¤ìš´ë¡œë“œí•˜ê³ , ë‹¤ì–‘í•œ í¬ë§·ìœ¼ë¡œ ë‹¤ë£° ìˆ˜ ìˆë„ë¡ í•´ì¤ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ ì‹¤ì œë¡œ AG News ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì½”ë“œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì½”ë“œ ì…€ì—ì„œëŠ” ë°ì´í„°ì…‹ì„ ë©”ëª¨ë¦¬ë¡œ ë¡œë“œí•˜ëŠ” ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AG News ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤\n",
    "import datasets  # Hugging Faceì˜ ë°ì´í„°ì…‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "\n",
    "dataset = datasets.load_dataset('ag_news')  # 'ag_news' ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ\n",
    "\n",
    "# ë‹¤ìŒ ë‹¨ê³„: ë°ì´í„°ì…‹ì˜ ì‹¤ì œ ìƒ˜í”Œ êµ¬ì¡°ë¥¼ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74986d77",
   "metadata": {},
   "source": [
    "### ë°ì´í„°ì…‹ ìƒ˜í”Œ êµ¬ì¡° ì‚´í´ë³´ê¸°\n",
    "\n",
    "ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ë‹¤ë©´, ì‹¤ì œë¡œ ì–´ë–¤ í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ëŠ” ë°ì´í„° ì „ì²˜ë¦¬, í† í°í™”, ë¼ë²¨ ì¸ì½”ë”© ë“± í›„ì† ì‘ì—…ì„ ì„¤ê³„í•  ë•Œ ë°˜ë“œì‹œ í•„ìš”í•œ ê³¼ì •ì…ë‹ˆë‹¤.\n",
    "\n",
    "AG News ë°ì´í„°ì…‹ì˜ ê° ìƒ˜í”Œì€ 'text'(ê¸°ì‚¬ ë³¸ë¬¸)ì™€ 'label'(ì¹´í…Œê³ ë¦¬)ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ì½”ë“œì—ì„œëŠ” ì²« ë²ˆì§¸ í•™ìŠµ ìƒ˜í”Œì„ ì¶œë ¥í•˜ì—¬ êµ¬ì¡°ë¥¼ ì§ì ‘ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ ì‹¤ì œ ìƒ˜í”Œì„ ì¶œë ¥í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c601818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ì˜ ì²« ë²ˆì§¸ í•™ìŠµ ìƒ˜í”Œì„ ì¶œë ¥í•©ë‹ˆë‹¤\n",
    "print(dataset['train'][0])  # {'text': ..., 'label': ...} í˜•íƒœë¡œ ì¶œë ¥ë¨\n",
    "\n",
    "# ë‹¤ìŒ ë‹¨ê³„: ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ìƒì„±(NLG) ì‘ì—…ì˜ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a152c6",
   "metadata": {},
   "source": [
    "## ìì—°ì–´ ìƒì„±(NLG, Natural Language Generation) ì£¼ìš” í™œìš© ì˜ˆì‹œ\n",
    "\n",
    "ìì—°ì–´ ìƒì„±(NLG)ì€ ë‹¨ìˆœíˆ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¥˜í•˜ê±°ë‚˜ ê°ì •ì„ ë¶„ì„í•˜ëŠ” ê²ƒì—ì„œ ë‚˜ì•„ê°€, ì¸ê³µì§€ëŠ¥ì´ ì§ì ‘ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. NLGëŠ” ì‹¤ì œë¡œ ë§¤ìš° ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. ëŒ€í‘œì ì¸ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "- **í…ìŠ¤íŠ¸ ìš”ì•½(Summarization)**: ê¸´ ë¬¸ì„œë¥¼ í•µì‹¬ë§Œ ê°„ì¶”ë ¤ ì§§ì€ ìš”ì•½ë¬¸ì„ ìƒì„±\n",
    "- **ê¸°ê³„ ë²ˆì—­(Machine Translation)**: í•œ ì–¸ì–´ì˜ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ìë™ ë³€í™˜\n",
    "- **ì±—ë´‡(Chatbot)**: ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ìƒì„±\n",
    "- **ì½”ë“œ ìƒì„±(Code Generation)**: ìì—°ì–´ ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ ì½”ë“œ ìë™ ìƒì„±\n",
    "- **ì°½ì‘(Generative Writing)**: ì†Œì„¤, ì‹œ, ê¸°ì‚¬ ë“± ì°½ì˜ì ì¸ ê¸€ì“°ê¸° ìë™í™”\n",
    "\n",
    "ì´ì²˜ëŸ¼ NLGëŠ” ë‹¨ìˆœ ì •ë³´ ì „ë‹¬ì„ ë„˜ì–´ì„œ, ì¸ê°„ê³¼ ìœ ì‚¬í•œ ì–¸ì–´ í‘œí˜„ì„ ë§Œë“¤ì–´ë‚´ëŠ” ë° í•„ìˆ˜ì ì¸ ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±ì˜ ì‹¤ì œ êµ¬í˜„ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ì—ì„œëŠ” ëŒ€ê·œëª¨ ì›¹ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹(OpenWebText)ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c57502",
   "metadata": {},
   "source": [
    "### í…ìŠ¤íŠ¸ ìƒì„± ì‹¤ìŠµì„ ìœ„í•œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "\n",
    "ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM, Large Language Model)ì´ë‚˜ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì„ í•™ìŠµí•˜ë ¤ë©´, ë°©ëŒ€í•œ ì–‘ì˜ ì‹¤ì œ í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. OpenWebText(OWT)ëŠ” ì›¹ì—ì„œ ìˆ˜ì§‘ëœ ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ, GPT-2 ë“± ìœ ëª…í•œ ì–¸ì–´ ëª¨ë¸ì˜ í•™ìŠµì— ì‚¬ìš©ëœ ë°” ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì—¬ê¸°ì„œëŠ” Hugging Faceì˜ `load_dataset` í•¨ìˆ˜ì™€ ìŠ¤íŠ¸ë¦¬ë°(streaming) ì˜µì…˜ì„ í™œìš©í•˜ì—¬, OpenWebText ë°ì´í„°ì…‹ì„ ë©”ëª¨ë¦¬ì— ëª¨ë‘ ì˜¬ë¦¬ì§€ ì•Šê³ ë„ í•œ ì¤„ì”© ì½ì–´ì˜¬ ìˆ˜ ìˆë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤. ì´ëŠ” ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì„ ë‹¤ë£° ë•Œ ë§¤ìš° íš¨ìœ¨ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ ì½”ë“œì—ì„œëŠ” Parquet í¬ë§·ìœ¼ë¡œ ì €ì¥ëœ OWT ë°ì´í„°ì…‹ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ë°©ì‹ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœì†Œí™”í•˜ë©´ì„œë„, í•„ìš”í•œ ë§Œí¼ ë°ì´í„°ë¥¼ ì¦‰ì‹œ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ ì‹¤ì œë¡œ OWT ë°ì´í„°ì…‹ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ì½”ë“œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenWebText(OWT) ë°ì´í„°ì…‹ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤\n",
    "from datasets import load_dataset  # ë°ì´í„°ì…‹ ë¡œë“œ í•¨ìˆ˜ ì„í¬íŠ¸\n",
    "\n",
    "owt_stream = load_dataset(\n",
    "    \"parquet\",  # ë°ì´í„° í¬ë§·: Parquet(ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ì í•©í•œ ì»¬ëŸ¼ ê¸°ë°˜ ì €ì¥ í¬ë§·)\n",
    "    data_files=\"hf://datasets/Skylion007/openwebtext@refs/convert/parquet/plain_text/partial-train/*.parquet\",  # ë°ì´í„° íŒŒì¼ ê²½ë¡œ\n",
    "    split=\"train\",  # í•™ìŠµìš© ë°ì´í„°ë§Œ ì„ íƒ\n",
    "    streaming=True  # ìŠ¤íŠ¸ë¦¬ë° ì˜µì…˜ í™œì„±í™”(ë©”ëª¨ë¦¬ì— ëª¨ë‘ ì˜¬ë¦¬ì§€ ì•Šê³  í•œ ì¤„ì”© ì½ìŒ)\n",
    ")\n",
    "\n",
    "# ë‹¤ìŒ ë‹¨ê³„: ìŠ¤íŠ¸ë¦¬ë°ëœ ë°ì´í„°ì…‹ì—ì„œ ìƒ˜í”Œì„ ì¶”ì¶œí•˜ê³ , ì‹¤ì œ í…ìŠ¤íŠ¸ë¥¼ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9d5ad6",
   "metadata": {},
   "source": [
    "## ìì—°ì–´ ìƒì„±(Natural Language Generation, NLG) ë°ì´í„°ì˜ íŠ¹ì§•ê³¼ í™œìš©\n",
    "\n",
    "ìì—°ì–´ ìƒì„± ì‘ì—…(NLG)ì€ ì»´í“¨í„°ê°€ ì‚¬ëŒì²˜ëŸ¼ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì„ ë§Œë“¤ì–´ë‚´ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ë•Œ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ëŠ” ë¬¸ë§¥(Context)ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ëŠ” ì—°ì†ì ì¸ í…ìŠ¤íŠ¸ê°€ ì£¼ë¡œ í™œìš©ë©ë‹ˆë‹¤. ì™œëƒí•˜ë©´, ì‹¤ì œ ì–¸ì–´ëŠ” ì•ë’¤ ë¬¸ë§¥ì— ë”°ë¼ ì˜ë¯¸ê°€ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì†Œì„¤ì´ë‚˜ ë‰´ìŠ¤ ê¸°ì‚¬ì™€ ê°™ì´ ë¬¸ì¥ì´ ì„œë¡œ ì—°ê²°ë˜ì–´ ìˆëŠ” ë°ì´í„°ê°€ ëŒ€í‘œì ì…ë‹ˆë‹¤.\n",
    "\n",
    "í•˜ì§€ë§Œ, ë§Œì•½ ëª¨ë¸ì˜ ëª©ì ì´ ë‹¨ìˆœíˆ ì§§ì€ ë¬¸ì¥(ë‹¨ë¬¸, short sentence)ë§Œì„ ìƒì„±í•˜ëŠ” ê²ƒì´ë¼ë©´, ê¸´ ë¬¸ë§¥ì´ ë°˜ë“œì‹œ í•„ìš”í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì¦‰, ë°ì´í„°ì˜ ë¬¸ë§¥ ê¸¸ì´ì™€ ì—°ì†ì„±ì€ ëª¨ë¸ì˜ í™œìš© ëª©ì ì— ë”°ë¼ ì¤‘ìš”ë„ê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤.\n",
    "\n",
    "ë˜í•œ, í…ìŠ¤íŠ¸ ìƒì„± íƒœìŠ¤í¬ì—ì„œëŠ” ì…ë ¥ ë°ì´í„°(Input)ì™€ ì •ë‹µ ë°ì´í„°(Target)ê°€ ë™ì¼í•œ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê±°ë‚˜, ë¬¸ì¥ ì „ì²´ë¥¼ ë³µì›í•˜ëŠ” ì–¸ì–´ ëª¨ë¸ë§(Language Modeling)ì—ì„œëŠ” ì…ë ¥ê³¼ íƒ€ê²Ÿì´ í•œ ê¸€ìì”© ë°€ë ¤ìˆëŠ” í˜•íƒœë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ ì‹¤ì œë¡œ ìì—°ì–´ ë°ì´í„°ê°€ ì–´ë–¤ í˜•íƒœë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€, ë°ì´í„°ë¥¼ ì‚´í´ë³´ëŠ” ì½”ë“œë¥¼ ì‹¤í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d292cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# owt_streamì€ OpenWebTextì™€ ê°™ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë§¥ì„ ê°€ì§„ í…ìŠ¤íŠ¸ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì…ë‹ˆë‹¤\n",
    "for i, ex in enumerate(owt_stream):\n",
    "    print(ex)  # ê° ì˜ˆì‹œ(ex)ëŠ” í•˜ë‚˜ì˜ ë¬¸ì„œ ë˜ëŠ” ë¬¸ë‹¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤\n",
    "    if i == 2:\n",
    "        break  # ì²˜ìŒ 3ê°œ ì˜ˆì‹œë§Œ ì¶œë ¥í•˜ì—¬ ë°ì´í„° êµ¬ì¡°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤\n",
    "\n",
    "# ë‹¤ìŒ: ê¸°ê³„ ë²ˆì—­(Machine Translation) ë°ì´í„°ì…‹ì˜ êµ¬ì¡°ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacc7f56",
   "metadata": {},
   "source": [
    "## ê¸°ê³„ ë²ˆì—­(Machine Translation) ë°ì´í„°ì˜ êµ¬ì¡°ì™€ íŠ¹ì§•\n",
    "\n",
    "ê¸°ê³„ ë²ˆì—­ì€ í•œ ì–¸ì–´ë¡œ ëœ ë¬¸ì¥ì„ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ìë™ ë³€í™˜í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì´ íƒœìŠ¤í¬ì˜ ë°ì´í„°ëŠ” ë°˜ë“œì‹œ \"ì…ë ¥ ì–¸ì–´\"ì™€ \"ì¶œë ¥ ì–¸ì–´\"ê°€ ìŒ(Pair)ìœ¼ë¡œ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë…ì¼ì–´(Deutsch, de)ì™€ ì˜ì–´(English, en) ë¬¸ì¥ì´ í•œ ìŒìœ¼ë¡œ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ëŸ° ë³‘ë ¬ ë°ì´í„°(parallel data)ëŠ” ë²ˆì—­ ëª¨ë¸ì´ ë‘ ì–¸ì–´ ê°„ì˜ ì˜ë¯¸ì  ëŒ€ì‘ ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤. íŠ¹íˆ, ëŒ€ê·œëª¨ ê³µê°œ ë°ì´í„°ì…‹ì¸ WMT(Workshop on Machine Translation) ì‹œë¦¬ì¦ˆëŠ” ë‹¤ì–‘í•œ ì–¸ì–´ ìŒì— ëŒ€í•´ ê³ í’ˆì§ˆ ë²ˆì—­ ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ ì‹¤ì œë¡œ WMT14 ë°ì´í„°ì…‹ì—ì„œ ë…ì¼ì–´-ì˜ì–´ ìŒì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì™€, ë°ì´í„° êµ¬ì¡°ë¥¼ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206084c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•´ WMT14 ë…ì¼ì–´-ì˜ì–´ ë³‘ë ¬ ë°ì´í„°ì…‹ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤\n",
    "# 'split=\"train\"'ì€ í•™ìŠµìš© ë°ì´í„°ë¥¼ ì˜ë¯¸í•˜ë©°, streaming=TrueëŠ” ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤\n",
    "dataset = datasets.load_dataset('wmt14', 'de-en', split=\"train\", streaming=True)\n",
    "\n",
    "temp = None  # ì„ì‹œë¡œ ë°ì´í„°ë¥¼ ì €ì¥í•  ë³€ìˆ˜\n",
    "for i, example in enumerate(dataset):\n",
    "    temp = example  # ì²« ë²ˆì§¸ ì˜ˆì‹œë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤\n",
    "    break\n",
    "\n",
    "print(temp)  # {'translation': {'de': '...', 'en': '...'}} í˜•íƒœë¡œ ì¶œë ¥ë©ë‹ˆë‹¤\n",
    "\n",
    "# ë‹¤ìŒ: ì§ˆì˜ì‘ë‹µ(QA, Question Answering) ë°ì´í„°ì…‹ì˜ êµ¬ì¡°ì™€ íŠ¹ì§•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c98b16",
   "metadata": {},
   "source": [
    "## ì§ˆì˜ì‘ë‹µ(Question Answering, QA) íƒœìŠ¤í¬ì˜ ë°ì´í„° êµ¬ì¡°ì™€ í™œìš©\n",
    "\n",
    "ì§ˆì˜ì‘ë‹µ(QA)ì€ ì£¼ì–´ì§„ ë¬¸ì„œ(ë˜ëŠ” ë¬¸ë‹¨)ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë‹µ(Answer)ì„ ì¶”ë¡ í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì´ íƒœìŠ¤í¬ëŠ” ë‹¨ìˆœíˆ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬, ë¬¸ì„œ ë‚´ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ì •í™•íˆ ì°¾ì•„ë‚´ëŠ” ëŠ¥ë ¥ì´ ìš”êµ¬ë©ë‹ˆë‹¤.\n",
    "\n",
    "QA ë°ì´í„°ì…‹ì€ ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤:\n",
    "- **ë¬¸ì„œ(Document)**: ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆëŠ” ì •ë³´ê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸\n",
    "- **ì§ˆë¬¸(Question)**: ë¬¸ì„œì— ê¸°ë°˜í•˜ì—¬ ë‹µí•´ì•¼ í•˜ëŠ” ì§ˆì˜\n",
    "- **ì •ë‹µ(Answer)**: ë¬¸ì„œ ë‚´ì—ì„œ ì¶”ì¶œ ê°€ëŠ¥í•œ ë‹µë³€(ë˜ëŠ” ë‹µë³€ì˜ ìœ„ì¹˜)\n",
    "\n",
    "ì´ëŸ° ë°ì´í„° êµ¬ì¡°ëŠ” ëª¨ë¸ì´ ë‹¨ìˆœí•œ ì•”ê¸°ë‚˜ ìƒì„±ì´ ì•„ë‹Œ, ë¬¸ì„œ ì´í•´ì™€ ì •ë³´ ì¶”ì¶œ ëŠ¥ë ¥ì„ í•™ìŠµí•˜ë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒ ë‹¨ê³„ì—ì„œëŠ” ì‹¤ì œ QA ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê³ , ê·¸ êµ¬ì¡°ë¥¼ ì½”ë“œë¡œ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4c03c5",
   "metadata": {},
   "source": [
    "## ì¶”ì¶œí˜• QA(Extractive Question Answering) ë°ì´í„°ì…‹ ì‚´í´ë³´ê¸°\n",
    "\n",
    "ì§ˆë¬¸ ì‘ë‹µ(QA, Question Answering) ì‹œìŠ¤í…œì€ í¬ê²Œ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤. ê·¸ ì¤‘ **ì¶”ì¶œí˜• QA**ëŠ” ì£¼ì–´ì§„ ë¬¸ì„œ(context)ì—ì„œ ì •ë‹µì´ ë˜ëŠ” í…ìŠ¤íŠ¸ì˜ ì¼ë¶€ë¶„ì„ ê·¸ëŒ€ë¡œ \"ì¶”ì¶œ\"í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, SQuADì™€ ê°™ì€ ë°ì´í„°ì…‹ì´ ëŒ€í‘œì ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë°©ì‹ì´ ì¤‘ìš”í•œ ì´ìœ ëŠ”, ì‹¤ì œë¡œ ë§ì€ ì •ë³´ ê²€ìƒ‰ ë° ì±—ë´‡ ì‹œìŠ¤í…œì—ì„œ ì •ë‹µì´ ì´ë¯¸ ë¬¸ì„œ ë‚´ì— ì¡´ì¬í•˜ëŠ” ê²½ìš°ê°€ ë§ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë”°ë¼ì„œ, ì¶”ì¶œí˜• QAëŠ” ìì—°ì–´ ì´í•´(NLU)ì™€ ì •ë³´ ê²€ìƒ‰(IR) ê¸°ìˆ ì˜ í•µì‹¬ì ì¸ ì‘ìš© ë¶„ì•¼ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ SQuAD ë°ì´í„°ì…‹ì—ì„œ í•œ ìƒ˜í”Œì„ ì§ì ‘ ë¶ˆëŸ¬ì™€ êµ¬ì¡°ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ì´í›„, ìƒì„±í˜• QAì™€ ë¹„êµí•´ë³¼ ì˜ˆì •ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f28c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQuAD ë°ì´í„°ì…‹ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤\n",
    "import datasets  # ë°ì´í„°ì…‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "\n",
    "dataset = datasets.load_dataset('squad', split=\"train\", streaming=True)  # SQuAD í•™ìŠµ ë°ì´í„°ì…‹ ìŠ¤íŠ¸ë¦¬ë° ë¡œë“œ\n",
    "\n",
    "# ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ ì¶”ì¶œí•˜ì—¬ êµ¬ì¡°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤\n",
    "temp = None\n",
    "for i, example in enumerate(dataset):\n",
    "    temp = example  # ì²« ë²ˆì§¸ ìƒ˜í”Œ ì €ì¥\n",
    "    break  # ì²« ìƒ˜í”Œë§Œ í™•ì¸í•˜ê³  ë°˜ë³µë¬¸ ì¢…ë£Œ\n",
    "\n",
    "print(temp)  # ìƒ˜í”Œì˜ ì „ì²´ êµ¬ì¡° ì¶œë ¥\n",
    "\n",
    "# ë‹¤ìŒ ë‹¨ê³„: ìƒì„±í˜• QA ë°ì´í„°ì…‹(Natural Questions)ê³¼ ë¹„êµí•´ë´…ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86357558",
   "metadata": {},
   "source": [
    "## ìƒì„±í˜• QA(Generative Question Answering) ë°ì´í„°ì…‹ ì‚´í´ë³´ê¸°\n",
    "\n",
    "ì•ì„œ ë³¸ ì¶”ì¶œí˜• QAì™€ ë‹¬ë¦¬, **ìƒì„±í˜• QA**ëŠ” ë¬¸ì„œ ë‚´ì— ì •ë‹µì´ ëª…í™•íˆ ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì€ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ í˜•íƒœë¡œ ì •ë‹µì„ \"ìƒì„±\"í•´ì•¼ í•©ë‹ˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œ Natural Questions, NarrativeQA ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë°©ì‹ì€ ë³µì¡í•œ ì§ˆì˜ë‚˜ ìš”ì•½, ì„¤ëª…ì´ í•„ìš”í•œ ìƒí™©ì—ì„œ ìœ ìš©í•©ë‹ˆë‹¤. ìµœê·¼ ëŒ€í˜• ì–¸ì–´ëª¨ë¸(LLM) ê¸°ë°˜ì˜ QA ì‹œìŠ¤í…œì´ ì£¼ë¡œ ì´ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ë²ˆì—ëŠ” Natural Questions ë°ì´í„°ì…‹ì—ì„œ í•œ ìƒ˜í”Œì„ ë¶ˆëŸ¬ì™€, ë¬¸ì„œê°€ HTML êµ¬ì¡°ë¡œ ë˜ì–´ ìˆìŒì„ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤. ì´í›„, ë¬¸ì„œ ìš”ì•½(Summarization) íƒœìŠ¤í¬ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698389b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Questions ë°ì´í„°ì…‹ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤\n",
    "import datasets  # ë°ì´í„°ì…‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "from IPython.display import display, HTML  # Jupyterì—ì„œ HTML ë Œë”ë§ì„ ìœ„í•œ ëª¨ë“ˆ\n",
    "\n",
    "dataset = datasets.load_dataset('natural_questions', split=\"train\", streaming=True)  # Natural Questions í•™ìŠµ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "\n",
    "# ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ ì¶”ì¶œí•˜ì—¬ ë¬¸ì„œì˜ HTML êµ¬ì¡°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤\n",
    "temp = None\n",
    "for i, example in enumerate(dataset):\n",
    "    temp = example  # ì²« ë²ˆì§¸ ìƒ˜í”Œ ì €ì¥\n",
    "    break  # ì²« ìƒ˜í”Œë§Œ í™•ì¸í•˜ê³  ë°˜ë³µë¬¸ ì¢…ë£Œ\n",
    "\n",
    "# ë¬¸ì„œì˜ HTML ë‚´ìš©ì„ Jupyterì—ì„œ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤\n",
    "display(HTML(temp[\"document\"][\"html\"]))\n",
    "\n",
    "# ë‹¤ìŒ ë‹¨ê³„: ë¬¸ì„œ ìš”ì•½(Summarization) íƒœìŠ¤í¬ì˜ ë°ì´í„°ì…‹ êµ¬ì¡°ë¥¼ ì‚´í´ë´…ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae19f523",
   "metadata": {},
   "source": [
    "## ë¬¸ì„œ ìš”ì•½(Summarization) íƒœìŠ¤í¬ ì†Œê°œ\n",
    "\n",
    "ë¬¸ì„œ ìš”ì•½ì€ ê¸´ ë¬¸ì„œì—ì„œ í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ë½‘ì•„ë‚´ëŠ” ìì—°ì–´ ì²˜ë¦¬(NLP) ê¸°ìˆ ì…ë‹ˆë‹¤. QAì™€ ë‹¬ë¦¬, ìš”ì•½ íƒœìŠ¤í¬ëŠ” ë¬¸ì„œ ì „ì²´ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ê³ , ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì„ íƒì ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ëŠ” ëŠ¥ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ íƒœìŠ¤í¬ëŠ” ë‰´ìŠ¤ ê¸°ì‚¬, ë…¼ë¬¸, ë³´ê³ ì„œ ë“± ë‹¤ì–‘í•œ ì‹¤ìƒí™œ ì‘ìš©ì— í™œìš©ë©ë‹ˆë‹¤. ëŒ€í‘œì ì¸ ë°ì´í„°ì…‹ìœ¼ë¡œ CNN/DailyMailì´ ìˆìœ¼ë©°, ì´ ë°ì´í„°ì…‹ì€ ê¸°ì‚¬ì™€ ê·¸ì— ëŒ€í•œ ìš”ì•½ë¬¸(í•˜ì´ë¼ì´íŠ¸)ì„ ìŒìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ CNN/DailyMail ë°ì´í„°ì…‹ì—ì„œ í•œ ìƒ˜í”Œì„ ë¶ˆëŸ¬ì™€, ì‹¤ì œ ê¸°ì‚¬ì™€ ìš”ì•½ë¬¸ì´ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ì´í›„, ê°œì²´ëª… ì¸ì‹(NER) íƒœìŠ¤í¬ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN/DailyMail ë°ì´í„°ì…‹ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤\n",
    "import datasets  # ë°ì´í„°ì…‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "\n",
    "dataset = datasets.load_dataset('cnn_dailymail', '3.0.0', split=\"train\", streaming=True)  # CNN/DailyMail í•™ìŠµ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "\n",
    "# ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ ì¶”ì¶œí•˜ì—¬ ê¸°ì‚¬ì™€ ìš”ì•½ë¬¸ êµ¬ì¡°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤\n",
    "temp = None\n",
    "for i, example in enumerate(dataset):\n",
    "    temp = example  # ì²« ë²ˆì§¸ ìƒ˜í”Œ ì €ì¥\n",
    "    break  # ì²« ìƒ˜í”Œë§Œ í™•ì¸í•˜ê³  ë°˜ë³µë¬¸ ì¢…ë£Œ\n",
    "\n",
    "print(temp)  # ê¸°ì‚¬(article)ì™€ ìš”ì•½ë¬¸(highlights) í™•ì¸\n",
    "\n",
    "# ë‹¤ìŒ ë‹¨ê³„: ê°œì²´ëª… ì¸ì‹(NER) íƒœìŠ¤í¬ì˜ ê°œë…ê³¼ ë°ì´í„°ì…‹ êµ¬ì¡°ë¥¼ ì‚´í´ë´…ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c691f9",
   "metadata": {},
   "source": [
    "## ê°œì²´ëª… ì¸ì‹(Named Entity Recognition, NER) íƒœìŠ¤í¬ ì†Œê°œ\n",
    "\n",
    "ê°œì²´ëª… ì¸ì‹(NER)ì€ ë¬¸ì¥ì—ì„œ ì¸ë¬¼, ì¥ì†Œ, ê¸°ê´€, ë‚ ì§œ ë“± ì˜ë¯¸ ìˆëŠ” ê³ ìœ ëª…ì‚¬(ì—”í‹°í‹°, Entity)ë¥¼ ì‹ë³„í•˜ê³  ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. NERì€ ì •ë³´ ì¶”ì¶œ, ì§ˆì˜ì‘ë‹µ, ì¶”ì²œ ì‹œìŠ¤í…œ ë“± ë‹¤ì–‘í•œ NLP ì‘ìš©ì˜ ê¸°ì´ˆê°€ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ íƒœìŠ¤í¬ëŠ” ë‹¨ìˆœíˆ ë‹¨ì–´ë¥¼ ì°¾ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë¬¸ë§¥ì„ ì´í•´í•˜ì—¬ ì ì ˆí•œ ì—”í‹°í‹° ìœ í˜•ì„ ë¶€ì—¬í•´ì•¼ í•˜ë¯€ë¡œ, ìì—°ì–´ ì´í•´ì˜ ì¤‘ìš”í•œ ë‹¨ê³„ë¡œ ì—¬ê²¨ì§‘ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒ ì…€ì—ì„œëŠ” ëŒ€í‘œì ì¸ NER ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì™€, ì‹¤ì œ ë°ì´í„° êµ¬ì¡°ë¥¼ ì‚´í´ë³¼ ì˜ˆì •ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8adc9",
   "metadata": {},
   "source": [
    "## ì–´ë²¤ì ¸ìŠ¤ NER ë°ì´í„° ì§ì ‘ ìƒì„±í•˜ê¸°\n",
    "\n",
    "ì‹¤ì œ ìì—°ì–´ì²˜ë¦¬(NLP) í”„ë¡œì íŠ¸ì—ì„œëŠ” ì›í•˜ëŠ” íƒœìŠ¤í¬ì— ë§ëŠ” ë°ì´í„°ë¥¼ ì§ì ‘ ë§Œë“¤ì–´ì•¼ í•  ë•Œê°€ ë§ìŠµë‹ˆë‹¤. ì´ë²ˆì—ëŠ” ëŒ€í‘œì ì¸ ì‹œí€€ìŠ¤ íƒœìŠ¤í¬ì¸ ê°œì²´ëª… ì¸ì‹(Named Entity Recognition, NER)ì„ ìœ„í•´ ì–´ë²¤ì ¸ìŠ¤(Avengers) ê´€ë ¨ ì˜ˆì‹œ ë°ì´í„°ë¥¼ ì§ì ‘ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "NERì€ ë¬¸ì¥ ë‚´ì˜ í† í°(ë‹¨ì–´) ê°ê°ì— ëŒ€í•´ í•´ë‹¹ í† í°ì´ ì–´ë–¤ ê°œì²´ëª…(ì˜ˆ: ì¸ë¬¼, ì¡°ì§, ì¥ì†Œ ë“±)ì— ì†í•˜ëŠ”ì§€ íƒœê·¸ë¥¼ ë¶€ì—¬í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. CoNLL-2003 í˜•ì‹ì˜ BIO íƒœê·¸ ì²´ê³„ë¥¼ ì‚¬ìš©í•˜ë©°, ì´ëŠ” ì‹¤ì œ í˜„ì—… ë° ëŒ€íšŒì—ì„œë„ ë„ë¦¬ í™œìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ ì•„ë˜ ì½”ë“œì—ì„œëŠ” ì–´ë²¤ì ¸ìŠ¤ ê´€ë ¨ ë¬¸ì¥ì— ëŒ€í•´ í† í°í™”, NER íƒœê¹…, í’ˆì‚¬ íƒœê¹… ì •ë³´ë¥¼ í¬í•¨í•œ ì˜ˆì‹œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ì–´ì„œ ê° í† í°ë³„ NER íƒœê·¸ì˜ ì˜ë¯¸ë„ í•¨ê»˜ ì¶œë ¥í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "> ë‹¤ìŒ ì½”ë“œ ì…€ì—ì„œëŠ” ì–´ë²¤ì ¸ìŠ¤ NER ì˜ˆì‹œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³ , ê° íƒœê·¸ì˜ ì˜ë¯¸ë¥¼ í•´ì„í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f2bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì–´ë²¤ì ¸ìŠ¤ NER ì˜ˆì‹œ ë°ì´í„°ë¥¼ ì§ì ‘ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ ì •ì˜\n",
    "# ì‹¤ì œ NER íƒœìŠ¤í¬ì—ì„œ ì‚¬ìš©í•˜ëŠ” BIO íƒœê·¸ ì²´ê³„ë¥¼ ì ìš©\n",
    "\n",
    "def create_avengers_ner_example():\n",
    "    # NER íƒœê·¸ ì •ì˜ (CoNLL-2003 í˜•ì‹)\n",
    "    # 0: O (ê¸°íƒ€, Outside)\n",
    "    # 1: B-PER (ì¸ë¬¼ ì‹œì‘, Begin-Person)\n",
    "    # 2: I-PER (ì¸ë¬¼ ë‚´ë¶€, Inside-Person)\n",
    "    # 3: B-ORG (ì¡°ì§ ì‹œì‘, Begin-Organization)\n",
    "    # 4: I-ORG (ì¡°ì§ ë‚´ë¶€, Inside-Organization)\n",
    "    # 5: B-LOC (ì¥ì†Œ ì‹œì‘, Begin-Location)\n",
    "    # 6: I-LOC (ì¥ì†Œ ë‚´ë¶€, Inside-Location)\n",
    "    # 7: B-MISC (ê¸°íƒ€ ê°œì²´ ì‹œì‘, Begin-Miscellaneous)\n",
    "    # 8: I-MISC (ê¸°íƒ€ ê°œì²´ ë‚´ë¶€, Inside-Miscellaneous)\n",
    "\n",
    "    avengers_examples = [\n",
    "        {\n",
    "            'id': '1',\n",
    "            'tokens': ['Tony', 'Stark', 'is', 'Iron', 'Man', 'from', 'Marvel', 'Comics'],\n",
    "            'ner_tags': [1, 2, 0, 7, 8, 0, 3, 4],  # ê° í† í°ì— ëŒ€í•œ NER íƒœê·¸\n",
    "            'pos_tags': [22, 22, 42, 22, 22, 35, 22, 22]  # í’ˆì‚¬ íƒœê·¸ (ì„ì˜ê°’)\n",
    "        },\n",
    "        {\n",
    "            'id': '2',\n",
    "            'tokens': ['Steve', 'Rogers', 'became', 'Captain', 'America', 'in', 'New', 'York'],\n",
    "            'ner_tags': [1, 2, 0, 7, 8, 0, 5, 6],\n",
    "            'pos_tags': [22, 22, 42, 22, 22, 35, 22, 22]\n",
    "        },\n",
    "        {\n",
    "            'id': '3',\n",
    "            'tokens': ['The', 'Avengers', 'fought', 'in', 'Manhattan', 'against', 'Thanos'],\n",
    "            'ner_tags': [0, 3, 0, 0, 5, 0, 1],\n",
    "            'pos_tags': [12, 22, 42, 35, 22, 35, 22]\n",
    "        }\n",
    "    ]\n",
    "    return avengers_examples\n",
    "\n",
    "# í•¨ìˆ˜ ì‹¤í–‰: ì–´ë²¤ì ¸ìŠ¤ NER ì˜ˆì‹œ ë°ì´í„° ìƒì„±\n",
    "examples = create_avengers_ner_example()\n",
    "temp = examples[0]  # ì²« ë²ˆì§¸ ì˜ˆì‹œ ì„ íƒ\n",
    "\n",
    "print(\"ì–´ë²¤ì ¸ìŠ¤ NER ì˜ˆì‹œ:\")\n",
    "print(\"í† í°ë“¤:\", temp['tokens'])\n",
    "print(\"NER íƒœê·¸ë“¤:\", temp['ner_tags'])\n",
    "\n",
    "# íƒœê·¸ ë²ˆí˜¸ë¥¼ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ì´ë¦„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥\n",
    "# íƒœê·¸ ë²ˆí˜¸ì™€ ì˜ë¯¸ ë§¤í•‘\n",
    "# ì‹¤ì œ NER ëª¨ë¸ í•™ìŠµ ì‹œì—ë„ ì´ ë§¤í•‘ì´ ì¤‘ìš”í•¨\n",
    "\n",
    "tag_names = {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'}\n",
    "print(\"\\níƒœê·¸ í•´ì„:\")\n",
    "for token, tag in zip(temp['tokens'], temp['ner_tags']):\n",
    "    print(f\"{token}: {tag_names[tag]}\")\n",
    "\n",
    "# ë‹¤ìŒ: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì™€ ì‹¤ì œ ë¬¸ì¥ ìŒê³¼ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d119e535",
   "metadata": {},
   "source": [
    "## í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ (Textual Similarity)\n",
    "\n",
    "í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ëŠ” ë‘ ë¬¸ì¥ì´ ì˜ë¯¸ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ì§€ë¥¼ ìˆ˜ì¹˜ë¡œ í‘œí˜„í•˜ëŠ” íƒœìŠ¤í¬ì…ë‹ˆë‹¤. ì´ëŠ” ê²€ìƒ‰, ì¶”ì²œ, ì§ˆì˜ì‘ë‹µ ë“± ë‹¤ì–‘í•œ NLP ì‘ìš© ë¶„ì•¼ì—ì„œ í•µì‹¬ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¨ìˆœíˆ ë‹¨ì–´ê°€ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ì§€ ë³´ëŠ” ê²ƒë¿ ì•„ë‹ˆë¼, ë¬¸ì¥ ì „ì²´ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ í‰ê°€í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, \"A plane is taking off.\"ì™€ \"An air plane is taking off.\"ëŠ” ê±°ì˜ ê°™ì€ ì˜ë¯¸ì´ë¯€ë¡œ ë†’ì€ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "> ë‹¤ìŒ ì½”ë“œ ì…€ì—ì„œëŠ” ëŒ€í‘œì ì¸ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë°ì´í„°ì…‹(STS Benchmark)ì„ ë¶ˆëŸ¬ì™€ ì˜ˆì‹œ ë¬¸ì¥ ìŒê³¼ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ STS Benchmark ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# STS Benchmark: ë¬¸ì¥ ìŒê³¼ ê·¸ ìœ ì‚¬ë„ ì ìˆ˜(0~5)ë¥¼ ì œê³µí•˜ëŠ” ëŒ€í‘œì  ë°ì´í„°ì…‹\n",
    "\n",
    "import datasets  # ë°ì´í„°ì…‹ ë¡œë“œë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "dataset = datasets.load_dataset('stsb_multi_mt', 'en', split=\"train\", streaming=True)  # ì˜ì–´ í•™ìŠµ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (ìŠ¤íŠ¸ë¦¬ë°)\n",
    "\n",
    "temp = None  # ì²« ë²ˆì§¸ ì˜ˆì‹œë¥¼ ì €ì¥í•  ë³€ìˆ˜\n",
    "for i, example in enumerate(dataset):\n",
    "    temp = example\n",
    "    break  # ì²« ë²ˆì§¸ ì˜ˆì‹œë§Œ ì¶”ì¶œ\n",
    "\n",
    "print(temp)  # ì˜ˆì‹œ ì¶œë ¥: ë¬¸ì¥1, ë¬¸ì¥2, ìœ ì‚¬ë„ ì ìˆ˜\n",
    "\n",
    "# ë‹¤ìŒ: RNN(ìˆœí™˜ ì‹ ê²½ë§)ì˜ ê°œë…ê³¼ í•„ìš”ì„±ì— ëŒ€í•´ ì„¤ëª…í•˜ê³ , ì‹¤ì œ ê°ì„±ë¶„ë¥˜ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì„ ì‚´í´ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73516f98",
   "metadata": {},
   "source": [
    "## RNN (ìˆœí™˜ ì‹ ê²½ë§, Recurrent Neural Network)\n",
    "\n",
    "### ì™œ RNNì´ í•„ìš”í•œê°€?\n",
    "\n",
    "ìì—°ì–´ëŠ” ì‹œê°„ì  ìˆœì„œ(ë¬¸ë§¥)ì— ë”°ë¼ ì˜ë¯¸ê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì™„ì „ì—°ê²° ì‹ ê²½ë§(Feedforward Neural Network)ì€ ì…ë ¥ì˜ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ëª»í•´ ë¬¸ì¥ì´ë‚˜ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, \"ë‚˜ëŠ” ë°¥ì„ ë¨¹ì—ˆë‹¤\"ì™€ \"ë°¥ì„ ë‚˜ëŠ” ë¨¹ì—ˆë‹¤\"ëŠ” ë‹¨ì–´ëŠ” ê°™ì§€ë§Œ ìˆœì„œì— ë”°ë¼ ì˜ë¯¸ê°€ ë‹¤ë¦…ë‹ˆë‹¤.\n",
    "\n",
    "RNN(ìˆœí™˜ ì‹ ê²½ë§)ì€ ì…ë ¥ ë°ì´í„°ê°€ ìˆœì°¨ì ìœ¼ë¡œ ì£¼ì–´ì§ˆ ë•Œ, ì´ì „ ì…ë ¥ì˜ ì •ë³´ë¥¼ ë‚´ë¶€ ìƒíƒœ(hidden state)ì— ì €ì¥í•˜ì—¬ ë‹¤ìŒ ì…ë ¥ê³¼ í•¨ê»˜ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì¦‰, ì‹œí€€ìŠ¤ì˜ ë¬¸ë§¥ ì •ë³´ë¥¼ ë°˜ì˜í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ êµ¬ì¡°ì…ë‹ˆë‹¤. ì´ ë•ë¶„ì— ìì—°ì–´ ì²˜ë¦¬, ìŒì„± ì¸ì‹, ì‹œê³„ì—´ ì˜ˆì¸¡ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í•„ìˆ˜ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "> ë‹¤ìŒ ì½”ë“œ ì…€ì—ì„œëŠ” ì‹¤ì œ ê°ì„±ë¶„ë¥˜(ì˜í™” ë¦¬ë·°) ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ RNN í•™ìŠµì„ ìœ„í•œ ì „ì²˜ë¦¬ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜í™” ë¦¬ë·° ê°ì„±ë¶„ë¥˜ ë°ì´í„°(nsmc)ë¥¼ ë¶ˆëŸ¬ì™€ RNN ì…ë ¥ìš©ìœ¼ë¡œ ì „ì²˜ë¦¬í•˜ëŠ” ì „ì²´ ê³¼ì •\n",
    "# ê° ë‹¨ê³„ë³„ë¡œ ìƒì„¸ ì£¼ì„ ì¶”ê°€\n",
    "\n",
    "import pandas as pd  # ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬\n",
    "from sklearn.model_selection import train_test_split  # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• \n",
    "from collections import Counter  # ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "import re  # ì •ê·œí‘œí˜„ì‹(í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬)\n",
    "import numpy as np  # ìˆ˜ì¹˜ ì—°ì‚°\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ: ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ë°ì´í„°(nsmc)\n",
    "url = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\"\n",
    "df = pd.read_csv(url, delimiter='\\t')  # íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ íŒŒì¼ ì½ê¸°\n",
    "\n",
    "# 2. NaN(ê²°ì¸¡ì¹˜) ì œê±°\n",
    "# ë¦¬ë·° í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš°ê°€ ìˆìœ¼ë¯€ë¡œ, ê²°ì¸¡ì¹˜ë¥¼ ì œê±°í•´ì•¼ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì§€ ì•ŠìŒ\n",
    "# ì‹¤ì œ ë°ì´í„° ì „ì²˜ë¦¬ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ë‹¨ê³„\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# 3. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• \n",
    "# ì „ì²´ ë°ì´í„°ë¥¼ 8:2 ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµ(train)ê³¼ í‰ê°€(test)ì— ì‚¬ìš©\n",
    "x_train_text, x_test_text, y_train, y_test = train_test_split(\n",
    "    df['document'],\n",
    "    df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •\n",
    ")\n",
    "\n",
    "# 4. ì¸ë±ìŠ¤ ë¦¬ì…‹\n",
    "# ë°ì´í„° ë¶„í•  í›„ ì¸ë±ìŠ¤ê°€ ë’¤ì„ì´ë¯€ë¡œ, ì¸ë±ìŠ¤ë¥¼ ë‹¤ì‹œ 0ë¶€í„° ë¶€ì—¬\n",
    "x_train_text = x_train_text.reset_index(drop=True)\n",
    "x_test_text = x_test_text.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# 5. ê°„ë‹¨í•œ í† í°í™” í•¨ìˆ˜ ì •ì˜\n",
    "# í•œê¸€, ì˜ì–´, ìˆ«ìë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ê³µë°±ìœ¼ë¡œ ëŒ€ì²´í•œ ë’¤, ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬\n",
    "# ì‹¤ì œ í˜„ì—…ì—ì„œëŠ” ë” ì •êµí•œ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ í† í°í™”\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    return text.split()  # ê³µë°± ê¸°ì¤€ ë¶„ë¦¬\n",
    "\n",
    "# 6. ì „ì²´ í•™ìŠµ ë°ì´í„°ì—ì„œ ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "all_words = []\n",
    "for text in x_train_text:\n",
    "    all_words.extend(simple_tokenize(text))  # ëª¨ë“  í† í°ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "\n",
    "# 7. ìƒìœ„ 19,999ê°œ ë‹¨ì–´ë§Œ ì‚¬ìš© (ì´ 20,000ê°œì—ì„œ <OOV> ìë¦¬ 1ê°œ ë‚¨ê¹€)\n",
    "# ë°ì´í„°ê°€ ë°©ëŒ€í•  ë•ŒëŠ” ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë§Œ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬\n",
    "word_counts = Counter(all_words)\n",
    "most_common = word_counts.most_common(19999)  # (ë‹¨ì–´, ë¹ˆë„ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "# 8. ë‹¨ì–´ -> ì¸ë±ìŠ¤ ë§¤í•‘ ìƒì„±\n",
    "# 1ë²ˆë¶€í„° ì‹œì‘, 0ë²ˆì€ íŒ¨ë”©(padding)ìš©ìœ¼ë¡œ ë¹„ì›Œë‘ \n",
    "# <OOV> í† í°(Out-Of-Vocabulary, ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´)ì€ 1ë²ˆ ì¸ë±ìŠ¤ì— í• ë‹¹\n",
    "word_to_idx = {'<OOV>': 1}\n",
    "for i, (word, _) in enumerate(most_common, 2):\n",
    "    word_to_idx[word] = i\n",
    "\n",
    "print(f\"ë‹¨ì–´ ì‚¬ì „ í¬ê¸°: {len(word_to_idx)}\")\n",
    "\n",
    "# 9. í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "# ê° í† í°ì„ í•´ë‹¹ ì¸ë±ìŠ¤ë¡œ ë³€í™˜, ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ëŠ” <OOV>(1)ë¡œ ì²˜ë¦¬\n",
    "\n",
    "def text_to_sequence(text, word_to_idx):\n",
    "    tokens = simple_tokenize(text)\n",
    "    return [word_to_idx.get(token, 1) for token in tokens]  # ì—†ëŠ” ë‹¨ì–´ëŠ” <OOV>(1)\n",
    "\n",
    "# 10. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²´ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "x_train = [text_to_sequence(text, word_to_idx) for text in x_train_text]\n",
    "x_test = [text_to_sequence(text, word_to_idx) for text in x_test_text]\n",
    "\n",
    "# 11. ë¼ë²¨(y)ì„ numpy ë°°ì—´ë¡œ ë³€í™˜ (ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ í•„ìš”)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# 12. ë°ì´í„° í¬ê¸° ë° ì˜ˆì‹œ ì¶œë ¥\n",
    "print(f\"x_train ê°œìˆ˜: {len(x_train)}\")\n",
    "print(f\"x_test ê°œìˆ˜: {len(x_test)}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print()\n",
    "print(\"ì²« ë²ˆì§¸ ë¦¬ë·° (ì •ìˆ˜ ì‹œí€€ìŠ¤):\")\n",
    "print(x_train[0])\n",
    "print()\n",
    "print(f\"ì²« 3ê°œ ë¦¬ë·° ê¸¸ì´: {len(x_train[0])}, {len(x_train[1])}, {len(x_train[2])}\")\n",
    "\n",
    "# 13. ì›ë³¸ í…ìŠ¤íŠ¸ì™€ í† í°í™” ê²°ê³¼ ë¹„êµ\n",
    "print(\"\\n=== ì›ë³¸ vs í† í°í™” ë¹„êµ ===\")\n",
    "sample_idx = 0\n",
    "original_text = x_train_text[sample_idx]\n",
    "tokenized = x_train[sample_idx]\n",
    "print(f\"ì›ë³¸: {original_text}\")\n",
    "print(f\"í† í°í™”: {tokenized[:20]}...\")  # ì²˜ìŒ 20ê°œë§Œ ì¶œë ¥\n",
    "\n",
    "# ë‹¤ìŒ: ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ RNNì— ì…ë ¥í•˜ê¸° ìœ„í•´ íŒ¨ë”©(padding) ë° ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88242b9a",
   "metadata": {},
   "source": [
    "## ì‹œí€€ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬: íŒ¨ë”©(Padding) í•¨ìˆ˜ êµ¬í˜„ê³¼ ì ìš©\n",
    "\n",
    "ë”¥ëŸ¬ë‹ ëª¨ë¸, íŠ¹íˆ RNN(ìˆœí™˜ ì‹ ê²½ë§)ì´ë‚˜ Transformer(íŠ¸ëœìŠ¤í¬ë¨¸) ê³„ì—´ ëª¨ë¸ì€ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ê°€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹¤ì œ í…ìŠ¤íŠ¸ ë°ì´í„°ëŠ” ë¬¸ì¥ë§ˆë‹¤ ê¸¸ì´ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì—, ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ ëª¨ë“  ì‹œí€€ìŠ¤ë¥¼ ë™ì¼í•œ ê¸¸ì´ë¡œ ë§ì¶”ëŠ” 'íŒ¨ë”©(padding)' ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. íŒ¨ë”©ì€ ì§§ì€ ë¬¸ì¥ì—ëŠ” 0(í˜¹ì€ ì§€ì •í•œ ê°’)ìœ¼ë¡œ ì±„ì›Œ ê¸¸ì´ë¥¼ ë§ì¶”ê³ , ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ì˜ë¼ë‚´ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ê³¼ì •ì€ í…ì„œí”Œë¡œìš°(TensorFlow)ë‚˜ íŒŒì´í† ì¹˜(PyTorch) ë“± í”„ë ˆì„ì›Œí¬ì— ë‚´ì¥ëœ í•¨ìˆ˜ë¥¼ ì“¸ ìˆ˜ë„ ìˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì›ë¦¬ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ì§ì ‘ íŒ¨ë”© í•¨ìˆ˜ë¥¼ êµ¬í˜„í•´ë´…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ ì§ì ‘ íŒ¨ë”© í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ê³ , ì‹¤ì œ ë°ì´í„°ì— ì ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤. ì´í›„, íŒ¨ë”© ì „í›„ì˜ ë°ì´í„°ê°€ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d53ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# =============================\n",
    "# 1. ì‹œí€€ìŠ¤ íŒ¨ë”© í•¨ìˆ˜ ì§ì ‘ êµ¬í˜„\n",
    "# =============================\n",
    "def pad_sequences(sequences, maxlen=80, padding='pre', truncating='pre', value=0):\n",
    "    \"\"\"\n",
    "    ì‹œí€€ìŠ¤(ì •ìˆ˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸)ë“¤ì„ ê°™ì€ ê¸¸ì´ë¡œ ë§ì¶”ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "    - sequences: ì •ìˆ˜ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤ë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "    - maxlen: ìµœì¢… ì‹œí€€ìŠ¤ ê¸¸ì´(ëª¨ë“  ë¬¸ì¥ì„ ì´ ê¸¸ì´ë¡œ ë§ì¶¤)\n",
    "    - padding: 'pre'ë©´ ì•ìª½ì—, 'post'ë©´ ë’¤ìª½ì— íŒ¨ë”©ì„ ì¶”ê°€\n",
    "    - truncating: 'pre'ë©´ ì•ìª½ì„ ì˜ë¼ë‚´ê³ , 'post'ë©´ ë’¤ìª½ì„ ì˜ë¼ëƒ„\n",
    "    - value: íŒ¨ë”©ì— ì‚¬ìš©í•  ê°’(ì¼ë°˜ì ìœ¼ë¡œ 0)\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for seq in sequences:\n",
    "        # [1] ì‹œí€€ìŠ¤ê°€ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ëƒ„\n",
    "        if len(seq) > maxlen:\n",
    "            if truncating == 'pre':\n",
    "                seq = seq[-maxlen:]  # ë’¤ìª½ maxlenê°œë§Œ ë‚¨ê¹€\n",
    "            else:  # 'post'\n",
    "                seq = seq[:maxlen]   # ì•ìª½ maxlenê°œë§Œ ë‚¨ê¹€\n",
    "        # [2] ì‹œí€€ìŠ¤ê°€ ì§§ìœ¼ë©´ íŒ¨ë”© ì¶”ê°€\n",
    "        if len(seq) < maxlen:\n",
    "            pad_length = maxlen - len(seq)\n",
    "            if padding == 'pre':\n",
    "                seq = [value] * pad_length + seq  # ì•ì— íŒ¨ë”© ì¶”ê°€\n",
    "            else:  # 'post'\n",
    "                seq = seq + [value] * pad_length  # ë’¤ì— íŒ¨ë”© ì¶”ê°€\n",
    "        result.append(seq)\n",
    "    return np.array(result)\n",
    "\n",
    "# =============================\n",
    "# 2. ì‹¤ì œ ë°ì´í„°ì— íŒ¨ë”© ì ìš©\n",
    "# =============================\n",
    "pad_x_train = pad_sequences(x_train, maxlen=80)\n",
    "pad_x_test = pad_sequences(x_test, maxlen=80)\n",
    "\n",
    "# =============================\n",
    "# 3. íŒ¨ë”© ì „í›„ ê²°ê³¼ í™•ì¸\n",
    "# =============================\n",
    "print(f\"íŒ¨ë”© í›„ x_train shape: {pad_x_train.shape}\")\n",
    "print(f\"íŒ¨ë”© í›„ x_test shape: {pad_x_test.shape}\")\n",
    "print(f\"ì²« 3ê°œ ë¦¬ë·° ê¸¸ì´: {len(pad_x_train[0])}, {len(pad_x_train[1])}, {len(pad_x_train[2])}\")\n",
    "\n",
    "print(\"\\n=== íŒ¨ë”© ì „í›„ ë¹„êµ ===\")\n",
    "print(f\"íŒ¨ë”© ì „ ì²« ë²ˆì§¸ ë¦¬ë·° ê¸¸ì´: {len(x_train[0])}\")\n",
    "print(f\"íŒ¨ë”© í›„ ì²« ë²ˆì§¸ ë¦¬ë·° ê¸¸ì´: {len(pad_x_train[0])}\")\n",
    "print(f\"íŒ¨ë”© í›„ ì²« ë²ˆì§¸ ë¦¬ë·°: {pad_x_train[0]}\")\n",
    "\n",
    "# ë‹¤ìŒ: íŒ¨ë”©ëœ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” RNN(ìˆœí™˜ ì‹ ê²½ë§) ëª¨ë¸ì„ PyTorchë¡œ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ebc38e",
   "metadata": {},
   "source": [
    "## RNN ê¸°ë°˜ ê°ì„± ë¶„ë¥˜ ëª¨ë¸ êµ¬í˜„ (PyTorch)\n",
    "\n",
    "íŒ¨ë”©ì„ í†µí•´ ëª¨ë“  ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ ë§ì·„ìœ¼ë‹ˆ, ì´ì œ ì´ ë°ì´í„°ë¥¼ í™œìš©í•´ ê°ì„± ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” ìˆœí™˜ ì‹ ê²½ë§(RNN, Recurrent Neural Network) ëª¨ë¸ì„ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "RNNì€ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ëŒ€í‘œì ì¸ ì‹ ê²½ë§ êµ¬ì¡°ë¡œ, ì…ë ¥ ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì—¬ ë¬¸ë§¥ ì •ë³´ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì„ë² ë”©(Embedding) ë ˆì´ì–´ë¡œ ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•œ ë’¤, RNN ë ˆì´ì–´ë¥¼ ê±°ì³ ë§ˆì§€ë§‰ ì¶œë ¥ë§Œì„ ì‚¬ìš©í•´ ê¸ì •/ë¶€ì • ê°ì„±ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë˜í•œ, ëª¨ë¸ êµ¬ì¡°ì™€ ê° ë ˆì´ì–´ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì§ê´€ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆëŠ” ìš”ì•½ í•¨ìˆ˜ë„ í•¨ê»˜ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ PyTorchë¡œ RNN ëª¨ë¸ì„ ì •ì˜í•˜ê³ , ëª¨ë¸ êµ¬ì¡°ë¥¼ ì¶œë ¥í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0aa6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# =============================\n",
    "# 1. RNN ê¸°ë°˜ ê°ì„± ë¶„ë¥˜ ëª¨ë¸ ì •ì˜\n",
    "# =============================\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size=20000, embedding_dim=128, hidden_dim=128):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        # ë‹¨ì–´ ì¸ë±ìŠ¤ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ (ë‹¨ì–´ ì˜ë¯¸ë¥¼ ë²¡í„°ë¡œ í‘œí˜„)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # RNN ë ˆì´ì–´: ì‹œí€€ìŠ¤ ì •ë³´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì¶œë ¥ì„ ë°›ì•„ ì´ì§„ ë¶„ë¥˜ ìˆ˜í–‰\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        # ì‹œê·¸ëª¨ì´ë“œ(Sigmoid): 0~1 ì‚¬ì´ í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        rnn_out, hidden = self.rnn(embedded)  # rnn_out: (batch_size, seq_len, hidden_dim)\n",
    "        # ë§ˆì§€ë§‰ ì‹œì ì˜ RNN ì¶œë ¥ë§Œ ì‚¬ìš© (ë¬¸ì¥ ì „ì²´ ì •ë³´)\n",
    "        last_output = rnn_out[:, -1, :]  # (batch_size, hidden_dim)\n",
    "        output = self.fc(last_output)  # (batch_size, 1)\n",
    "        output = self.sigmoid(output)  # (batch_size, 1)\n",
    "        return output\n",
    "\n",
    "# =============================\n",
    "# 2. ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "# =============================\n",
    "model = SentimentRNN(vocab_size=20000, embedding_dim=128, hidden_dim=128)\n",
    "\n",
    "# =============================\n",
    "# 3. ëª¨ë¸ êµ¬ì¡° ìš”ì•½ í•¨ìˆ˜ êµ¬í˜„\n",
    "# =============================\n",
    "def model_summary(model, input_size):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ì˜ ê° ë ˆì´ì–´ë³„ ì…ì¶œë ¥ í˜•íƒœì™€ íŒŒë¼ë¯¸í„° ê°œìˆ˜ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "    - input_size: (batch_size, seq_len) í˜•íƒœì˜ ë”ë¯¸ ì…ë ¥ í¬ê¸°\n",
    "    \"\"\"\n",
    "    def register_hook(module):\n",
    "        def hook(module, input, output):\n",
    "            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "            module_idx = len(summary)\n",
    "            m_key = f\"{class_name}-{module_idx+1}\"\n",
    "            summary[m_key] = {}\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size()) if isinstance(input, tuple) else [input.size()]\n",
    "            summary[m_key][\"output_shape\"] = list(output.size()) if not isinstance(output, tuple) else [list(o.size()) for o in output]\n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.tensor(module.weight.size()))\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.tensor(module.bias.size()))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "        # nn.Sequential, nn.ModuleList, ì „ì²´ ëª¨ë¸ì€ ì œì™¸\n",
    "        if not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList) and not (module == model):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "    summary = {}\n",
    "    hooks = []\n",
    "    model.apply(register_hook)\n",
    "    # ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ forward pass (ì‹¤ì œ ë°ì´í„°ì™€ ë™ì¼í•œ í˜•íƒœ)\n",
    "    x = torch.randint(0, 20000, input_size)\n",
    "    model(x)\n",
    "    # hook ì œê±°\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    print(\"================================================================\")\n",
    "    print(\"Layer (type)               Output Shape         Param #\")\n",
    "    print(\"================================================================\")\n",
    "    total_params = 0\n",
    "    for layer in summary:\n",
    "        output_shape = summary[layer][\"output_shape\"]\n",
    "        if isinstance(output_shape[0], list):\n",
    "            output_shape = output_shape[0]  # RNNì˜ ê²½ìš° ì²« ë²ˆì§¸ ì¶œë ¥ë§Œ ì‚¬ìš©\n",
    "        params = summary[layer][\"nb_params\"]\n",
    "        print(f\"{layer:<25} {str(output_shape):<20} {params:>10,}\")\n",
    "        total_params += params\n",
    "    print(\"================================================================\")\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(\"================================================================\")\n",
    "\n",
    "# =============================\n",
    "# 4. ëª¨ë¸ êµ¬ì¡° ì¶œë ¥\n",
    "# =============================\n",
    "model_summary(model, (1, 80))  # batch_size=1, seq_len=80\n",
    "\n",
    "# ë‹¤ìŒ: ì‹¤ì œ ë°ì´í„°ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜í•˜ê³ , ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹ê³¼ DataLoaderë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb673b9",
   "metadata": {},
   "source": [
    "## PyTorchìš© ë°ì´í„° ì¤€ë¹„ ë° ëª¨ë¸ í•™ìŠµ\n",
    "\n",
    "ì´ì œ íŒ¨ë”©ëœ ë°ì´í„°ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜í•˜ê³ , íš¨ìœ¨ì ì¸ í•™ìŠµì„ ìœ„í•´ DataLoader(ë°ì´í„° ë°°ì¹˜ ìƒì„±ê¸°)ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ í…ì„œí”Œë¡œìš°ì˜ `fit`ê³¼ ë§¤ìš° ìœ ì‚¬í•˜ì§€ë§Œ, PyTorchì—ì„œëŠ” ì§ì ‘ í•™ìŠµ ë£¨í”„ë¥¼ êµ¬í˜„í•´ì•¼ í•˜ë¯€ë¡œ, ê° ë‹¨ê³„ì˜ ì›ë¦¬ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë˜í•œ, ì‹¤ì œ ë‹¨ì–´ ì‚¬ì „ì˜ í¬ê¸°ì— ë§ì¶° ëª¨ë¸ì„ ì¬ìƒì„±í•˜ê³ , ì˜µí‹°ë§ˆì´ì €(Optimizer)ì™€ ì†ì‹¤ í•¨ìˆ˜(Loss Function)ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, í•™ìŠµ ë° ê²€ì¦ ì •í™•ë„ë¥¼ ì§ì ‘ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ì™€ ì „ì²´ í•™ìŠµ ë£¨í”„ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ ë°ì´í„°ë¥¼ í…ì„œë¡œ ë³€í™˜í•˜ê³ , DataLoader ë° í•™ìŠµ ë£¨í”„ë¥¼ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec63fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =============================\n",
    "# 1. ì‹¤ì œ ë‹¨ì–´ ì‚¬ì „ í¬ê¸°ë¡œ ëª¨ë¸ ì¬ìƒì„±\n",
    "# =============================\n",
    "actual_vocab_size = len(word_to_idx)\n",
    "model = SentimentRNN(vocab_size=actual_vocab_size + 1, embedding_dim=128, hidden_dim=128)\n",
    "\n",
    "# =============================\n",
    "# 2. ë°ì´í„°ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜\n",
    "# =============================\n",
    "pad_x_train_tensor = torch.tensor(pad_x_train, dtype=torch.long)\n",
    "pad_x_test_tensor = torch.tensor(pad_x_test, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # (N, 1) í˜•íƒœë¡œ ë³€í™˜\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# =============================\n",
    "# 3. Datasetê³¼ DataLoader ìƒì„±\n",
    "# =============================\n",
    "train_dataset = TensorDataset(pad_x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(pad_x_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# =============================\n",
    "# 4. ì˜µí‹°ë§ˆì´ì €ì™€ ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •\n",
    "# =============================\n",
    "optimizer = optim.Adam(model.parameters())  # Adam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©\n",
    "criterion = nn.BCELoss()  # ì´ì§„ ë¶„ë¥˜ìš© Binary Cross Entropy Loss\n",
    "\n",
    "# =============================\n",
    "# 5. ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜ êµ¬í˜„\n",
    "# =============================\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    # 0.5ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜ ê²°ê³¼ ê³„ì‚°\n",
    "    predicted = (outputs > 0.5).float()\n",
    "    correct = (predicted == labels).float()\n",
    "    return correct.mean()\n",
    "\n",
    "# =============================\n",
    "# 6. í›ˆë ¨ í•¨ìˆ˜ êµ¬í˜„\n",
    "# =============================\n",
    "def train_model(model, train_loader, test_loader, epochs=15):\n",
    "    for epoch in range(epochs):\n",
    "        # [1] í›ˆë ¨ ë‹¨ê³„\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        num_batches = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()  # ê¸°ìš¸ê¸° ì´ˆê¸°í™”\n",
    "            outputs = model(batch_x)  # ì˜ˆì¸¡ê°’ ê³„ì‚°\n",
    "            loss = criterion(outputs, batch_y)  # ì†ì‹¤ ê³„ì‚°\n",
    "            loss.backward()  # ì—­ì „íŒŒ\n",
    "            optimizer.step()  # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\n",
    "            total_loss += loss.item()\n",
    "            total_acc += calculate_accuracy(outputs, batch_y)\n",
    "            num_batches += 1\n",
    "        # [2] ê²€ì¦ ë‹¨ê³„\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                val_acc += calculate_accuracy(outputs, batch_y)\n",
    "                val_batches += 1\n",
    "        # [3] ì—í­ë³„ ê²°ê³¼ ì¶œë ¥\n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'  loss: {total_loss/num_batches:.4f} - accuracy: {total_acc/num_batches:.4f} - val_loss: {val_loss/val_batches:.4f} - val_accuracy: {val_acc/val_batches:.4f}')\n",
    "\n",
    "# =============================\n",
    "# 7. ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰\n",
    "# =============================\n",
    "train_model(model, train_loader, test_loader, epochs=15)\n",
    "\n",
    "# ë‹¤ìŒ: IMDB ìŠ¤íƒ€ì¼ì˜ ë‹¨ì–´ ì‚¬ì „ì„ ì§ì ‘ êµ¬ì„±í•˜ê³ , íŠ¹ìˆ˜ í† í°ì„ ì¶”ê°€í•˜ëŠ” ê³¼ì •ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a7f820",
   "metadata": {},
   "source": [
    "## IMDB ìŠ¤íƒ€ì¼ ë‹¨ì–´ ì‚¬ì „ êµ¬ì„± ë° íŠ¹ìˆ˜ í† í° ì¶”ê°€\n",
    "\n",
    "ì‹¤ì œ ìì—°ì–´ ì²˜ë¦¬(NLP)ì—ì„œëŠ” ë‹¨ì–´ ì‚¬ì „ì„ ì–´ë–»ê²Œ êµ¬ì„±í•˜ëŠëƒê°€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. IMDB ë°ì´í„°ì…‹ì²˜ëŸ¼, íŠ¹ìˆ˜ í† í°(ì˜ˆ: <PAD>, <BOS>, <UNK>, <UNUSED>)ì„ ëª…í™•íˆ êµ¬ë¶„í•´ë‘ë©´ ì¶”í›„ ëª¨ë¸ í•´ì„ì´ë‚˜ ë””ì½”ë”©, OOV(Out-Of-Vocabulary) ì²˜ë¦¬ ë“±ì— í° ë„ì›€ì´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì—¬ê¸°ì„œëŠ” ê¸°ì¡´ NSMC(ë„¤ì´ë²„ ì˜í™”ë¦¬ë·°) ë‹¨ì–´ ì‚¬ì „ì„ IMDB ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì¦‰, ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ +3ë§Œí¼ ì´ë™ì‹œí‚¤ê³ , 0~3ë²ˆ ì¸ë±ìŠ¤ì— íŠ¹ìˆ˜ í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤. ë˜í•œ, ì¸ë±ìŠ¤â†’ë‹¨ì–´ë¡œ ë³€í™˜í•˜ëŠ” ì—­ë°©í–¥ ì‚¬ì „ë„ í•¨ê»˜ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ IMDB ìŠ¤íƒ€ì¼ ë‹¨ì–´ ì‚¬ì „ê³¼ ì—­ë°©í–¥ ì‚¬ì „ì„ ì§ì ‘ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f1c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 1. ê¸°ì¡´ ë‹¨ì–´ ì‚¬ì „ì—ì„œ íŠ¹ìˆ˜ í† í° ì œì™¸ ë° ì¸ë±ìŠ¤ ì¡°ì •\n",
    "# =============================\n",
    "origin_word_dict = {}\n",
    "for word, idx in word_to_idx.items():\n",
    "    if word != '<OOV>':  # <OOV> í† í°ì€ ì œì™¸\n",
    "        origin_word_dict[word] = idx - 1  # ì¸ë±ìŠ¤ë¥¼ 0ë¶€í„° ì‹œì‘í•˜ë„ë¡ ì¡°ì •\n",
    "\n",
    "# =============================\n",
    "# 2. IMDB ìŠ¤íƒ€ì¼ë¡œ ì¸ë±ìŠ¤ +3 ì´ë™\n",
    "# =============================\n",
    "word_dict = {k: (v + 3) for k, v in origin_word_dict.items()}\n",
    "\n",
    "# =============================\n",
    "# 3. íŠ¹ìˆ˜ í† í° ì¶”ê°€\n",
    "# =============================\n",
    "word_dict['<PAD>'] = 0      # íŒ¨ë”© í† í°\n",
    "word_dict['<BOS>'] = 1      # ë¬¸ì¥ ì‹œì‘(Beginning of Sequence)\n",
    "word_dict['<UNK>'] = 2      # ëª¨ë¥´ëŠ” ë‹¨ì–´(Unknown)\n",
    "word_dict['<UNUSED>'] = 3   # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í† í°\n",
    "\n",
    "print(\"=== íŠ¹ìˆ˜ í† í° í™•ì¸ ===\")\n",
    "print(f\"<PAD>: {word_dict['<PAD>']}\")\n",
    "print(f\"<BOS>: {word_dict['<BOS>']}\")\n",
    "print(f\"<UNK>: {word_dict['<UNK>']}\")\n",
    "print(f\"<UNUSED>: {word_dict['<UNUSED>']}\")\n",
    "\n",
    "print(f\"\\nì´ ë‹¨ì–´ ì‚¬ì „ í¬ê¸°: {len(word_dict)}\")\n",
    "print(f\"ë‹¨ì–´ ì‚¬ì „ ìµœëŒ€ ì¸ë±ìŠ¤: {max(word_dict.values())}\")\n",
    "\n",
    "# =============================\n",
    "# 4. ì¼ë°˜ ë‹¨ì–´ ì˜ˆì‹œ ì¶œë ¥\n",
    "# =============================\n",
    "print(\"\\n=== ì¼ë°˜ ë‹¨ì–´ ì˜ˆì‹œ ===\")\n",
    "sample_words = list(word_dict.keys())[4:10]  # íŠ¹ìˆ˜ í† í° ì œì™¸ 6ê°œ ë‹¨ì–´\n",
    "for word in sample_words:\n",
    "    print(f\"'{word}': {word_dict[word]}\")\n",
    "\n",
    "# =============================\n",
    "# 5. ì—­ë°©í–¥ ì‚¬ì „(ì¸ë±ìŠ¤â†’ë‹¨ì–´) ìƒì„±\n",
    "# =============================\n",
    "idx_to_word = {idx: word for word, idx in word_dict.items()}\n",
    "print(f\"\\nì—­ë°©í–¥ ì‚¬ì „ ìƒì„± ì™„ë£Œ: {len(idx_to_word)}ê°œ í•­ëª©\")\n",
    "\n",
    "# ë‹¤ìŒ: í•™ìŠµí•œ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì„ì˜ì˜ ë¬¸ì¥ì— ëŒ€í•´ ê°ì„± ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a555ed83",
   "metadata": {},
   "source": [
    "## ì‚¬ìš©ì ì…ë ¥ ë¬¸ì¥ ê°ì„± ì˜ˆì¸¡ í•¨ìˆ˜ êµ¬í˜„\n",
    "\n",
    "ëª¨ë¸ì„ í•™ìŠµí•œ í›„ì—ëŠ” ì‹¤ì œë¡œ ì„ì˜ì˜ ë¬¸ì¥(ì˜ˆ: ì‚¬ìš©ìê°€ ì§ì ‘ ì…ë ¥í•œ ë¬¸ì¥)ì— ëŒ€í•´ ê°ì„±(ê¸ì •/ë¶€ì •)ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ê°€ í•„ìš”í•©ë‹ˆë‹¤:\n",
    "\n",
    "1. ì…ë ¥ ë¬¸ì¥ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
    "2. ê° ë‹¨ì–´ë¥¼ ë‹¨ì–´ ì‚¬ì „ì˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜(ëª¨ë¥´ëŠ” ë‹¨ì–´ëŠ” <UNK>ë¡œ ì²˜ë¦¬)\n",
    "3. <BOS> í† í°ì„ ë¬¸ì¥ ì•ì— ì¶”ê°€\n",
    "4. ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´ íŒ¨ë”© ì ìš©\n",
    "5. ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ì˜ˆì¸¡ê°’(í™•ë¥ ) ì¶œë ¥\n",
    "6. ì„ê³„ê°’(0.5) ê¸°ì¤€ìœ¼ë¡œ ê¸ì •/ë¶€ì • íŒë‹¨\n",
    "\n",
    "ì´ì œ ìœ„ ê³¼ì •ì„ ëª¨ë‘ í¬í•¨í•œ ì˜ˆì¸¡ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7222acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def predict(words):\n",
    "    # 1. ì…ë ¥ ë¬¸ì¥ì„ ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ ë¶„ë¦¬\n",
    "    user_sentence = words.split(' ')\n",
    "    # 2. ê° ë‹¨ì–´ë¥¼ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ (ëª¨ë¥´ëŠ” ë‹¨ì–´ëŠ” <UNK>ë¡œ ëŒ€ì²´)\n",
    "    encoded_sentence = [word_dict['<BOS>']]  # ë¬¸ì¥ ì‹œì‘ í† í° ì¶”ê°€\n",
    "    for word in user_sentence:\n",
    "        try:\n",
    "            # ì‚¬ì „ì— ìˆê³ , ì¸ë±ìŠ¤ê°€ 2ë§Œ ë¯¸ë§Œì´ë©´ í•´ë‹¹ ì¸ë±ìŠ¤ ì‚¬ìš©\n",
    "            if word_dict[word] < 20000:\n",
    "                encoded_sentence.append(word_dict[word])\n",
    "            else:\n",
    "                encoded_sentence.append(word_dict['<UNK>'])\n",
    "        except KeyError:\n",
    "            # ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ëŠ” <UNK>ë¡œ ì²˜ë¦¬\n",
    "            encoded_sentence.append(word_dict['<UNK>'])\n",
    "    # 3. íŒ¨ë”© ì ìš© (ê¸¸ì´ 80ìœ¼ë¡œ ë§ì¶¤)\n",
    "    def pad_sequences(sequences, maxlen=80, padding='pre', value=0):\n",
    "        result = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) > maxlen:\n",
    "                seq = seq[-maxlen:]  # ë’¤ìª½ maxlenê°œë§Œ ë‚¨ê¹€\n",
    "            else:\n",
    "                pad_length = maxlen - len(seq)\n",
    "                seq = [value] * pad_length + seq  # ì•ì— íŒ¨ë”© ì¶”ê°€\n",
    "            result.append(seq)\n",
    "        return np.array(result)\n",
    "    pad_encoded_sentence = pad_sequences([encoded_sentence], maxlen=80)\n",
    "    # 4. ëª¨ë¸ì— ì…ë ¥ (PyTorch í…ì„œë¡œ ë³€í™˜)\n",
    "    pad_encoded_tensor = torch.tensor(pad_encoded_sentence, dtype=torch.long)\n",
    "    model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜\n",
    "    with torch.no_grad():\n",
    "        result = model(pad_encoded_tensor)\n",
    "    print(result)\n",
    "    # 5. ì„ê³„ê°’(0.5) ê¸°ì¤€ìœ¼ë¡œ ê¸ì •/ë¶€ì • íŒë‹¨\n",
    "    if result[0][0] > 0.5:\n",
    "        print(f'{result[0][0]*100}%ë¡œ ê¸ì •')\n",
    "    else:\n",
    "        print('ë¶€ì •')\n",
    "\n",
    "# ë‹¤ìŒ: ì‹¤ì œë¡œ predict í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ì˜ì˜ ë¬¸ì¥ì— ëŒ€í•œ ê°ì„± ì˜ˆì¸¡ì„ ì‹¤ìŠµí•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69227e6d",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ í•´ì„ ë° ì‹¤ì „ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ì•ì„œ í•™ìŠµí•œ ê°ì„± ë¶„ë¥˜(sentiment classification) ëª¨ë¸ì˜ ì‹¤ì œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤. ì´ ê³¼ì •ì€ ëª¨ë¸ì´ ì‹¤ì œë¡œ ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ê¸ì •/ë¶€ì •ì„ íŒë‹¨í•˜ëŠ”ì§€ ê²€ì¦í•˜ëŠ” ì¤‘ìš”í•œ ë‹¨ê³„ì…ë‹ˆë‹¤.\n",
    "\n",
    "### ì™œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì§ì ‘ í™•ì¸í•´ì•¼ í• ê¹Œìš”?\n",
    "- **ëª¨ë¸ì˜ ì‹¤ì „ ì ìš© ê°€ëŠ¥ì„±**: ë‹¨ìˆœíˆ í‰ê°€ ì§€í‘œ(ì •í™•ë„, F1 ë“±)ë§Œìœ¼ë¡œëŠ” ì‹¤ì œ ì‚¬ìš© í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ì„ ì™„ì „íˆ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ì˜ˆì‹œ ë¬¸ì¥ì— ëŒ€í•´ ëª¨ë¸ì´ ì–´ë–»ê²Œ ë°˜ì‘í•˜ëŠ”ì§€ ì§ì ‘ í™•ì¸í•´ì•¼ ì‹ ë¢°ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- **ëª¨ë¸ì˜ í•œê³„ íŒŒì•…**: ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë³´ë©´ ëª¨ë¸ì´ ì–´ë–¤ ë¬¸ì¥ì— ì•½í•œì§€, ì˜¤ë¶„ë¥˜(ì˜ëª» ë¶„ë¥˜)í•˜ëŠ” íŒ¨í„´ì´ ìˆëŠ”ì§€ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ì´ë‚˜ ëª¨ë¸ ê°œì„ ì˜ ì‹¤ë§ˆë¦¬ê°€ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ ì‹¤ì œë¡œ ê¸ì •ì ì¸ ë¬¸ì¥ê³¼ ë¶€ì •ì ì¸ ë¬¸ì¥ ê°ê°ì— ëŒ€í•´ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ì´í›„ì—ëŠ” ì¸ê³µì§€ëŠ¥ í”„ë¡œì íŠ¸ ì „ì²´ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•´ ì‹¬ì¸µì ìœ¼ë¡œ ì •ë¦¬í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efbe67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ë°°ìš°ë“¤ ì—°ê¸°ê°€ ì •ë§ í›Œë¥­í–ˆìŠµë‹ˆë‹¤'ë¼ëŠ” ê¸ì •ì ì¸ ë¬¸ì¥ì— ëŒ€í•´ ì˜ˆì¸¡\n",
    "# predict í•¨ìˆ˜ëŠ” ì…ë ¥ëœ ë¬¸ì¥ì„ í† í°í™”(tokenize)í•˜ê³ , ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ì˜ˆì¸¡ í™•ë¥ ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "# ë°˜í™˜ê°’ì€ ê¸ì •ì¼ í™•ë¥ (0~1 ì‚¬ì´ ê°’)ì…ë‹ˆë‹¤.\n",
    "predict(\"ë°°ìš°ë“¤ ì—°ê¸°ê°€ ì •ë§ í›Œë¥­í–ˆìŠµë‹ˆë‹¤\")  # ê¸ì • ë¬¸ì¥ ì˜ˆì¸¡\n",
    "\n",
    "# ì¶œë ¥ ê²°ê³¼ ì˜ˆì‹œ:\n",
    "# tensor([[0.5274]])\n",
    "# 52.74%ë¡œ ê¸ì •\n",
    "\n",
    "# ë‹¤ìŒ: ë¶€ì •ì ì¸ ë¬¸ì¥ì— ëŒ€í•´ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391cb57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ì´ ì˜í™” ì •ë§ ì¬ë¯¸ì—†ì–´ìš”'ë¼ëŠ” ë¶€ì •ì ì¸ ë¬¸ì¥ì— ëŒ€í•´ ì˜ˆì¸¡\n",
    "# ì´ ë¬¸ì¥ì€ ë¶€ì • ê°ì •ì„ ë‹´ê³  ìˆìœ¼ë¯€ë¡œ, ëª¨ë¸ì´ ë‚®ì€ ê¸ì • í™•ë¥ ì„ ë°˜í™˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "predict(\"ì´ ì˜í™” ì •ë§ ì¬ë¯¸ì—†ì–´ìš”\")  # ë¶€ì • ë¬¸ì¥ ì˜ˆì¸¡\n",
    "\n",
    "# ì¶œë ¥ ê²°ê³¼ ì˜ˆì‹œ:\n",
    "# tensor([[0.4615]])\n",
    "# 46.15%ë¡œ ë¶€ì •\n",
    "\n",
    "# ë‹¤ìŒ: ì¸ê³µì§€ëŠ¥ í•™ìŠµ í”„ë¡œì íŠ¸ì˜ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ë‹¨ê³„ë³„ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf0717",
   "metadata": {},
   "source": [
    "## ì¸ê³µì§€ëŠ¥(AI) í•™ìŠµ í”„ë¡œì íŠ¸ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¬ì¸µ ì •ë¦¬\n",
    "\n",
    "ì¸ê³µì§€ëŠ¥ í”„ë¡œì íŠ¸ëŠ” ë‹¨ìˆœíˆ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì— ê·¸ì¹˜ì§€ ì•Šê³ , ë°ì´í„° ìˆ˜ì§‘ë¶€í„° ëª¨ë¸ ë°°í¬ê¹Œì§€ ì—¬ëŸ¬ ë‹¨ê³„ê°€ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê° ë‹¨ê³„ì˜ ëª©ì ê³¼ ì¤‘ìš”ì„±ì„ ê¹Šì´ ìˆê²Œ ì´í•´í•˜ëŠ” ê²ƒì´ ì‹¤ì œ í”„ë¡œì íŠ¸ ì„±ê³µì˜ í•µì‹¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "### 1. í”„ë¡œì íŠ¸ ëª©í‘œ ì„¤ì •\n",
    "- **ì™œ í•„ìš”í•œê°€?**\n",
    "  - ëª…í™•í•œ ëª©í‘œê°€ ì—†ìœ¼ë©´, ì–´ë–¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ì•¼ í• ì§€, ì–´ë–¤ ëª¨ë¸ì„ ì¨ì•¼ í• ì§€ ê²°ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
    "  - ë°ì´í„° ìˆ˜ì§‘ ë° ë¼ë²¨ë§ì€ ì‹œê°„ê³¼ ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì‘ì—…ì´ë¯€ë¡œ, ì´ˆê¸°ì— ëª©í‘œë¥¼ ëª…í™•íˆ í•´ì•¼ ë¶ˆí•„ìš”í•œ ë‚­ë¹„ë¥¼ ë§‰ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- **ì‹¤ë¬´ ì˜ˆì‹œ**: ê°ì„± ë¶„ì„ ëª¨ë¸ì„ ë§Œë“¤ê³ ì í•œë‹¤ë©´, ì˜í™” ë¦¬ë·°ì—ì„œ ê¸ì •/ë¶€ì • ê°ì •ì„ ë¶„ë¥˜í•˜ëŠ” ê²ƒì´ ëª©í‘œê°€ ë©ë‹ˆë‹¤.\n",
    "\n",
    "### 2. ë°ì´í„° ìˆ˜ì§‘\n",
    "- **ì™œ ì¤‘ìš”í•œê°€?**\n",
    "  - ë°ì´í„°ì˜ ì–‘ê³¼ ì§ˆì´ ëª¨ë¸ ì„±ëŠ¥ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n",
    "  - ë°ì´í„°ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ í¸í–¥(bias)ì´ ìˆë‹¤ë©´, ëª¨ë¸ì€ ì‹¤ì œ í™˜ê²½ì—ì„œ ì˜¤ì‘ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- **ì‹¤ë¬´ ì˜ˆì‹œ**: ë‹¤ì–‘í•œ ì¥ë¥´ì™€ ì—°ë ¹ëŒ€ì˜ ì˜í™” ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•´ì•¼, íŠ¹ì • ì¥ë¥´ë‚˜ ì—°ë ¹ëŒ€ì— ì¹˜ìš°ì¹˜ì§€ ì•Šì€ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### 3. ë°ì´í„° ë¶„ì„\n",
    "- **ì™œ í•„ìš”í•œê°€?**\n",
    "  - ìˆ˜ì§‘í•œ ë°ì´í„°ê°€ ì‹¤ì œë¡œ ëª©í‘œì— ì í•©í•œì§€, ê²°ì¸¡ì¹˜(missing value)ë‚˜ ì´ìƒì¹˜(outlier)ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "  - ë°ì´í„°ì˜ ë¶„í¬ì™€ íŠ¹ì„±ì„ íŒŒì•…í•˜ë©´, ì´í›„ ì „ì²˜ë¦¬ ë° ëª¨ë¸ ì„¤ê³„ì— í° ë„ì›€ì´ ë©ë‹ˆë‹¤.\n",
    "- **ì‹¤ë¬´ ì˜ˆì‹œ**: ê¸ì •/ë¶€ì • ë¹„ìœ¨ì´ 9:1ì´ë¼ë©´, ë°ì´í„° ë¶ˆê· í˜• ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### 4. ë°ì´í„° ë¼ë²¨ë§(Labeling)\n",
    "- **ì™œ ì¤‘ìš”í•œê°€?**\n",
    "  - ì§€ë„í•™ìŠµ(supervised learning)ì—ì„œëŠ” ì •í™•í•œ ì •ë‹µ(ground truth)ì´ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "  - ë¼ë²¨ë§ì´ ë¶€ì •í™•í•˜ë©´, ëª¨ë¸ì´ ì˜ëª»ëœ íŒ¨í„´ì„ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "- **ì‹¤ë¬´ ì˜ˆì‹œ**: í…ìŠ¤íŠ¸ ë¶„ë¥˜ì—ì„œëŠ” ì‚¬ëŒì´ ì§ì ‘ ë¬¸ì¥ì„ ì½ê³  ê¸ì •/ë¶€ì •ìœ¼ë¡œ íƒœê¹…í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "### 5. ë°ì´í„° ì „ì²˜ë¦¬(Preprocessing)\n",
    "- **ì™œ í•„ìš”í•œê°€?**\n",
    "  - ì›ë³¸ ë°ì´í„°ì—ëŠ” ë…¸ì´ì¦ˆ(noise)ë‚˜ ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ ë§ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "  - ì „ì²˜ë¦¬ë¥¼ í†µí•´ ëª¨ë¸ì´ í•™ìŠµí•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "- **ì£¼ìš” ì‘ì—…**: ë¶ˆìš©ì–´(stopword) ì œê±°, í† í°í™”(tokenization), ì •ê·œí™”(normalization), í•™ìŠµ/í‰ê°€ ë°ì´í„° ë¶„ë¦¬ ë“±\n",
    "\n",
    "### 6. ëª¨ë¸ ë¹Œë“œ(Build)\n",
    "- **ì™œ ì¤‘ìš”í•œê°€?**\n",
    "  - ë¬¸ì œì˜ íŠ¹ì„±ê³¼ ë°ì´í„°ì— ë§ëŠ” ëª¨ë¸ êµ¬ì¡°(ì•„í‚¤í…ì²˜, architecture)ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "  - ì˜ëª»ëœ ëª¨ë¸ ì„ íƒì€ ì„±ëŠ¥ ì €í•˜ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- **ì‹¤ë¬´ ì˜ˆì‹œ**: ê°„ë‹¨í•œ ë¬¸ì œì—ëŠ” ì„ í˜• íšŒê·€(linear regression), ë³µì¡í•œ ë¬¸ì œì—ëŠ” ë”¥ëŸ¬ë‹(deep learning) ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "### 7. ëª¨ë¸ í•™ìŠµ(Training)\n",
    "- **ì™œ í•„ìš”í•œê°€?**\n",
    "  - ëª¨ë¸ì´ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•˜ë„ë¡ ê°€ì¤‘ì¹˜(weight)ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.\n",
    "  - ì†ì‹¤ í•¨ìˆ˜(loss function)ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜(optimizer)ì„ í†µí•´ ì„±ëŠ¥ì„ ìµœëŒ€í™”í•©ë‹ˆë‹¤.\n",
    "- **ì‹¤ë¬´ ì˜ˆì‹œ**: í•™ìŠµ ê³¼ì •ì—ì„œ ê³¼ì í•©(overfitting)ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ êµì°¨ ê²€ì¦(cross-validation)ì´ë‚˜ ì¡°ê¸° ì¢…ë£Œ(early stopping)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "### 8. ëª¨ë¸ í‰ê°€(Evaluation)\n",
    "- **ì™œ ì¤‘ìš”í•œê°€?**\n",
    "  - ëª¨ë¸ì´ ì‹¤ì œë¡œ ì–¼ë§ˆë‚˜ ì˜ ë™ì‘í•˜ëŠ”ì§€, ì¼ë°˜í™” ëŠ¥ë ¥ì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "  - í‰ê°€ ì§€í‘œ(metric)ëŠ” ë¬¸ì œ ìœ í˜•ì— ë”°ë¼ ë‹¤ë¥´ë©°, í…ìŠ¤íŠ¸ ìƒì„±ì²˜ëŸ¼ ìë™ í‰ê°€ê°€ ì–´ë ¤ìš´ ê²½ìš° ì‚¬ëŒì´ ì§ì ‘ í‰ê°€í•´ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "- **ì‹¤ë¬´ ì˜ˆì‹œ**: ê°ì„± ë¶„ë¥˜ì—ì„œëŠ” ì •í™•ë„(accuracy), ì •ë°€ë„(precision), ì¬í˜„ìœ¨(recall), F1 ì ìˆ˜(f1-score) ë“±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "### 9. ëª¨ë¸ ë°°í¬(Deployment)\n",
    "- **ì™œ í•„ìš”í•œê°€?**\n",
    "  - í•™ìŠµëœ ëª¨ë¸ì„ ì‹¤ì œ ì„œë¹„ìŠ¤ì— ì ìš©í•´ì•¼ ë¹„ë¡œì†Œ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ê°€ ì°½ì¶œë©ë‹ˆë‹¤.\n",
    "  - ë°°í¬ í›„ì—ë„ ì§€ì†ì ìœ¼ë¡œ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•˜ê³ , í•„ìš”ì‹œ ëª¨ë¸ì„ ì¬í•™ìŠµí•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "- **ì‹¤ë¬´ ì˜ˆì‹œ**: REST API í˜•íƒœë¡œ ëª¨ë¸ì„ ë°°í¬í•˜ì—¬, ì›¹/ì•±ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "ì´ë ‡ê²Œ ê° ë‹¨ê³„ê°€ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ì•¼ ì¸ê³µì§€ëŠ¥ í”„ë¡œì íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ì„±ë©ë‹ˆë‹¤. \n",
    "\n",
    "**ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´:**\n",
    "ì´ì œ ì‹¤ì œë¡œ í”„ë¡œì íŠ¸ì˜ ê° ë‹¨ê³„ë³„ë¡œ í•„ìš”í•œ ì½”ë“œë¥¼ ì–´ë–»ê²Œ ì‘ì„±í•˜ëŠ”ì§€, ì‹¤ìŠµ ì˜ˆì‹œë¥¼ í†µí•´ êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
