{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64733c91",
   "metadata": {},
   "source": [
    "# 자연어처리(NLP) 입문과 실습\n",
    "\n",
    "이 노트북에서는 자연어처리(Natural Language Processing, NLP)의 핵심 개념과 실제 데이터 활용 방법을 단계별로 학습합니다. 단순히 이론을 나열하는 것이 아니라, 왜 NLP가 중요한지, 실제로 어떻게 데이터를 다루는지, 그리고 각 단계에서 어떤 고민이 필요한지까지 깊이 있게 다룹니다.\n",
    "\n",
    "## 학습 목표\n",
    "- 자연어처리(NLP)의 주요 목적과 필요성을 이해한다.\n",
    "- NLP의 대표적 작업인 자연어이해(NLU)와 자연어생성(NLG)의 차이와 예시를 설명할 수 있다.\n",
    "- 실제로 NLP 데이터를 다루기 위해 필요한 라이브러리 설치 및 환경 구성을 할 수 있다.\n",
    "- 데이터셋(datasets) 라이브러리를 활용하여 실제 자연어 데이터를 불러오고 확인하는 방법을 익힌다.\n",
    "\n",
    "## 전체 흐름 소개\n",
    "1. **NLP의 개념과 필요성**: 왜 자연어처리가 중요한지, 어떤 문제를 해결하는지 살펴봅니다.\n",
    "2. **NLP 작업의 분류**: 자연어이해(NLU)와 자연어생성(NLG)의 차이와 실제 적용 예시를 학습합니다.\n",
    "3. **실습 환경 준비**: 데이터셋 라이브러리 설치 및 불러오기 과정을 실습합니다.\n",
    "4. **데이터 확인**: 실제 자연어 데이터를 어떻게 다루는지 단계별로 살펴봅니다.\n",
    "\n",
    "이제 자연어처리의 기본 개념부터 시작하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dace20",
   "metadata": {},
   "source": [
    "## 자연어처리(NLP)란 무엇인가?\n",
    "\n",
    "자연어처리(Natural Language Processing, NLP)는 인간이 일상적으로 사용하는 언어(자연어)를 컴퓨터가 이해하고, 해석하며, 생성할 수 있도록 하는 인공지능(AI) 분야입니다.\n",
    "\n",
    "### 왜 NLP가 필요한가?\n",
    "- 인간의 언어는 구조가 복잡하고, 맥락에 따라 의미가 달라집니다. 컴퓨터는 숫자나 명확한 규칙에는 강하지만, 자연어처럼 모호하고 다양한 표현을 이해하는 데는 한계가 있습니다.\n",
    "- NLP는 이러한 한계를 극복하여, 검색엔진, 챗봇, 번역기, 감정 분석 등 실생활에서 매우 다양한 서비스의 기반이 됩니다.\n",
    "\n",
    "이제 NLP의 대표적인 작업들을 분류해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26eb5e9",
   "metadata": {},
   "source": [
    "## NLP 작업의 분류: NLU와 NLG\n",
    "\n",
    "NLP에서 수행하는 작업은 크게 두 가지로 나눌 수 있습니다.\n",
    "\n",
    "- **자연어이해(NLU, Natural Language Understanding)**: 컴퓨터가 인간의 언어를 읽고, 그 의미와 의도를 파악하는 과정입니다. 예를 들어, 문장의 감정을 분석하거나, 질문에 대한 답을 찾는 작업이 여기에 해당합니다.\n",
    "    - *왜 중요한가?* 컴퓨터가 텍스트의 의미를 제대로 이해해야만, 적절한 답변이나 처리를 할 수 있기 때문입니다.\n",
    "\n",
    "- **자연어생성(NLG, Natural Language Generation)**: 컴퓨터가 의미 있는 문장이나 텍스트를 직접 만들어내는 과정입니다. 예를 들어, 번역기에서 새로운 언어로 문장을 생성하거나, 챗봇이 자연스러운 답변을 만들어내는 것이 여기에 해당합니다.\n",
    "    - *왜 중요한가?* 사용자가 이해할 수 있는 자연스러운 언어로 정보를 제공해야, 실제 서비스에서 활용될 수 있기 때문입니다.\n",
    "\n",
    "이제 실제로 NLP 데이터를 다루기 위해 필요한 환경을 준비해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d992bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets  # 🤖 NLP 데이터셋을 쉽게 불러올 수 있는 라이브러리(datasets) 설치\n",
    "# 'datasets'는 다양한 자연어처리 데이터셋을 간편하게 다운로드하고 사용할 수 있게 해줍니다\n",
    "# 다음: 설치한 라이브러리를 실제로 불러와서 사용할 준비를 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets  # 데이터셋(datasets) 라이브러리를 불러옵니다\n",
    "# 이 라이브러리는 NLP 실습에서 자주 사용되는 데이터셋을 쉽게 불러오고 관리할 수 있도록 도와줍니다\n",
    "# 주의: 실행 환경에 따라 경고 메시지가 나올 수 있지만, 대부분의 경우 실습에는 큰 영향이 없습니다\n",
    "# 다음: 데이터셋 라이브러리를 활용하여 실제 데이터를 불러오고 확인하는 방법을 알아봅니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16538e19",
   "metadata": {},
   "source": [
    "### 데이터 확인 단계 안내\n",
    "\n",
    "이제 데이터셋 라이브러리를 활용하여 실제 자연어 데이터를 불러오고, 그 구조와 내용을 확인하는 방법을 실습해보겠습니다.\n",
    "\n",
    "다음 단계에서는 대표적인 NLP 데이터셋을 불러오고, 데이터의 예시를 직접 살펴볼 예정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8974ea96",
   "metadata": {},
   "source": [
    "## AG News 데이터셋 불러오기와 구조 이해\n",
    "\n",
    "머신러닝, 특히 자연어 처리(NLP)에서는 실제 텍스트 데이터를 이용해 모델을 학습시키는 것이 매우 중요합니다. 그 중 AG News 데이터셋은 뉴스 기사 텍스트와 해당 기사 카테고리(라벨)가 포함된 대표적인 분류용 데이터셋입니다. 이 데이터셋을 활용하면 텍스트 분류(Text Classification) 문제를 실습할 수 있습니다.\n",
    "\n",
    "여기서는 Hugging Face의 `datasets` 라이브러리를 사용해 AG News 데이터셋을 불러오겠습니다. 이 라이브러리는 대용량 데이터셋을 손쉽게 다운로드하고, 다양한 포맷으로 다룰 수 있도록 해줍니다.\n",
    "\n",
    "이제 실제로 AG News 데이터셋을 불러오는 코드를 살펴보겠습니다. 다음 코드 셀에서는 데이터셋을 메모리로 로드하는 과정을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AG News 데이터셋을 불러옵니다\n",
    "import datasets  # Hugging Face의 데이터셋 라이브러리 임포트\n",
    "\n",
    "dataset = datasets.load_dataset('ag_news')  # 'ag_news' 데이터셋을 다운로드 및 로드\n",
    "\n",
    "# 다음 단계: 데이터셋의 실제 샘플 구조를 확인해보겠습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74986d77",
   "metadata": {},
   "source": [
    "### 데이터셋 샘플 구조 살펴보기\n",
    "\n",
    "데이터를 불러왔다면, 실제로 어떤 형태로 저장되어 있는지 확인하는 것이 중요합니다. 이는 데이터 전처리, 토큰화, 라벨 인코딩 등 후속 작업을 설계할 때 반드시 필요한 과정입니다.\n",
    "\n",
    "AG News 데이터셋의 각 샘플은 'text'(기사 본문)와 'label'(카테고리)로 구성되어 있습니다. 아래 코드에서는 첫 번째 학습 샘플을 출력하여 구조를 직접 확인합니다.\n",
    "\n",
    "이제 실제 샘플을 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c601818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋의 첫 번째 학습 샘플을 출력합니다\n",
    "print(dataset['train'][0])  # {'text': ..., 'label': ...} 형태로 출력됨\n",
    "\n",
    "# 다음 단계: 다양한 텍스트 생성(NLG) 작업의 예시를 살펴보겠습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a152c6",
   "metadata": {},
   "source": [
    "## 자연어 생성(NLG, Natural Language Generation) 주요 활용 예시\n",
    "\n",
    "자연어 생성(NLG)은 단순히 텍스트를 분류하거나 감정을 분석하는 것에서 나아가, 인공지능이 직접 새로운 텍스트를 만들어내는 기술입니다. NLG는 실제로 매우 다양한 분야에서 활용되고 있습니다. 대표적인 예시는 다음과 같습니다.\n",
    "\n",
    "- **텍스트 요약(Summarization)**: 긴 문서를 핵심만 간추려 짧은 요약문을 생성\n",
    "- **기계 번역(Machine Translation)**: 한 언어의 텍스트를 다른 언어로 자동 변환\n",
    "- **챗봇(Chatbot)**: 사용자와 자연스러운 대화를 생성\n",
    "- **코드 생성(Code Generation)**: 자연어 설명을 바탕으로 코드 자동 생성\n",
    "- **창작(Generative Writing)**: 소설, 시, 기사 등 창의적인 글쓰기 자동화\n",
    "\n",
    "이처럼 NLG는 단순 정보 전달을 넘어서, 인간과 유사한 언어 표현을 만들어내는 데 필수적인 기술입니다.\n",
    "\n",
    "이제 본격적으로 텍스트 생성의 실제 구현을 살펴보겠습니다. 다음 단계에서는 대규모 웹 텍스트 데이터셋(OpenWebText)을 스트리밍 방식으로 불러오는 방법을 알아봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c57502",
   "metadata": {},
   "source": [
    "### 텍스트 생성 실습을 위한 대규모 데이터셋 준비\n",
    "\n",
    "대형 언어 모델(LLM, Large Language Model)이나 텍스트 생성 모델을 학습하려면, 방대한 양의 실제 텍스트 데이터가 필요합니다. OpenWebText(OWT)는 웹에서 수집된 대규모 텍스트 데이터셋으로, GPT-2 등 유명한 언어 모델의 학습에 사용된 바 있습니다.\n",
    "\n",
    "여기서는 Hugging Face의 `load_dataset` 함수와 스트리밍(streaming) 옵션을 활용하여, OpenWebText 데이터셋을 메모리에 모두 올리지 않고도 한 줄씩 읽어올 수 있도록 설정합니다. 이는 대용량 데이터셋을 다룰 때 매우 효율적인 방법입니다.\n",
    "\n",
    "아래 코드에서는 Parquet 포맷으로 저장된 OWT 데이터셋을 스트리밍 방식으로 불러오는 과정을 보여줍니다. 이 방식은 메모리 사용량을 최소화하면서도, 필요한 만큼 데이터를 즉시 처리할 수 있다는 장점이 있습니다.\n",
    "\n",
    "이제 실제로 OWT 데이터셋을 스트리밍으로 불러오는 코드를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenWebText(OWT) 데이터셋을 스트리밍 방식으로 불러옵니다\n",
    "from datasets import load_dataset  # 데이터셋 로드 함수 임포트\n",
    "\n",
    "owt_stream = load_dataset(\n",
    "    \"parquet\",  # 데이터 포맷: Parquet(대용량 데이터에 적합한 컬럼 기반 저장 포맷)\n",
    "    data_files=\"hf://datasets/Skylion007/openwebtext@refs/convert/parquet/plain_text/partial-train/*.parquet\",  # 데이터 파일 경로\n",
    "    split=\"train\",  # 학습용 데이터만 선택\n",
    "    streaming=True  # 스트리밍 옵션 활성화(메모리에 모두 올리지 않고 한 줄씩 읽음)\n",
    ")\n",
    "\n",
    "# 다음 단계: 스트리밍된 데이터셋에서 샘플을 추출하고, 실제 텍스트를 확인해보겠습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9d5ad6",
   "metadata": {},
   "source": [
    "## 자연어 생성(Natural Language Generation, NLG) 데이터의 특징과 활용\n",
    "\n",
    "자연어 생성 작업(NLG)은 컴퓨터가 사람처럼 자연스러운 문장을 만들어내는 기술입니다. 이때 사용하는 데이터는 문맥(Context)이 자연스럽게 이어지는 연속적인 텍스트가 주로 활용됩니다. 왜냐하면, 실제 언어는 앞뒤 문맥에 따라 의미가 달라지기 때문입니다. 예를 들어, 소설이나 뉴스 기사와 같이 문장이 서로 연결되어 있는 데이터가 대표적입니다.\n",
    "\n",
    "하지만, 만약 모델의 목적이 단순히 짧은 문장(단문, short sentence)만을 생성하는 것이라면, 긴 문맥이 반드시 필요하지 않을 수도 있습니다. 즉, 데이터의 문맥 길이와 연속성은 모델의 활용 목적에 따라 중요도가 달라집니다.\n",
    "\n",
    "또한, 텍스트 생성 태스크에서는 입력 데이터(Input)와 정답 데이터(Target)가 동일한 경우가 많습니다. 예를 들어, 다음 단어를 예측하거나, 문장 전체를 복원하는 언어 모델링(Language Modeling)에서는 입력과 타겟이 한 글자씩 밀려있는 형태로 구성됩니다.\n",
    "\n",
    "이제 실제로 자연어 데이터가 어떤 형태로 구성되어 있는지, 데이터를 살펴보는 코드를 실행해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d292cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# owt_stream은 OpenWebText와 같이 자연스러운 문맥을 가진 텍스트 데이터 스트림입니다\n",
    "for i, ex in enumerate(owt_stream):\n",
    "    print(ex)  # 각 예시(ex)는 하나의 문서 또는 문단을 나타냅니다\n",
    "    if i == 2:\n",
    "        break  # 처음 3개 예시만 출력하여 데이터 구조를 확인합니다\n",
    "\n",
    "# 다음: 기계 번역(Machine Translation) 데이터셋의 구조를 살펴보겠습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacc7f56",
   "metadata": {},
   "source": [
    "## 기계 번역(Machine Translation) 데이터의 구조와 특징\n",
    "\n",
    "기계 번역은 한 언어로 된 문장을 다른 언어로 자동 변환하는 작업입니다. 이 태스크의 데이터는 반드시 \"입력 언어\"와 \"출력 언어\"가 쌍(Pair)으로 존재해야 합니다. 예를 들어, 독일어(Deutsch, de)와 영어(English, en) 문장이 한 쌍으로 저장되어 있습니다.\n",
    "\n",
    "이런 병렬 데이터(parallel data)는 번역 모델이 두 언어 간의 의미적 대응 관계를 학습하는 데 필수적입니다. 특히, 대규모 공개 데이터셋인 WMT(Workshop on Machine Translation) 시리즈는 다양한 언어 쌍에 대해 고품질 번역 데이터를 제공합니다.\n",
    "\n",
    "이제 실제로 WMT14 데이터셋에서 독일어-영어 쌍을 스트리밍 방식으로 불러와, 데이터 구조를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206084c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets 라이브러리를 이용해 WMT14 독일어-영어 병렬 데이터셋을 스트리밍으로 불러옵니다\n",
    "# 'split=\"train\"'은 학습용 데이터를 의미하며, streaming=True는 메모리 효율적으로 데이터를 처리합니다\n",
    "dataset = datasets.load_dataset('wmt14', 'de-en', split=\"train\", streaming=True)\n",
    "\n",
    "temp = None  # 임시로 데이터를 저장할 변수\n",
    "for i, example in enumerate(dataset):\n",
    "    temp = example  # 첫 번째 예시만 가져옵니다\n",
    "    break\n",
    "\n",
    "print(temp)  # {'translation': {'de': '...', 'en': '...'}} 형태로 출력됩니다\n",
    "\n",
    "# 다음: 질의응답(QA, Question Answering) 데이터셋의 구조와 특징을 살펴보겠습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c98b16",
   "metadata": {},
   "source": [
    "## 질의응답(Question Answering, QA) 태스크의 데이터 구조와 활용\n",
    "\n",
    "질의응답(QA)은 주어진 문서(또는 문단)에서 질문에 대한 정답(Answer)을 추론하는 작업입니다. 이 태스크는 단순히 문장을 생성하는 것과 달리, 문서 내에서 필요한 정보를 정확히 찾아내는 능력이 요구됩니다.\n",
    "\n",
    "QA 데이터셋은 일반적으로 다음과 같은 구조를 가집니다:\n",
    "- **문서(Document)**: 질문에 답할 수 있는 정보가 포함된 텍스트\n",
    "- **질문(Question)**: 문서에 기반하여 답해야 하는 질의\n",
    "- **정답(Answer)**: 문서 내에서 추출 가능한 답변(또는 답변의 위치)\n",
    "\n",
    "이런 데이터 구조는 모델이 단순한 암기나 생성이 아닌, 문서 이해와 정보 추출 능력을 학습하도록 도와줍니다.\n",
    "\n",
    "다음 단계에서는 실제 QA 데이터셋을 불러오고, 그 구조를 코드로 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4c03c5",
   "metadata": {},
   "source": [
    "## 추출형 QA(Extractive Question Answering) 데이터셋 살펴보기\n",
    "\n",
    "질문 응답(QA, Question Answering) 시스템은 크게 두 가지 방식으로 나뉩니다. 그 중 **추출형 QA**는 주어진 문서(context)에서 정답이 되는 텍스트의 일부분을 그대로 \"추출\"하는 방식입니다. 예를 들어, SQuAD와 같은 데이터셋이 대표적입니다.\n",
    "\n",
    "이 방식이 중요한 이유는, 실제로 많은 정보 검색 및 챗봇 시스템에서 정답이 이미 문서 내에 존재하는 경우가 많기 때문입니다. 따라서, 추출형 QA는 자연어 이해(NLU)와 정보 검색(IR) 기술의 핵심적인 응용 분야입니다.\n",
    "\n",
    "이제 SQuAD 데이터셋에서 한 샘플을 직접 불러와 구조를 살펴보겠습니다. 이후, 생성형 QA와 비교해볼 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f28c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQuAD 데이터셋을 스트리밍 방식으로 불러옵니다\n",
    "import datasets  # 데이터셋 라이브러리 임포트\n",
    "\n",
    "dataset = datasets.load_dataset('squad', split=\"train\", streaming=True)  # SQuAD 학습 데이터셋 스트리밍 로드\n",
    "\n",
    "# 첫 번째 샘플만 추출하여 구조를 확인합니다\n",
    "temp = None\n",
    "for i, example in enumerate(dataset):\n",
    "    temp = example  # 첫 번째 샘플 저장\n",
    "    break  # 첫 샘플만 확인하고 반복문 종료\n",
    "\n",
    "print(temp)  # 샘플의 전체 구조 출력\n",
    "\n",
    "# 다음 단계: 생성형 QA 데이터셋(Natural Questions)과 비교해봅니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86357558",
   "metadata": {},
   "source": [
    "## 생성형 QA(Generative Question Answering) 데이터셋 살펴보기\n",
    "\n",
    "앞서 본 추출형 QA와 달리, **생성형 QA**는 문서 내에 정답이 명확히 존재하지 않을 수도 있습니다. 모델은 문서 내용을 바탕으로 자연스러운 문장 형태로 정답을 \"생성\"해야 합니다. 대표적으로 Natural Questions, NarrativeQA 등이 있습니다.\n",
    "\n",
    "이 방식은 복잡한 질의나 요약, 설명이 필요한 상황에서 유용합니다. 최근 대형 언어모델(LLM) 기반의 QA 시스템이 주로 이 방식을 사용합니다.\n",
    "\n",
    "이번에는 Natural Questions 데이터셋에서 한 샘플을 불러와, 문서가 HTML 구조로 되어 있음을 확인해보겠습니다. 이후, 문서 요약(Summarization) 태스크로 넘어갑니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698389b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Questions 데이터셋을 스트리밍 방식으로 불러옵니다\n",
    "import datasets  # 데이터셋 라이브러리 임포트\n",
    "from IPython.display import display, HTML  # Jupyter에서 HTML 렌더링을 위한 모듈\n",
    "\n",
    "dataset = datasets.load_dataset('natural_questions', split=\"train\", streaming=True)  # Natural Questions 학습 데이터셋 로드\n",
    "\n",
    "# 첫 번째 샘플만 추출하여 문서의 HTML 구조를 확인합니다\n",
    "temp = None\n",
    "for i, example in enumerate(dataset):\n",
    "    temp = example  # 첫 번째 샘플 저장\n",
    "    break  # 첫 샘플만 확인하고 반복문 종료\n",
    "\n",
    "# 문서의 HTML 내용을 Jupyter에서 시각적으로 확인합니다\n",
    "display(HTML(temp[\"document\"][\"html\"]))\n",
    "\n",
    "# 다음 단계: 문서 요약(Summarization) 태스크의 데이터셋 구조를 살펴봅니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae19f523",
   "metadata": {},
   "source": [
    "## 문서 요약(Summarization) 태스크 소개\n",
    "\n",
    "문서 요약은 긴 문서에서 핵심 내용을 간결하게 뽑아내는 자연어 처리(NLP) 기술입니다. QA와 달리, 요약 태스크는 문서 전체의 의미를 파악하고, 중요한 정보를 선택적으로 재구성하는 능력이 필요합니다.\n",
    "\n",
    "이 태스크는 뉴스 기사, 논문, 보고서 등 다양한 실생활 응용에 활용됩니다. 대표적인 데이터셋으로 CNN/DailyMail이 있으며, 이 데이터셋은 기사와 그에 대한 요약문(하이라이트)을 쌍으로 제공합니다.\n",
    "\n",
    "이제 CNN/DailyMail 데이터셋에서 한 샘플을 불러와, 실제 기사와 요약문이 어떻게 구성되어 있는지 살펴보겠습니다. 이후, 개체명 인식(NER) 태스크로 넘어갑니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN/DailyMail 데이터셋을 스트리밍 방식으로 불러옵니다\n",
    "import datasets  # 데이터셋 라이브러리 임포트\n",
    "\n",
    "dataset = datasets.load_dataset('cnn_dailymail', '3.0.0', split=\"train\", streaming=True)  # CNN/DailyMail 학습 데이터셋 로드\n",
    "\n",
    "# 첫 번째 샘플만 추출하여 기사와 요약문 구조를 확인합니다\n",
    "temp = None\n",
    "for i, example in enumerate(dataset):\n",
    "    temp = example  # 첫 번째 샘플 저장\n",
    "    break  # 첫 샘플만 확인하고 반복문 종료\n",
    "\n",
    "print(temp)  # 기사(article)와 요약문(highlights) 확인\n",
    "\n",
    "# 다음 단계: 개체명 인식(NER) 태스크의 개념과 데이터셋 구조를 살펴봅니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c691f9",
   "metadata": {},
   "source": [
    "## 개체명 인식(Named Entity Recognition, NER) 태스크 소개\n",
    "\n",
    "개체명 인식(NER)은 문장에서 인물, 장소, 기관, 날짜 등 의미 있는 고유명사(엔티티, Entity)를 식별하고 분류하는 작업입니다. NER은 정보 추출, 질의응답, 추천 시스템 등 다양한 NLP 응용의 기초가 됩니다.\n",
    "\n",
    "이 태스크는 단순히 단어를 찾는 것이 아니라, 문맥을 이해하여 적절한 엔티티 유형을 부여해야 하므로, 자연어 이해의 중요한 단계로 여겨집니다.\n",
    "\n",
    "다음 셀에서는 대표적인 NER 데이터셋을 불러와, 실제 데이터 구조를 살펴볼 예정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8adc9",
   "metadata": {},
   "source": [
    "## 어벤져스 NER 데이터 직접 생성하기\n",
    "\n",
    "실제 자연어처리(NLP) 프로젝트에서는 원하는 태스크에 맞는 데이터를 직접 만들어야 할 때가 많습니다. 이번에는 대표적인 시퀀스 태스크인 개체명 인식(Named Entity Recognition, NER)을 위해 어벤져스(Avengers) 관련 예시 데이터를 직접 만들어보겠습니다.\n",
    "\n",
    "NER은 문장 내의 토큰(단어) 각각에 대해 해당 토큰이 어떤 개체명(예: 인물, 조직, 장소 등)에 속하는지 태그를 부여하는 작업입니다. CoNLL-2003 형식의 BIO 태그 체계를 사용하며, 이는 실제 현업 및 대회에서도 널리 활용됩니다.\n",
    "\n",
    "이제 아래 코드에서는 어벤져스 관련 문장에 대해 토큰화, NER 태깅, 품사 태깅 정보를 포함한 예시 데이터를 생성합니다. 이어서 각 토큰별 NER 태그의 의미도 함께 출력해보겠습니다.\n",
    "\n",
    "> 다음 코드 셀에서는 어벤져스 NER 예시 데이터를 생성하고, 각 태그의 의미를 해석하는 방법을 살펴봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f2bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어벤져스 NER 예시 데이터를 직접 생성하는 함수 정의\n",
    "# 실제 NER 태스크에서 사용하는 BIO 태그 체계를 적용\n",
    "\n",
    "def create_avengers_ner_example():\n",
    "    # NER 태그 정의 (CoNLL-2003 형식)\n",
    "    # 0: O (기타, Outside)\n",
    "    # 1: B-PER (인물 시작, Begin-Person)\n",
    "    # 2: I-PER (인물 내부, Inside-Person)\n",
    "    # 3: B-ORG (조직 시작, Begin-Organization)\n",
    "    # 4: I-ORG (조직 내부, Inside-Organization)\n",
    "    # 5: B-LOC (장소 시작, Begin-Location)\n",
    "    # 6: I-LOC (장소 내부, Inside-Location)\n",
    "    # 7: B-MISC (기타 개체 시작, Begin-Miscellaneous)\n",
    "    # 8: I-MISC (기타 개체 내부, Inside-Miscellaneous)\n",
    "\n",
    "    avengers_examples = [\n",
    "        {\n",
    "            'id': '1',\n",
    "            'tokens': ['Tony', 'Stark', 'is', 'Iron', 'Man', 'from', 'Marvel', 'Comics'],\n",
    "            'ner_tags': [1, 2, 0, 7, 8, 0, 3, 4],  # 각 토큰에 대한 NER 태그\n",
    "            'pos_tags': [22, 22, 42, 22, 22, 35, 22, 22]  # 품사 태그 (임의값)\n",
    "        },\n",
    "        {\n",
    "            'id': '2',\n",
    "            'tokens': ['Steve', 'Rogers', 'became', 'Captain', 'America', 'in', 'New', 'York'],\n",
    "            'ner_tags': [1, 2, 0, 7, 8, 0, 5, 6],\n",
    "            'pos_tags': [22, 22, 42, 22, 22, 35, 22, 22]\n",
    "        },\n",
    "        {\n",
    "            'id': '3',\n",
    "            'tokens': ['The', 'Avengers', 'fought', 'in', 'Manhattan', 'against', 'Thanos'],\n",
    "            'ner_tags': [0, 3, 0, 0, 5, 0, 1],\n",
    "            'pos_tags': [12, 22, 42, 35, 22, 35, 22]\n",
    "        }\n",
    "    ]\n",
    "    return avengers_examples\n",
    "\n",
    "# 함수 실행: 어벤져스 NER 예시 데이터 생성\n",
    "examples = create_avengers_ner_example()\n",
    "temp = examples[0]  # 첫 번째 예시 선택\n",
    "\n",
    "print(\"어벤져스 NER 예시:\")\n",
    "print(\"토큰들:\", temp['tokens'])\n",
    "print(\"NER 태그들:\", temp['ner_tags'])\n",
    "\n",
    "# 태그 번호를 사람이 읽을 수 있는 이름으로 변환하여 출력\n",
    "# 태그 번호와 의미 매핑\n",
    "# 실제 NER 모델 학습 시에도 이 매핑이 중요함\n",
    "\n",
    "tag_names = {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'}\n",
    "print(\"\\n태그 해석:\")\n",
    "for token, tag in zip(temp['tokens'], temp['ner_tags']):\n",
    "    print(f\"{token}: {tag_names[tag]}\")\n",
    "\n",
    "# 다음: 텍스트 유사도 데이터셋을 불러와 실제 문장 쌍과 유사도 점수를 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d119e535",
   "metadata": {},
   "source": [
    "## 텍스트 유사도 (Textual Similarity)\n",
    "\n",
    "텍스트 유사도는 두 문장이 의미적으로 얼마나 가까운지를 수치로 표현하는 태스크입니다. 이는 검색, 추천, 질의응답 등 다양한 NLP 응용 분야에서 핵심적으로 사용됩니다.\n",
    "\n",
    "단순히 단어가 얼마나 겹치는지 보는 것뿐 아니라, 문장 전체의 의미적 유사성을 평가하는 것이 중요합니다. 예를 들어, \"A plane is taking off.\"와 \"An air plane is taking off.\"는 거의 같은 의미이므로 높은 유사도 점수를 가져야 합니다.\n",
    "\n",
    "> 다음 코드 셀에서는 대표적인 텍스트 유사도 데이터셋(STS Benchmark)을 불러와 예시 문장 쌍과 유사도 점수를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface Datasets 라이브러리를 사용하여 STS Benchmark 데이터셋 불러오기\n",
    "# STS Benchmark: 문장 쌍과 그 유사도 점수(0~5)를 제공하는 대표적 데이터셋\n",
    "\n",
    "import datasets  # 데이터셋 로드를 위한 라이브러리\n",
    "\n",
    "dataset = datasets.load_dataset('stsb_multi_mt', 'en', split=\"train\", streaming=True)  # 영어 학습 데이터 불러오기 (스트리밍)\n",
    "\n",
    "temp = None  # 첫 번째 예시를 저장할 변수\n",
    "for i, example in enumerate(dataset):\n",
    "    temp = example\n",
    "    break  # 첫 번째 예시만 추출\n",
    "\n",
    "print(temp)  # 예시 출력: 문장1, 문장2, 유사도 점수\n",
    "\n",
    "# 다음: RNN(순환 신경망)의 개념과 필요성에 대해 설명하고, 실제 감성분류 데이터 전처리 과정을 살펴봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73516f98",
   "metadata": {},
   "source": [
    "## RNN (순환 신경망, Recurrent Neural Network)\n",
    "\n",
    "### 왜 RNN이 필요한가?\n",
    "\n",
    "자연어는 시간적 순서(문맥)에 따라 의미가 달라집니다. 기존의 완전연결 신경망(Feedforward Neural Network)은 입력의 순서를 고려하지 못해 문장이나 시퀀스 데이터를 처리하는 데 한계가 있습니다. 예를 들어, \"나는 밥을 먹었다\"와 \"밥을 나는 먹었다\"는 단어는 같지만 순서에 따라 의미가 다릅니다.\n",
    "\n",
    "RNN(순환 신경망)은 입력 데이터가 순차적으로 주어질 때, 이전 입력의 정보를 내부 상태(hidden state)에 저장하여 다음 입력과 함께 처리합니다. 즉, 시퀀스의 문맥 정보를 반영할 수 있도록 설계된 구조입니다. 이 덕분에 자연어 처리, 음성 인식, 시계열 예측 등 다양한 분야에서 필수적으로 사용됩니다.\n",
    "\n",
    "> 다음 코드 셀에서는 실제 감성분류(영화 리뷰) 데이터를 불러와 RNN 학습을 위한 전처리 과정을 단계별로 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 리뷰 감성분류 데이터(nsmc)를 불러와 RNN 입력용으로 전처리하는 전체 과정\n",
    "# 각 단계별로 상세 주석 추가\n",
    "\n",
    "import pandas as pd  # 데이터프레임 처리\n",
    "from sklearn.model_selection import train_test_split  # 학습/테스트 분할\n",
    "from collections import Counter  # 단어 빈도수 계산\n",
    "import re  # 정규표현식(텍스트 전처리)\n",
    "import numpy as np  # 수치 연산\n",
    "\n",
    "# 1. 데이터 로드: 네이버 영화 리뷰 데이터(nsmc)\n",
    "url = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\"\n",
    "df = pd.read_csv(url, delimiter='\\t')  # 탭으로 구분된 파일 읽기\n",
    "\n",
    "# 2. NaN(결측치) 제거\n",
    "# 리뷰 텍스트가 없는 경우가 있으므로, 결측치를 제거해야 오류가 발생하지 않음\n",
    "# 실제 데이터 전처리에서 매우 중요한 단계\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# 3. 학습/테스트 데이터 분할\n",
    "# 전체 데이터를 8:2 비율로 나누어 학습(train)과 평가(test)에 사용\n",
    "x_train_text, x_test_text, y_train, y_test = train_test_split(\n",
    "    df['document'],\n",
    "    df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42  # 재현성을 위한 시드 고정\n",
    ")\n",
    "\n",
    "# 4. 인덱스 리셋\n",
    "# 데이터 분할 후 인덱스가 뒤섞이므로, 인덱스를 다시 0부터 부여\n",
    "x_train_text = x_train_text.reset_index(drop=True)\n",
    "x_test_text = x_test_text.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# 5. 간단한 토큰화 함수 정의\n",
    "# 한글, 영어, 숫자만 남기고 나머지는 공백으로 대체한 뒤, 공백 기준으로 분리\n",
    "# 실제 현업에서는 더 정교한 형태소 분석기를 사용하지만, 여기서는 간단한 규칙 기반 토큰화\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', ' ', text)  # 특수문자 제거\n",
    "    return text.split()  # 공백 기준 분리\n",
    "\n",
    "# 6. 전체 학습 데이터에서 단어 빈도수 계산\n",
    "all_words = []\n",
    "for text in x_train_text:\n",
    "    all_words.extend(simple_tokenize(text))  # 모든 토큰을 리스트에 추가\n",
    "\n",
    "# 7. 상위 19,999개 단어만 사용 (총 20,000개에서 <OOV> 자리 1개 남김)\n",
    "# 데이터가 방대할 때는 자주 등장하는 단어만 사용하여 효율적으로 처리\n",
    "word_counts = Counter(all_words)\n",
    "most_common = word_counts.most_common(19999)  # (단어, 빈도수) 튜플 리스트\n",
    "\n",
    "# 8. 단어 -> 인덱스 매핑 생성\n",
    "# 1번부터 시작, 0번은 패딩(padding)용으로 비워둠\n",
    "# <OOV> 토큰(Out-Of-Vocabulary, 사전에 없는 단어)은 1번 인덱스에 할당\n",
    "word_to_idx = {'<OOV>': 1}\n",
    "for i, (word, _) in enumerate(most_common, 2):\n",
    "    word_to_idx[word] = i\n",
    "\n",
    "print(f\"단어 사전 크기: {len(word_to_idx)}\")\n",
    "\n",
    "# 9. 텍스트를 정수 시퀀스로 변환하는 함수\n",
    "# 각 토큰을 해당 인덱스로 변환, 사전에 없는 단어는 <OOV>(1)로 처리\n",
    "\n",
    "def text_to_sequence(text, word_to_idx):\n",
    "    tokens = simple_tokenize(text)\n",
    "    return [word_to_idx.get(token, 1) for token in tokens]  # 없는 단어는 <OOV>(1)\n",
    "\n",
    "# 10. 학습/테스트 데이터 전체를 정수 시퀀스로 변환\n",
    "x_train = [text_to_sequence(text, word_to_idx) for text in x_train_text]\n",
    "x_test = [text_to_sequence(text, word_to_idx) for text in x_test_text]\n",
    "\n",
    "# 11. 라벨(y)을 numpy 배열로 변환 (모델 학습을 위해 필요)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# 12. 데이터 크기 및 예시 출력\n",
    "print(f\"x_train 개수: {len(x_train)}\")\n",
    "print(f\"x_test 개수: {len(x_test)}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print()\n",
    "print(\"첫 번째 리뷰 (정수 시퀀스):\")\n",
    "print(x_train[0])\n",
    "print()\n",
    "print(f\"첫 3개 리뷰 길이: {len(x_train[0])}, {len(x_train[1])}, {len(x_train[2])}\")\n",
    "\n",
    "# 13. 원본 텍스트와 토큰화 결과 비교\n",
    "print(\"\\n=== 원본 vs 토큰화 비교 ===\")\n",
    "sample_idx = 0\n",
    "original_text = x_train_text[sample_idx]\n",
    "tokenized = x_train[sample_idx]\n",
    "print(f\"원본: {original_text}\")\n",
    "print(f\"토큰화: {tokenized[:20]}...\")  # 처음 20개만 출력\n",
    "\n",
    "# 다음: 시퀀스 데이터를 RNN에 입력하기 위해 패딩(padding) 및 배치 처리를 수행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88242b9a",
   "metadata": {},
   "source": [
    "## 시퀀스 데이터 전처리: 패딩(Padding) 함수 구현과 적용\n",
    "\n",
    "딥러닝 모델, 특히 RNN(순환 신경망)이나 Transformer(트랜스포머) 계열 모델은 입력 시퀀스의 길이가 동일해야 합니다. 하지만 실제 텍스트 데이터는 문장마다 길이가 다르기 때문에, 모델 학습을 위해 모든 시퀀스를 동일한 길이로 맞추는 '패딩(padding)' 과정이 필요합니다. 패딩은 짧은 문장에는 0(혹은 지정한 값)으로 채워 길이를 맞추고, 너무 긴 문장은 잘라내는 방식입니다.\n",
    "\n",
    "이 과정은 텐서플로우(TensorFlow)나 파이토치(PyTorch) 등 프레임워크에 내장된 함수를 쓸 수도 있지만, 여기서는 원리를 이해하기 위해 직접 패딩 함수를 구현해봅니다.\n",
    "\n",
    "이제 직접 패딩 함수를 구현하고, 실제 데이터에 적용해보겠습니다. 이후, 패딩 전후의 데이터가 어떻게 달라지는지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d53ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# =============================\n",
    "# 1. 시퀀스 패딩 함수 직접 구현\n",
    "# =============================\n",
    "def pad_sequences(sequences, maxlen=80, padding='pre', truncating='pre', value=0):\n",
    "    \"\"\"\n",
    "    시퀀스(정수 인덱스 리스트)들을 같은 길이로 맞추는 함수입니다.\n",
    "    - sequences: 정수 인덱스 시퀀스들의 리스트\n",
    "    - maxlen: 최종 시퀀스 길이(모든 문장을 이 길이로 맞춤)\n",
    "    - padding: 'pre'면 앞쪽에, 'post'면 뒤쪽에 패딩을 추가\n",
    "    - truncating: 'pre'면 앞쪽을 잘라내고, 'post'면 뒤쪽을 잘라냄\n",
    "    - value: 패딩에 사용할 값(일반적으로 0)\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for seq in sequences:\n",
    "        # [1] 시퀀스가 너무 길면 잘라냄\n",
    "        if len(seq) > maxlen:\n",
    "            if truncating == 'pre':\n",
    "                seq = seq[-maxlen:]  # 뒤쪽 maxlen개만 남김\n",
    "            else:  # 'post'\n",
    "                seq = seq[:maxlen]   # 앞쪽 maxlen개만 남김\n",
    "        # [2] 시퀀스가 짧으면 패딩 추가\n",
    "        if len(seq) < maxlen:\n",
    "            pad_length = maxlen - len(seq)\n",
    "            if padding == 'pre':\n",
    "                seq = [value] * pad_length + seq  # 앞에 패딩 추가\n",
    "            else:  # 'post'\n",
    "                seq = seq + [value] * pad_length  # 뒤에 패딩 추가\n",
    "        result.append(seq)\n",
    "    return np.array(result)\n",
    "\n",
    "# =============================\n",
    "# 2. 실제 데이터에 패딩 적용\n",
    "# =============================\n",
    "pad_x_train = pad_sequences(x_train, maxlen=80)\n",
    "pad_x_test = pad_sequences(x_test, maxlen=80)\n",
    "\n",
    "# =============================\n",
    "# 3. 패딩 전후 결과 확인\n",
    "# =============================\n",
    "print(f\"패딩 후 x_train shape: {pad_x_train.shape}\")\n",
    "print(f\"패딩 후 x_test shape: {pad_x_test.shape}\")\n",
    "print(f\"첫 3개 리뷰 길이: {len(pad_x_train[0])}, {len(pad_x_train[1])}, {len(pad_x_train[2])}\")\n",
    "\n",
    "print(\"\\n=== 패딩 전후 비교 ===\")\n",
    "print(f\"패딩 전 첫 번째 리뷰 길이: {len(x_train[0])}\")\n",
    "print(f\"패딩 후 첫 번째 리뷰 길이: {len(pad_x_train[0])}\")\n",
    "print(f\"패딩 후 첫 번째 리뷰: {pad_x_train[0]}\")\n",
    "\n",
    "# 다음: 패딩된 데이터를 입력으로 사용하는 RNN(순환 신경망) 모델을 PyTorch로 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ebc38e",
   "metadata": {},
   "source": [
    "## RNN 기반 감성 분류 모델 구현 (PyTorch)\n",
    "\n",
    "패딩을 통해 모든 입력 시퀀스의 길이를 맞췄으니, 이제 이 데이터를 활용해 감성 분류를 수행하는 순환 신경망(RNN, Recurrent Neural Network) 모델을 구현해보겠습니다.\n",
    "\n",
    "RNN은 시퀀스 데이터를 처리하는 대표적인 신경망 구조로, 입력 단어의 순서를 고려하여 문맥 정보를 학습할 수 있습니다. 여기서는 임베딩(Embedding) 레이어로 단어를 벡터로 변환한 뒤, RNN 레이어를 거쳐 마지막 출력만을 사용해 긍정/부정 감성을 분류합니다.\n",
    "\n",
    "또한, 모델 구조와 각 레이어의 파라미터 수를 직관적으로 확인할 수 있는 요약 함수도 함께 구현합니다.\n",
    "\n",
    "이제 PyTorch로 RNN 모델을 정의하고, 모델 구조를 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0aa6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# =============================\n",
    "# 1. RNN 기반 감성 분류 모델 정의\n",
    "# =============================\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size=20000, embedding_dim=128, hidden_dim=128):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        # 단어 인덱스를 임베딩 벡터로 변환 (단어 의미를 벡터로 표현)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # RNN 레이어: 시퀀스 정보를 순차적으로 처리\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # 마지막 타임스텝의 출력을 받아 이진 분류 수행\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        # 시그모이드(Sigmoid): 0~1 사이 확률값으로 변환\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        rnn_out, hidden = self.rnn(embedded)  # rnn_out: (batch_size, seq_len, hidden_dim)\n",
    "        # 마지막 시점의 RNN 출력만 사용 (문장 전체 정보)\n",
    "        last_output = rnn_out[:, -1, :]  # (batch_size, hidden_dim)\n",
    "        output = self.fc(last_output)  # (batch_size, 1)\n",
    "        output = self.sigmoid(output)  # (batch_size, 1)\n",
    "        return output\n",
    "\n",
    "# =============================\n",
    "# 2. 모델 인스턴스 생성\n",
    "# =============================\n",
    "model = SentimentRNN(vocab_size=20000, embedding_dim=128, hidden_dim=128)\n",
    "\n",
    "# =============================\n",
    "# 3. 모델 구조 요약 함수 구현\n",
    "# =============================\n",
    "def model_summary(model, input_size):\n",
    "    \"\"\"\n",
    "    모델의 각 레이어별 입출력 형태와 파라미터 개수를 출력합니다.\n",
    "    - input_size: (batch_size, seq_len) 형태의 더미 입력 크기\n",
    "    \"\"\"\n",
    "    def register_hook(module):\n",
    "        def hook(module, input, output):\n",
    "            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "            module_idx = len(summary)\n",
    "            m_key = f\"{class_name}-{module_idx+1}\"\n",
    "            summary[m_key] = {}\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size()) if isinstance(input, tuple) else [input.size()]\n",
    "            summary[m_key][\"output_shape\"] = list(output.size()) if not isinstance(output, tuple) else [list(o.size()) for o in output]\n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.tensor(module.weight.size()))\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.tensor(module.bias.size()))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "        # nn.Sequential, nn.ModuleList, 전체 모델은 제외\n",
    "        if not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList) and not (module == model):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "    summary = {}\n",
    "    hooks = []\n",
    "    model.apply(register_hook)\n",
    "    # 더미 입력으로 forward pass (실제 데이터와 동일한 형태)\n",
    "    x = torch.randint(0, 20000, input_size)\n",
    "    model(x)\n",
    "    # hook 제거\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    print(\"================================================================\")\n",
    "    print(\"Layer (type)               Output Shape         Param #\")\n",
    "    print(\"================================================================\")\n",
    "    total_params = 0\n",
    "    for layer in summary:\n",
    "        output_shape = summary[layer][\"output_shape\"]\n",
    "        if isinstance(output_shape[0], list):\n",
    "            output_shape = output_shape[0]  # RNN의 경우 첫 번째 출력만 사용\n",
    "        params = summary[layer][\"nb_params\"]\n",
    "        print(f\"{layer:<25} {str(output_shape):<20} {params:>10,}\")\n",
    "        total_params += params\n",
    "    print(\"================================================================\")\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(\"================================================================\")\n",
    "\n",
    "# =============================\n",
    "# 4. 모델 구조 출력\n",
    "# =============================\n",
    "model_summary(model, (1, 80))  # batch_size=1, seq_len=80\n",
    "\n",
    "# 다음: 실제 데이터를 PyTorch 텐서로 변환하고, 모델 학습을 위한 데이터셋과 DataLoader를 구성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb673b9",
   "metadata": {},
   "source": [
    "## PyTorch용 데이터 준비 및 모델 학습\n",
    "\n",
    "이제 패딩된 데이터를 PyTorch 텐서로 변환하고, 효율적인 학습을 위해 DataLoader(데이터 배치 생성기)를 구성합니다. 이 과정은 텐서플로우의 `fit`과 매우 유사하지만, PyTorch에서는 직접 학습 루프를 구현해야 하므로, 각 단계의 원리를 이해하는 것이 중요합니다.\n",
    "\n",
    "또한, 실제 단어 사전의 크기에 맞춰 모델을 재생성하고, 옵티마이저(Optimizer)와 손실 함수(Loss Function)를 설정합니다. 마지막으로, 학습 및 검증 정확도를 직접 계산하는 함수와 전체 학습 루프를 구현합니다.\n",
    "\n",
    "이제 데이터를 텐서로 변환하고, DataLoader 및 학습 루프를 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec63fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =============================\n",
    "# 1. 실제 단어 사전 크기로 모델 재생성\n",
    "# =============================\n",
    "actual_vocab_size = len(word_to_idx)\n",
    "model = SentimentRNN(vocab_size=actual_vocab_size + 1, embedding_dim=128, hidden_dim=128)\n",
    "\n",
    "# =============================\n",
    "# 2. 데이터를 PyTorch 텐서로 변환\n",
    "# =============================\n",
    "pad_x_train_tensor = torch.tensor(pad_x_train, dtype=torch.long)\n",
    "pad_x_test_tensor = torch.tensor(pad_x_test, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # (N, 1) 형태로 변환\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# =============================\n",
    "# 3. Dataset과 DataLoader 생성\n",
    "# =============================\n",
    "train_dataset = TensorDataset(pad_x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(pad_x_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# =============================\n",
    "# 4. 옵티마이저와 손실 함수 설정\n",
    "# =============================\n",
    "optimizer = optim.Adam(model.parameters())  # Adam 옵티마이저 사용\n",
    "criterion = nn.BCELoss()  # 이진 분류용 Binary Cross Entropy Loss\n",
    "\n",
    "# =============================\n",
    "# 5. 정확도 계산 함수 구현\n",
    "# =============================\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    # 0.5를 기준으로 이진 분류 결과 계산\n",
    "    predicted = (outputs > 0.5).float()\n",
    "    correct = (predicted == labels).float()\n",
    "    return correct.mean()\n",
    "\n",
    "# =============================\n",
    "# 6. 훈련 함수 구현\n",
    "# =============================\n",
    "def train_model(model, train_loader, test_loader, epochs=15):\n",
    "    for epoch in range(epochs):\n",
    "        # [1] 훈련 단계\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        num_batches = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()  # 기울기 초기화\n",
    "            outputs = model(batch_x)  # 예측값 계산\n",
    "            loss = criterion(outputs, batch_y)  # 손실 계산\n",
    "            loss.backward()  # 역전파\n",
    "            optimizer.step()  # 파라미터 업데이트\n",
    "            total_loss += loss.item()\n",
    "            total_acc += calculate_accuracy(outputs, batch_y)\n",
    "            num_batches += 1\n",
    "        # [2] 검증 단계\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                val_acc += calculate_accuracy(outputs, batch_y)\n",
    "                val_batches += 1\n",
    "        # [3] 에폭별 결과 출력\n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'  loss: {total_loss/num_batches:.4f} - accuracy: {total_acc/num_batches:.4f} - val_loss: {val_loss/val_batches:.4f} - val_accuracy: {val_acc/val_batches:.4f}')\n",
    "\n",
    "# =============================\n",
    "# 7. 모델 훈련 실행\n",
    "# =============================\n",
    "train_model(model, train_loader, test_loader, epochs=15)\n",
    "\n",
    "# 다음: IMDB 스타일의 단어 사전을 직접 구성하고, 특수 토큰을 추가하는 과정을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a7f820",
   "metadata": {},
   "source": [
    "## IMDB 스타일 단어 사전 구성 및 특수 토큰 추가\n",
    "\n",
    "실제 자연어 처리(NLP)에서는 단어 사전을 어떻게 구성하느냐가 매우 중요합니다. IMDB 데이터셋처럼, 특수 토큰(예: <PAD>, <BOS>, <UNK>, <UNUSED>)을 명확히 구분해두면 추후 모델 해석이나 디코딩, OOV(Out-Of-Vocabulary) 처리 등에 큰 도움이 됩니다.\n",
    "\n",
    "여기서는 기존 NSMC(네이버 영화리뷰) 단어 사전을 IMDB 스타일로 변환합니다. 즉, 기존 인덱스를 +3만큼 이동시키고, 0~3번 인덱스에 특수 토큰을 추가합니다. 또한, 인덱스→단어로 변환하는 역방향 사전도 함께 만듭니다.\n",
    "\n",
    "이제 IMDB 스타일 단어 사전과 역방향 사전을 직접 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f1c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 1. 기존 단어 사전에서 특수 토큰 제외 및 인덱스 조정\n",
    "# =============================\n",
    "origin_word_dict = {}\n",
    "for word, idx in word_to_idx.items():\n",
    "    if word != '<OOV>':  # <OOV> 토큰은 제외\n",
    "        origin_word_dict[word] = idx - 1  # 인덱스를 0부터 시작하도록 조정\n",
    "\n",
    "# =============================\n",
    "# 2. IMDB 스타일로 인덱스 +3 이동\n",
    "# =============================\n",
    "word_dict = {k: (v + 3) for k, v in origin_word_dict.items()}\n",
    "\n",
    "# =============================\n",
    "# 3. 특수 토큰 추가\n",
    "# =============================\n",
    "word_dict['<PAD>'] = 0      # 패딩 토큰\n",
    "word_dict['<BOS>'] = 1      # 문장 시작(Beginning of Sequence)\n",
    "word_dict['<UNK>'] = 2      # 모르는 단어(Unknown)\n",
    "word_dict['<UNUSED>'] = 3   # 사용하지 않는 토큰\n",
    "\n",
    "print(\"=== 특수 토큰 확인 ===\")\n",
    "print(f\"<PAD>: {word_dict['<PAD>']}\")\n",
    "print(f\"<BOS>: {word_dict['<BOS>']}\")\n",
    "print(f\"<UNK>: {word_dict['<UNK>']}\")\n",
    "print(f\"<UNUSED>: {word_dict['<UNUSED>']}\")\n",
    "\n",
    "print(f\"\\n총 단어 사전 크기: {len(word_dict)}\")\n",
    "print(f\"단어 사전 최대 인덱스: {max(word_dict.values())}\")\n",
    "\n",
    "# =============================\n",
    "# 4. 일반 단어 예시 출력\n",
    "# =============================\n",
    "print(\"\\n=== 일반 단어 예시 ===\")\n",
    "sample_words = list(word_dict.keys())[4:10]  # 특수 토큰 제외 6개 단어\n",
    "for word in sample_words:\n",
    "    print(f\"'{word}': {word_dict[word]}\")\n",
    "\n",
    "# =============================\n",
    "# 5. 역방향 사전(인덱스→단어) 생성\n",
    "# =============================\n",
    "idx_to_word = {idx: word for word, idx in word_dict.items()}\n",
    "print(f\"\\n역방향 사전 생성 완료: {len(idx_to_word)}개 항목\")\n",
    "\n",
    "# 다음: 학습한 모델을 활용하여 임의의 문장에 대해 감성 예측을 수행하는 함수를 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a555ed83",
   "metadata": {},
   "source": [
    "## 사용자 입력 문장 감성 예측 함수 구현\n",
    "\n",
    "모델을 학습한 후에는 실제로 임의의 문장(예: 사용자가 직접 입력한 문장)에 대해 감성(긍정/부정)을 예측할 수 있어야 합니다. 이를 위해서는 다음과 같은 단계가 필요합니다:\n",
    "\n",
    "1. 입력 문장을 단어 단위로 분리\n",
    "2. 각 단어를 단어 사전의 인덱스로 변환(모르는 단어는 <UNK>로 처리)\n",
    "3. <BOS> 토큰을 문장 앞에 추가\n",
    "4. 시퀀스 길이를 맞추기 위해 패딩 적용\n",
    "5. 모델에 입력하여 예측값(확률) 출력\n",
    "6. 임계값(0.5) 기준으로 긍정/부정 판단\n",
    "\n",
    "이제 위 과정을 모두 포함한 예측 함수를 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7222acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def predict(words):\n",
    "    # 1. 입력 문장을 띄어쓰기 기준으로 단어 분리\n",
    "    user_sentence = words.split(' ')\n",
    "    # 2. 각 단어를 인덱스로 변환 (모르는 단어는 <UNK>로 대체)\n",
    "    encoded_sentence = [word_dict['<BOS>']]  # 문장 시작 토큰 추가\n",
    "    for word in user_sentence:\n",
    "        try:\n",
    "            # 사전에 있고, 인덱스가 2만 미만이면 해당 인덱스 사용\n",
    "            if word_dict[word] < 20000:\n",
    "                encoded_sentence.append(word_dict[word])\n",
    "            else:\n",
    "                encoded_sentence.append(word_dict['<UNK>'])\n",
    "        except KeyError:\n",
    "            # 사전에 없는 단어는 <UNK>로 처리\n",
    "            encoded_sentence.append(word_dict['<UNK>'])\n",
    "    # 3. 패딩 적용 (길이 80으로 맞춤)\n",
    "    def pad_sequences(sequences, maxlen=80, padding='pre', value=0):\n",
    "        result = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) > maxlen:\n",
    "                seq = seq[-maxlen:]  # 뒤쪽 maxlen개만 남김\n",
    "            else:\n",
    "                pad_length = maxlen - len(seq)\n",
    "                seq = [value] * pad_length + seq  # 앞에 패딩 추가\n",
    "            result.append(seq)\n",
    "        return np.array(result)\n",
    "    pad_encoded_sentence = pad_sequences([encoded_sentence], maxlen=80)\n",
    "    # 4. 모델에 입력 (PyTorch 텐서로 변환)\n",
    "    pad_encoded_tensor = torch.tensor(pad_encoded_sentence, dtype=torch.long)\n",
    "    model.eval()  # 평가 모드로 전환\n",
    "    with torch.no_grad():\n",
    "        result = model(pad_encoded_tensor)\n",
    "    print(result)\n",
    "    # 5. 임계값(0.5) 기준으로 긍정/부정 판단\n",
    "    if result[0][0] > 0.5:\n",
    "        print(f'{result[0][0]*100}%로 긍정')\n",
    "    else:\n",
    "        print('부정')\n",
    "\n",
    "# 다음: 실제로 predict 함수를 사용하여 임의의 문장에 대한 감성 예측을 실습해볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69227e6d",
   "metadata": {},
   "source": [
    "## 모델 예측 결과 해석 및 실전 테스트\n",
    "\n",
    "앞서 학습한 감성 분류(sentiment classification) 모델의 실제 예측 결과를 확인해보겠습니다. 이 과정은 모델이 실제로 입력 문장에 대해 얼마나 정확하게 긍정/부정을 판단하는지 검증하는 중요한 단계입니다.\n",
    "\n",
    "### 왜 예측 결과를 직접 확인해야 할까요?\n",
    "- **모델의 실전 적용 가능성**: 단순히 평가 지표(정확도, F1 등)만으로는 실제 사용 환경에서의 성능을 완전히 알 수 없습니다. 다양한 예시 문장에 대해 모델이 어떻게 반응하는지 직접 확인해야 신뢰성을 높일 수 있습니다.\n",
    "- **모델의 한계 파악**: 예측 결과를 보면 모델이 어떤 문장에 약한지, 오분류(잘못 분류)하는 패턴이 있는지 파악할 수 있습니다. 이는 추가 데이터 수집이나 모델 개선의 실마리가 됩니다.\n",
    "\n",
    "이제 실제로 긍정적인 문장과 부정적인 문장 각각에 대해 모델의 예측 결과를 살펴보겠습니다. 이후에는 인공지능 프로젝트 전체 프로세스에 대해 심층적으로 정리하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efbe67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '배우들 연기가 정말 훌륭했습니다'라는 긍정적인 문장에 대해 예측\n",
    "# predict 함수는 입력된 문장을 토큰화(tokenize)하고, 모델에 입력하여 예측 확률을 반환합니다.\n",
    "# 반환값은 긍정일 확률(0~1 사이 값)입니다.\n",
    "predict(\"배우들 연기가 정말 훌륭했습니다\")  # 긍정 문장 예측\n",
    "\n",
    "# 출력 결과 예시:\n",
    "# tensor([[0.5274]])\n",
    "# 52.74%로 긍정\n",
    "\n",
    "# 다음: 부정적인 문장에 대해 예측 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391cb57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '이 영화 정말 재미없어요'라는 부정적인 문장에 대해 예측\n",
    "# 이 문장은 부정 감정을 담고 있으므로, 모델이 낮은 긍정 확률을 반환하는지 확인합니다.\n",
    "predict(\"이 영화 정말 재미없어요\")  # 부정 문장 예측\n",
    "\n",
    "# 출력 결과 예시:\n",
    "# tensor([[0.4615]])\n",
    "# 46.15%로 부정\n",
    "\n",
    "# 다음: 인공지능 학습 프로젝트의 전체 프로세스를 단계별로 정리합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf0717",
   "metadata": {},
   "source": [
    "## 인공지능(AI) 학습 프로젝트 전체 프로세스 심층 정리\n",
    "\n",
    "인공지능 프로젝트는 단순히 모델을 학습시키는 것에 그치지 않고, 데이터 수집부터 모델 배포까지 여러 단계가 유기적으로 연결되어 있습니다. 각 단계의 목적과 중요성을 깊이 있게 이해하는 것이 실제 프로젝트 성공의 핵심입니다.\n",
    "\n",
    "### 1. 프로젝트 목표 설정\n",
    "- **왜 필요한가?**\n",
    "  - 명확한 목표가 없으면, 어떤 데이터를 수집해야 할지, 어떤 모델을 써야 할지 결정할 수 없습니다.\n",
    "  - 데이터 수집 및 라벨링은 시간과 비용이 많이 드는 작업이므로, 초기에 목표를 명확히 해야 불필요한 낭비를 막을 수 있습니다.\n",
    "- **실무 예시**: 감성 분석 모델을 만들고자 한다면, 영화 리뷰에서 긍정/부정 감정을 분류하는 것이 목표가 됩니다.\n",
    "\n",
    "### 2. 데이터 수집\n",
    "- **왜 중요한가?**\n",
    "  - 데이터의 양과 질이 모델 성능에 직접적인 영향을 미칩니다.\n",
    "  - 데이터가 부족하거나 편향(bias)이 있다면, 모델은 실제 환경에서 오작동할 수 있습니다.\n",
    "- **실무 예시**: 다양한 장르와 연령대의 영화 리뷰를 수집해야, 특정 장르나 연령대에 치우치지 않은 모델을 만들 수 있습니다.\n",
    "\n",
    "### 3. 데이터 분석\n",
    "- **왜 필요한가?**\n",
    "  - 수집한 데이터가 실제로 목표에 적합한지, 결측치(missing value)나 이상치(outlier)가 있는지 확인해야 합니다.\n",
    "  - 데이터의 분포와 특성을 파악하면, 이후 전처리 및 모델 설계에 큰 도움이 됩니다.\n",
    "- **실무 예시**: 긍정/부정 비율이 9:1이라면, 데이터 불균형 문제가 발생할 수 있습니다.\n",
    "\n",
    "### 4. 데이터 라벨링(Labeling)\n",
    "- **왜 중요한가?**\n",
    "  - 지도학습(supervised learning)에서는 정확한 정답(ground truth)이 반드시 필요합니다.\n",
    "  - 라벨링이 부정확하면, 모델이 잘못된 패턴을 학습하게 됩니다.\n",
    "- **실무 예시**: 텍스트 분류에서는 사람이 직접 문장을 읽고 긍정/부정으로 태깅해야 합니다.\n",
    "\n",
    "### 5. 데이터 전처리(Preprocessing)\n",
    "- **왜 필요한가?**\n",
    "  - 원본 데이터에는 노이즈(noise)나 불필요한 정보가 많을 수 있습니다.\n",
    "  - 전처리를 통해 모델이 학습하기 쉬운 형태로 데이터를 변환합니다.\n",
    "- **주요 작업**: 불용어(stopword) 제거, 토큰화(tokenization), 정규화(normalization), 학습/평가 데이터 분리 등\n",
    "\n",
    "### 6. 모델 빌드(Build)\n",
    "- **왜 중요한가?**\n",
    "  - 문제의 특성과 데이터에 맞는 모델 구조(아키텍처, architecture)를 선택해야 합니다.\n",
    "  - 잘못된 모델 선택은 성능 저하로 이어질 수 있습니다.\n",
    "- **실무 예시**: 간단한 문제에는 선형 회귀(linear regression), 복잡한 문제에는 딥러닝(deep learning) 모델을 사용합니다.\n",
    "\n",
    "### 7. 모델 학습(Training)\n",
    "- **왜 필요한가?**\n",
    "  - 모델이 데이터에서 패턴을 학습하도록 가중치(weight)를 최적화합니다.\n",
    "  - 손실 함수(loss function)와 최적화 알고리즘(optimizer)을 통해 성능을 최대화합니다.\n",
    "- **실무 예시**: 학습 과정에서 과적합(overfitting)을 방지하기 위해 교차 검증(cross-validation)이나 조기 종료(early stopping)를 사용합니다.\n",
    "\n",
    "### 8. 모델 평가(Evaluation)\n",
    "- **왜 중요한가?**\n",
    "  - 모델이 실제로 얼마나 잘 동작하는지, 일반화 능력이 있는지 확인합니다.\n",
    "  - 평가 지표(metric)는 문제 유형에 따라 다르며, 텍스트 생성처럼 자동 평가가 어려운 경우 사람이 직접 평가해야 할 수도 있습니다.\n",
    "- **실무 예시**: 감성 분류에서는 정확도(accuracy), 정밀도(precision), 재현율(recall), F1 점수(f1-score) 등을 사용합니다.\n",
    "\n",
    "### 9. 모델 배포(Deployment)\n",
    "- **왜 필요한가?**\n",
    "  - 학습된 모델을 실제 서비스에 적용해야 비로소 비즈니스 가치가 창출됩니다.\n",
    "  - 배포 후에도 지속적으로 성능을 모니터링하고, 필요시 모델을 재학습해야 합니다.\n",
    "- **실무 예시**: REST API 형태로 모델을 배포하여, 웹/앱에서 실시간으로 예측 결과를 사용할 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "이렇게 각 단계가 유기적으로 연결되어야 인공지능 프로젝트가 성공적으로 완성됩니다. \n",
    "\n",
    "**다음 단계 안내:**\n",
    "이제 실제로 프로젝트의 각 단계별로 필요한 코드를 어떻게 작성하는지, 실습 예시를 통해 구체적으로 살펴보겠습니다."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
