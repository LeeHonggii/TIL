{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f7f2e17",
   "metadata": {},
   "source": [
    "## NLP 동작\n",
    "- 데이터 수집  \n",
    "  다양한 소스(웹, 문서, 데이터베이스 등)에서 텍스트 데이터를 모으는 단계입니다.\n",
    "- 데이터 정제  \n",
    "  수집된 데이터에서 불필요한 문자, 특수기호, 중복 등을 제거하고, 정제된 형태로 만듭니다.\n",
    "- 표현 변환  \n",
    "  텍스트 데이터를 컴퓨터가 이해할 수 있도록 토큰화, 임베딩 등으로 변환합니다.\n",
    "- 모델 입력  \n",
    "  변환된 데이터를 딥러닝 또는 머신러닝 모델에 입력할 수 있는 형태로 준비합니다.\n",
    "- 텐서 계산  \n",
    "  모델이 입력 데이터를 처리하여 내부적으로 연산(텐서 계산)을 수행합니다.\n",
    "- 출력후 디코딩  \n",
    "  모델의 예측 결과를 사람이 이해할 수 있도록 다시 텍스트 등으로 변환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe84ef",
   "metadata": {},
   "source": [
    "## 데이터 수집\n",
    "**raw 데이터 수집**\n",
    "\n",
    "**PDF 텍스트:**\n",
    "```\n",
    "Attention(Q, K, V ) = softmax(QKT √ dk )V\n",
    "```\n",
    "\n",
    "**LaTeX 텍스트:**\n",
    "```latex\n",
    "\\begin{equation}\n",
    " \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "\\end{equation}\n",
    "```\n",
    "\n",
    "### 설명\n",
    "- **Q (Query)**: 주목하려는 정보\n",
    "- **K (Key)**: 각 위치의 특성  \n",
    "- **V (Value)**: 실제 정보 내용\n",
    "- **QK^T**: Query와 Key 유사도 계산\n",
    "- **√d_k**: 스케일링 (차원 크기로 정규화)\n",
    "- **softmax**: 확률 분포 변환\n",
    "- **최종 결과**: 중요한 정보에 집중한 가중평균\n",
    "\n",
    "Transformer의 핵심 어텐션 메커니즘으로 문맥 이해와 장거리 의존성 포착에 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6f6dda",
   "metadata": {},
   "source": [
    "## 데이터 정제\n",
    "**목표에 맞춰서 필요 내용만 남기는 것**\n",
    "\n",
    "### 정제 과정\n",
    "- 불필요한 요소 제거 (중복 문장, 조사)\n",
    "- 정규화 (대소문자 통일)\n",
    "- 토큰화 (문장, 문자 단위)\n",
    "\n",
    "### 변경 예시\n",
    "\n",
    "**변경 전:**\n",
    "```latex\n",
    "\\begin{equation}\n",
    " \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "\\end{equation}\n",
    "```\n",
    "\n",
    "**변경 후:**\n",
    "```\n",
    "Attention, (, Q, K, V, ), =, softmax, \\frac{, Q, K, ^, T, }, {, \\sqrt, {, d, _k, }, }, V\n",
    "```\n",
    "\n",
    "### 정제 결과\n",
    "- LaTeX 환경 태그 제거 (`\\begin{equation}`, `\\end{equation}`)\n",
    "- 수식을 개별 토큰으로 분리\n",
    "- 각 기호, 함수, 변수를 독립적 요소로 처리\n",
    "- 구조화된 텍스트를 모델이 학습 가능한 형태로 변환\n",
    "\n",
    "이를 통해 원본 수식의 의미는 보존하면서 기계학습 모델이 처리하기 적합한 형태로 변환됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b57bdbb",
   "metadata": {},
   "source": [
    "#### 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello? Welcome to Text Preprocessing Session. Today, we gonna learn about NLP.\"\n",
    "sentences = re.split(r'(?<=[?.])\\s*', text)\n",
    "\n",
    "print(\"문장 분리 결과:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7543fa",
   "metadata": {},
   "source": [
    "## 문장 분리의 한계와 전문 도구의 필요성\n",
    "\n",
    "문장 분리(sentence segmentation)는 자연어 처리(NLP)에서 매우 중요한 전처리 단계입니다. 하지만 단순히 마침표(.), 물음표(?) 등으로 문장을 나누면, 약어(Dr., Mr.), 이메일 주소(test.mail@example.com) 등에서 잘못 분리되는 문제가 발생합니다.\n",
    "\n",
    "아래 예시에서는 정규표현식(regular expression, 정규식)을 사용해 문장 분리를 시도했지만, 이메일 주소와 약어 때문에 의도하지 않은 결과가 나왔습니다.\n",
    "\n",
    "이제, 이 문제를 코드로 직접 확인해보겠습니다. 이후, 전문적인 문장 분리 도구의 필요성을 설명하고, 실제로 적용해볼 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b518de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식을 사용한 문장 분리 예시\n",
    "import re  # 정규표현식(regular expression) 모듈 불러오기\n",
    "\n",
    "# 예시 문장: 약어와 이메일 주소가 포함되어 있음\n",
    "text = \"Dr. Smith is on the vacation. Please contact him at test.mail@example.com for further details.\"\n",
    "\n",
    "# 마침표(.) 또는 물음표(?) 뒤에 공백이 있는 부분에서 문장 분리 시도\n",
    "sentences = re.split(r'(?<=[?.])\\s*', text)\n",
    "\n",
    "print(\"문장 분리 결과:\", sentences)  # 결과 출력\n",
    "\n",
    "# 다음: 전문적인 문장 분리 도구인 NLTK의 sent_tokenize를 사용해 문제를 해결해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12eb95",
   "metadata": {},
   "source": [
    "위 코드의 실행 결과를 보면, 이메일 주소와 약어에서 잘못 분리되는 문제가 있습니다. 이는 단순 정규표현식만으로는 실제 문장 구조를 모두 반영하기 어렵기 때문입니다.\n",
    "\n",
    "이런 한계를 극복하기 위해, 자연어 처리 분야에서는 문장 경계 인식(sbd, sentence boundary detection)을 위한 전문 라이브러리를 사용합니다. 대표적으로 NLTK(Natural Language Toolkit)의 `sent_tokenize` 함수가 있습니다.\n",
    "\n",
    "이제 NLTK를 설치하고, 실제로 문장 분리를 어떻게 개선할 수 있는지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e1c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK(Natural Language Toolkit) 설치\n",
    "# NLTK는 파이썬에서 자연어 처리를 위한 대표적 라이브러리입니다.\n",
    "!pip install nltk  # 터미널 명령어로 nltk 설치\n",
    "\n",
    "# 다음: NLTK의 sent_tokenize를 사용하여 문장 분리를 정확하게 수행해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b7055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK를 이용한 문장 분리\n",
    "from nltk.tokenize import sent_tokenize  # 문장 분리 함수 불러오기\n",
    "import nltk  # NLTK 전체 모듈 불러오기\n",
    "\n",
    "# punkt: 영어 등 여러 언어의 문장 분리 규칙이 포함된 데이터\n",
    "nltk.download('punkt')  # punkt 모델 다운로드 (최초 1회)\n",
    "nltk.download('punkt_tab')  # punkt_tab은 punkt의 부가 정보 (실제론 필요 없음, 오류 방지용)\n",
    "\n",
    "# 앞서 사용한 동일한 예시 문장\n",
    "text = \"Dr. Smith is on the vacation. Please contact him at test.mail@example.com for further details.\"\n",
    "\n",
    "# sent_tokenize 함수는 약어, 이메일 등 다양한 예외를 고려하여 문장 분리\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"문장 분리 결과:\", sentences)  # 올바른 문장 분리 결과 출력\n",
    "\n",
    "# 다음: NLTK 라이브러리의 다양한 기능을 탐색해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf3772",
   "metadata": {},
   "source": [
    "NLTK의 `sent_tokenize`는 약어(Dr.), 이메일 주소(test.mail@example.com) 등에서 잘못 분리되는 문제를 효과적으로 해결합니다. 이는 NLTK가 다양한 언어적 예외 규칙을 내장하고 있기 때문입니다.\n",
    "\n",
    "이처럼, 실제 자연어 처리에서는 단순 규칙이 아닌, 언어적 맥락과 예외를 반영한 전문 도구가 필수적입니다.\n",
    "\n",
    "이제, NLTK가 제공하는 다양한 기능이 무엇이 있는지 간단히 살펴보겠습니다. 이후에는 단어 토큰화(word tokenization)로 넘어갈 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6e0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK 라이브러리의 주요 기능 살펴보기\n",
    "import nltk  # 이미 불러온 경우에도 재사용 가능\n",
    "\n",
    "# dir() 함수로 nltk 모듈이 제공하는 다양한 클래스와 함수 목록 확인\n",
    "print(dir(nltk))  # NLTK의 다양한 기능(토큰화, 형태소 분석, 말뭉치 등) 확인\n",
    "\n",
    "# 다음: 단어 토큰화(word tokenization) 개념과 필요성에 대해 알아보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57b1346",
   "metadata": {},
   "source": [
    "#### 단어 토큰화(Word Tokenization)\n",
    "\n",
    "문장 분리 다음 단계는 단어 토큰화입니다. 단어 토큰화는 문장을 의미 있는 최소 단위(주로 단어)로 나누는 과정입니다. 이 과정은 자연어 처리의 거의 모든 응용(텍스트 분류, 감정 분석, 기계 번역 등)에 필수적입니다.\n",
    "\n",
    "단어 토큰화가 필요한 이유는 다음과 같습니다:\n",
    "- 컴퓨터는 문장을 그대로 처리하지 못하므로, 의미 단위(단어)로 쪼개야만 각 단어의 빈도, 위치, 의미 등을 분석할 수 있습니다.\n",
    "- 영어의 경우, 공백으로 단어를 구분할 수 있지만, 약어, 구두점, 숫자 등 다양한 예외 상황이 존재합니다. 예를 들어, \"Mr. Smith's e-mail is test@mail.com\"과 같은 문장은 단순 공백 분리로는 정확히 처리할 수 없습니다.\n",
    "\n",
    "이제, NLTK를 활용하여 단어 토큰화를 실제로 어떻게 수행하는지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb019d",
   "metadata": {},
   "source": [
    "## 자연어 처리에서의 토큰화(Tokenization) 이해하기\n",
    "\n",
    "자연어 처리(Natural Language Processing, NLP)에서 텍스트 데이터를 컴퓨터가 이해할 수 있도록 변환하는 첫 단계가 바로 토큰화(tokenization)입니다. 토큰화란 긴 문장이나 문서를 의미 있는 단위(토큰, token)로 나누는 과정입니다. 이때 토큰의 단위는 상황에 따라 단어, 형태소, 글자 등 다양할 수 있습니다.\n",
    "\n",
    "왜 토큰화가 필요할까요? 컴퓨터는 텍스트를 그대로 처리할 수 없기 때문에, 텍스트를 일정한 규칙에 따라 잘게 쪼개어야만 각 단어 혹은 의미 단위별로 분석, 통계, 학습 등이 가능합니다. 예를 들어, 단어 빈도수 계산, 품사 태깅, 감정 분석, 기계 번역 등 거의 모든 NLP 작업의 출발점이 토큰화입니다.\n",
    "\n",
    "이번에는 영어와 한글 텍스트에 대해 다양한 토큰화 방법을 실습해보겠습니다. 먼저, 영어 문장에 대해 단어 단위로 토큰화하는 방법을 살펴보겠습니다. 이후 한글 텍스트에 대해 글자 단위, 형태소 단위 등 다양한 토큰화 방법을 차례대로 실습할 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc9052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK(Natural Language Toolkit) 라이브러리에서 토큰화 도구를 불러옵니다\n",
    "import nltk\n",
    "# punkt: NLTK에서 제공하는 영어 문장 및 단어 토큰화 모델을 다운로드합니다\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize  # 단어 토큰화 함수 불러오기\n",
    "\n",
    "text = \"NLTK is a tool that can preprocess natural language.\"\n",
    "\n",
    "# 영어 문장을 단어 단위로 토큰화합니다\n",
    "words = word_tokenize(text)\n",
    "print(\"단어 토큰화 결과:\", words)  # 토큰화된 단어 리스트 출력\n",
    "\n",
    "# 다음: 한글 텍스트에 대해 글자 단위 토큰화를 실습합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e115e4",
   "metadata": {},
   "source": [
    "### 영어 단어 토큰화 결과 해설\n",
    "\n",
    "위 코드에서는 NLTK의 `word_tokenize` 함수를 사용하여 영어 문장을 단어 단위로 분리했습니다. 이 함수는 단순히 공백 기준으로 자르는 것이 아니라, 문장 부호(예: 마침표, 쉼표)도 별도의 토큰으로 분리합니다. 실제 출력 결과를 보면 마침표('.')도 하나의 토큰으로 처리된 것을 볼 수 있습니다.\n",
    "\n",
    "이처럼 영어의 경우 띄어쓰기와 문장 부호가 명확하므로, 단어 단위 토큰화가 비교적 간단하게 이루어집니다.\n",
    "\n",
    "이제 한글 텍스트에 대해 글자 단위로 토큰화하는 방법을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba5489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 텍스트를 글자 단위로 분리하는 예제입니다\n",
    "text = \"안녕하세요? 텍스트 전처리 시간에 오신 것을 환영합니다.\"\n",
    "\n",
    "# list() 함수를 사용하면 문자열을 한 글자씩 쪼개어 리스트로 만듭니다\n",
    "characters = list(text)\n",
    "print(\"글자 토큰화 결과:\", characters)  # 글자 단위로 분리된 결과 출력\n",
    "\n",
    "# 다음: 한글의 형태소 단위 토큰화를 위해 KoNLPy 라이브러리를 설치합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f8866e",
   "metadata": {},
   "source": [
    "### 한글 글자 토큰화의 한계와 필요성\n",
    "\n",
    "한글은 영어와 달리 띄어쓰기만으로 의미 단위를 정확히 구분하기 어렵습니다. 예를 들어, '전처리'는 두 글자가 합쳐져 하나의 의미를 가지지만, 글자 단위로 쪼개면 의미가 사라집니다. 따라서 한글에서는 글자 단위보다는 형태소(morpheme) 단위, 즉 의미를 가지는 최소 단위로 분리하는 것이 더 중요합니다.\n",
    "\n",
    "이를 위해서는 한글 형태소 분석기가 필요합니다. 대표적인 파이썬 라이브러리로 KoNLPy(코엔엘파이)가 있습니다.\n",
    "\n",
    "이제 KoNLPy를 설치하고, 형태소 단위 토큰화를 실습해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad54920d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## KoNLPy(코엔엘파이) 설치\n",
    "# KoNLPy는 파이썬에서 한글 형태소 분석을 지원하는 라이브러리입니다\n",
    "# 형태소 분석기 중 하나인 Okt(Open Korean Text)를 사용할 예정입니다\n",
    "!pip install konlpy\n",
    "\n",
    "# 다음: KoNLPy의 Okt 형태소 분석기를 사용하여 한글 문장을 형태소 단위로 토큰화합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345ee39",
   "metadata": {},
   "source": [
    "### KoNLPy와 형태소 분석의 필요성\n",
    "\n",
    "한글은 조사, 어미, 접사 등 다양한 문법 요소가 결합되어 단어가 만들어집니다. 이런 구조 때문에 단순히 띄어쓰기나 글자 단위로 분리하면 의미를 제대로 파악할 수 없습니다. 형태소 분석(morphological analysis)은 문장에서 의미를 가지는 최소 단위(형태소)로 분리하여, 각 단어의 어간(stem)과 어미, 조사 등을 구분할 수 있게 해줍니다.\n",
    "\n",
    "KoNLPy는 여러 한글 형태소 분석기를 파이썬에서 사용할 수 있도록 도와주는 라이브러리입니다. 그 중 Okt(Open Korean Text)는 트위터에서 개발한 형태소 분석기로, 비교적 최신 한글 문장에도 잘 동작합니다.\n",
    "\n",
    "이제 Okt 형태소 분석기를 사용하여 한글 문장을 형태소 단위로 분리하고, 어간 추출까지 실습해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c184f4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoNLPy에서 Okt(Open Korean Text) 형태소 분석기를 불러옵니다\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()  # Okt 형태소 분석기 객체 생성\n",
    "\n",
    "text = \"안녕하세요? 텍스트 전처리 시간에 오신 것을 환영합니다.\"\n",
    "\n",
    "# 형태소 단위로 토큰화 (어간 추출 없이)\n",
    "morphs = okt.morphs(text)\n",
    "# 형태소 단위로 토큰화 (어간 추출 포함)\n",
    "morphs_stem = okt.morphs(text, stem=True)\n",
    "\n",
    "print(\"형태소 토큰화 결과:\", morphs)\n",
    "print(\"형태소 토큰화 결과(어간 추출):\", morphs_stem)\n",
    "\n",
    "# 다음: 형태소 분석기의 다양한 기능(어절 추출, 품사 태깅, 명사 추출)을 실습합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d200228",
   "metadata": {},
   "source": [
    "### 형태소 토큰화 결과 해설\n",
    "\n",
    "위 코드에서 `okt.morphs()` 함수는 문장을 의미 단위인 형태소로 분리합니다. 예를 들어, '환영합니다'는 '환영', '합니다'로 분리되고, '안녕하세요'는 '안녕하세요' 하나의 형태소로 처리됩니다.\n",
    "\n",
    "`stem=True` 옵션을 주면 동사나 형용사의 어간(stem)만 추출합니다. 예를 들어, '합니다'는 '하다'로, '안녕하세요'는 '안녕하다'로 변환됩니다. 이는 텍스트 내에서 동일한 의미를 가진 단어를 통일적으로 처리할 수 있게 해줍니다.\n",
    "\n",
    "이제 형태소 분석기의 추가 기능인 어절 추출, 품사 태깅, 명사 추출을 실습해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7addde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okt 형태소 분석기의 다양한 기능 실습\n",
    "# 어절(phrase) 추출, 품사 태깅(POS tagging), 명사 추출 기능을 사용합니다\n",
    "\n",
    "phrases = okt.phrases(text)  # 어절 단위로 추출\n",
    "pos = okt.pos(text, join=True)  # 품사 태깅 결과를 '형태소/품사' 형태로 반환\n",
    "nouns = okt.nouns(text)  # 명사만 추출\n",
    "\n",
    "print(\"어절 추출 결과:\", phrases)\n",
    "print(\"품사 태깅 결과:\", pos)\n",
    "print(\"명사 추출 결과:\", nouns)\n",
    "\n",
    "# 다음: 형태소 분석 결과를 실제 자연어 처리(NLP) 작업에 어떻게 활용할 수 있는지 살펴봅니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3035f99",
   "metadata": {},
   "source": [
    "### 형태소 분석기의 다양한 기능 해설\n",
    "\n",
    "- **어절 추출(phrases)**: 문장에서 의미 있는 구(phrase) 또는 어절을 추출합니다. 예를 들어, '텍스트 전처리', '오신 것', '환영' 등은 각각 하나의 의미 단위로 볼 수 있습니다.\n",
    "- **품사 태깅(POS tagging)**: 각 형태소에 대해 품사(명사, 동사, 형용사 등)를 태깅합니다. 예를 들어, '텍스트/Noun', '합니다/Verb'와 같이 형태소와 품사를 함께 제공합니다. 이는 품사별로 단어를 분류하거나 특정 품사만 추출할 때 유용합니다.\n",
    "- **명사 추출(nouns)**: 문장에서 명사만을 추출합니다. 명사는 주로 텍스트 분석에서 핵심 키워드로 사용되기 때문에, 명사 추출은 텍스트 마이닝, 감정 분석, 요약 등 다양한 NLP 작업에서 매우 중요합니다.\n",
    "\n",
    "이제 형태소 분석 결과를 실제 자연어 처리(NLP) 작업에 어떻게 활용할 수 있는지, 그리고 토큰화 이후의 전처리 과정(불용어 제거, 정규화 등)에 대해 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce27416",
   "metadata": {},
   "source": [
    "## 불용어(Stopword) 제거\n",
    "\n",
    "자연어 처리(Natural Language Processing, NLP)에서 텍스트를 분석할 때, 의미를 크게 담고 있지 않은 단어들이 많이 등장합니다. 예를 들어 영어의 'is', 'the', 'and' 또는 한국어의 '이', '그', '저' 등이 있습니다. 이러한 단어들을 **불용어(Stopword)**라고 부릅니다.\n",
    "\n",
    "### 왜 불용어를 제거해야 할까?\n",
    "불용어는 문장 내에서 자주 등장하지만, 실제로 문장의 의미를 파악하는 데에는 큰 도움이 되지 않습니다. 따라서 불용어를 제거하면, 텍스트 데이터의 노이즈(noise)를 줄이고, 중요한 단어들에 더 집중할 수 있습니다. 특히 텍스트 분류, 감정 분석, 토픽 모델링 등에서 성능 향상에 도움이 됩니다.\n",
    "\n",
    "이제 NLTK 라이브러리를 활용하여 영어 불용어를 제거하는 방법을 실습해보겠습니다. 다음 코드에서는 불용어 리스트를 다운로드하고, 예시 문장에서 불용어를 제거하는 과정을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283c9b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK 라이브러리에서 불용어 리스트를 불러오기 위한 모듈 임포트\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 불용어 리스트를 다운로드 (최초 1회만 필요)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 예시 문장 정의\n",
    "text = \"This is a simple sentence for demonstrating stopword removal.\"\n",
    "\n",
    "# 영어 불용어 리스트를 set(집합) 형태로 가져옴\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 문장을 단어 단위로 분리 (공백 기준)\n",
    "words = text.split()\n",
    "\n",
    "# 불용어가 아닌 단어만 남기는 리스트 컴프리헨션\n",
    "filtered_sentence = [w for w in words if not w.lower() in stop_words]\n",
    "\n",
    "# 불용어가 제거된 결과 출력\n",
    "print(\"불용어 제거 결과:\", filtered_sentence)\n",
    "\n",
    "# 다음 단계: 어간 추출(단어의 뿌리 형태로 변환) 및 표제어 추출 실습으로 넘어갑니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8bdc5",
   "metadata": {},
   "source": [
    "## 어간 추출(Stemming)과 표제어 추출(Lemmatization)\n",
    "\n",
    "불용어를 제거한 후에도, 같은 의미를 가진 단어가 다양한 형태(예: run, running, ran)로 존재할 수 있습니다. 이런 변형된 단어들을 하나의 형태로 통일하면, 텍스트 분석의 효율이 크게 올라갑니다.\n",
    "\n",
    "이를 위해 사용하는 대표적인 방법이 **어간 추출(Stemming)**과 **표제어 추출(Lemmatization)**입니다.\n",
    "\n",
    "### 어간 추출(Stemming)이란?\n",
    "어간 추출은 단어에서 접사(어미, 접두사 등)를 제거하여, 단어의 뿌리(stem) 형태로 변환하는 기법입니다. 예를 들어, 'running', 'runs', 'ran' 모두 'run'이라는 어간으로 변환될 수 있습니다. 다만, 어간 추출은 언어학적 규칙을 무시하고 단순히 접미사 등을 잘라내기 때문에, 실제 단어가 아닌 형태가 나올 수 있습니다.\n",
    "\n",
    "이제 PorterStemmer를 사용하여 어간 추출을 직접 실습해보겠습니다. 다음 코드에서는 여러 형태의 단어를 어간으로 변환하는 과정을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK의 PorterStemmer를 사용하여 어간 추출을 수행합니다\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# PorterStemmer 객체 생성\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# 다양한 형태의 단어 리스트 정의\n",
    "words = [\"running\", \"ran\", \"runs\", \"easily\", \"fairly\"]\n",
    "\n",
    "# 각 단어에 대해 어간 추출을 적용\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "# 어간 추출 결과 출력\n",
    "print(\"어간 추출 결과:\", stemmed_words)\n",
    "\n",
    "# 주의: 어간 추출은 단순 규칙 기반이므로 실제 단어가 아닐 수도 있음(e.g., 'easili', 'fairli')\n",
    "\n",
    "# 다음 단계: 표제어 추출(Lemmatization)로 더 정확한 원형 복원을 실습합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b496e",
   "metadata": {},
   "source": [
    "### 표제어 추출(Lemmatization)이란?\n",
    "\n",
    "어간 추출이 단순히 접미사 등을 잘라내는 반면, **표제어 추출(Lemmatization)**은 단어의 품사와 문맥을 고려하여 사전에 등록된 '정확한 원형(lemma)'으로 변환합니다. 예를 들어, 'running', 'ran', 'runs' 모두 'run'이라는 실제 단어로 변환됩니다. 표제어 추출은 어간 추출보다 더 정확하고 자연스러운 결과를 제공합니다.\n",
    "\n",
    "이번에는 NLTK의 WordNetLemmatizer를 사용하여 표제어 추출을 실습해보겠습니다. 다음 코드에서는 동사 품사(pos='v')를 지정하여, 각 단어를 표제어로 변환하는 과정을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fb9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK의 WordNetLemmatizer를 사용하여 표제어 추출을 수행합니다\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# 표제어 추출에 필요한 WordNet 데이터 다운로드 (최초 1회만 필요)\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# WordNetLemmatizer 객체 생성\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 다양한 형태의 단어 리스트 정의\n",
    "words = [\"running\", \"ran\", \"runs\", \"easily\", \"fairly\"]\n",
    "\n",
    "# 각 단어에 대해 표제어 추출을 적용 (동사 품사로 지정)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=\"v\") for word in words]\n",
    "\n",
    "# 표제어 추출 결과 출력\n",
    "print(\"표제어 추출 결과:\", lemmatized_words)\n",
    "\n",
    "# 참고: 'easily', 'fairly'는 동사가 아니므로 원형 그대로 반환됨\n",
    "\n",
    "# 다음 단계: 전처리된 데이터를 활용한 텍스트 분석(예: 단어 빈도수 계산)으로 넘어갑니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e44629e",
   "metadata": {},
   "source": [
    "## 한국어 어간 추출 (Stemming)\n",
    "\n",
    "자연어 처리(NLP)에서 텍스트를 분석할 때, 단어의 다양한 형태(예: '뛰어다니는', '뛰어다니다', '뛰었다')를 같은 의미 단위로 인식하는 것이 매우 중요합니다. 이때 사용하는 기법이 바로 **어간 추출(stemming)** 입니다. 어간(stem)은 단어에서 변화하지 않는 핵심 부분을 의미하며, 어미(ending)나 접사(affix) 등은 제거합니다.\n",
    "\n",
    "한국어는 영어보다 형태소 변화가 훨씬 다양하고 복잡하기 때문에, 어간 추출이 특히 중요합니다. 예를 들어, '뛰어다니는', '뛰어다니다', '뛰어다녔다'는 모두 '뛰어다니다'라는 동일한 의미의 어간을 가집니다. 이를 통해 단어 빈도 분석, 텍스트 분류, 감정 분석 등 다양한 NLP 작업에서 데이터의 일관성을 높일 수 있습니다.\n",
    "\n",
    "이제 실제로 파이썬에서 대표적인 한국어 형태소 분석기인 **Okt(Open Korean Text)**를 이용해 어간 추출을 수행해보겠습니다. 다음 코드에서는 stem=True 옵션을 사용하여 어간만 추출하는 방법을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abacf320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# konlpy의 Okt(Open Korean Text) 형태소 분석기를 불러옵니다\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# Okt 객체 생성\n",
    "okt = Okt()\n",
    "\n",
    "# 분석할 문장 예시\n",
    "text = \"뛰어다니는 사람과 걸어다니는 사람 모두 책을 좋아합니다.\"\n",
    "\n",
    "# 어간 추출 수행 (stem=True 옵션을 사용하면 각 단어의 어간만 추출)\n",
    "stemmed_words = okt.morphs(text, stem=True)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"어간 추출 결과:\", stemmed_words)\n",
    "\n",
    "# 다음: 어간 추출 이후, 토큰화(tokenization) 방식의 발전인 서브워드(subword) 기법에 대해 알아봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec448b93",
   "metadata": {},
   "source": [
    "#### 서브워드(Subword) 기법의 필요성과 원리\n",
    "\n",
    "기존의 단어 단위 토큰화(word-level tokenization)는 사전에 없는 신조어나 희귀어(Out-Of-Vocabulary, OOV) 문제에 취약합니다. 예를 들어, 새로운 단어가 등장하면 모델이 이를 인식하지 못하거나, 모든 단어를 사전에 등록해야 하므로 어휘 집합이 지나치게 커집니다.\n",
    "\n",
    "이를 해결하기 위해 **서브워드(subword)** 기법이 등장했습니다. 서브워드는 단어보다 작은 의미 단위(예: 접두사, 접미사, 글자 등)로 텍스트를 분할합니다. 이 방식은 다음과 같은 장점이 있습니다:\n",
    "\n",
    "- 신조어나 희귀어도 기존의 서브워드 조합으로 표현 가능\n",
    "- 어휘 집합 크기를 효과적으로 줄일 수 있음\n",
    "- 다양한 언어(특히 교착어, 표의문자 언어)에 유연하게 적용 가능\n",
    "\n",
    "대표적인 서브워드 분할 알고리즘에는 **BPE(Byte Pair Encoding)**와 **Unigram Language Model**이 있습니다.\n",
    "\n",
    "이제 BPE 알고리즘의 동작 원리를 직접 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f219f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE(Byte Pair Encoding) 알고리즘을 직접 구현합니다\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# 예시 토큰 집합: 각 단어(공백으로 분리된 문자 시퀀스)와 빈도수\n",
    "# 실제로는 단어를 문자 단위로 분해해서 시작합니다\n",
    "tokens = {'l o w': 5, 'l o w e r': 2, 'n e w': 6, 'w i d e r': 3}\n",
    "\n",
    "# 각 토큰 집합에서 인접한 문자 쌍(빅그램)의 빈도수를 계산하는 함수\n",
    "def get_stats(tokens):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in tokens.items():\n",
    "        symbols = word.split()  # 단어를 문자 단위로 분리\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq  # 인접 문자쌍 빈도 누적\n",
    "    return pairs\n",
    "\n",
    "# 가장 많이 등장하는 문자쌍을 하나의 문자로 병합하는 함수\n",
    "def merge_vocab(pair, tokens):\n",
    "    new_vocab = {}\n",
    "    bigram = ' '.join(pair)  # 병합할 문자쌍\n",
    "    replacement = ''.join(pair)  # 병합 후 문자\n",
    "    for word in tokens:\n",
    "        # 해당 문자쌍을 하나의 문자로 치환\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_vocab[new_word] = tokens[word]\n",
    "    return new_vocab\n",
    "\n",
    "# BPE 어휘 구축 함수\n",
    "def bpe_vocab(tokens, num_merges):\n",
    "    vocab = set()  # 최종 서브워드 어휘 집합\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(tokens)  # 현재 상태에서 문자쌍 빈도 계산\n",
    "        if not pairs:\n",
    "            break  # 더 이상 병합할 쌍이 없으면 종료\n",
    "        best = max(pairs, key = pairs.get)  # 가장 빈도 높은 쌍 선택\n",
    "        tokens = merge_vocab(best, tokens)  # 병합 수행\n",
    "        vocab.add(''.join(best))  # 병합된 쌍을 어휘에 추가\n",
    "        print(f\"Merge {i + 1}: {best}\")\n",
    "        print(f\"Current tokens: {tokens}\")\n",
    "    return vocab\n",
    "\n",
    "# 반복 횟수 설정 (병합 횟수)\n",
    "iter_num = 10\n",
    "final_vocab = bpe_vocab(tokens, iter_num)\n",
    "print(f\"Final BPE Vocab: {final_vocab}\")\n",
    "\n",
    "# 다음: BPE와 달리 확률 기반으로 서브워드를 선택하는 Unigram Language Model을 구현해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85341e9",
   "metadata": {},
   "source": [
    "#### Unigram Language Model 기반 서브워드 분할\n",
    "\n",
    "BPE는 항상 가장 많이 등장하는 문자쌍을 병합하지만, **Unigram Language Model**은 각 서브워드가 등장할 확률을 학습하고, 확률이 높은 조합을 선택합니다. 이 방식은 다양한 분할 가능성을 평가할 수 있어 더 유연한 토큰화를 지원합니다.\n",
    "\n",
    "Unigram 방식의 핵심 아이디어는 다음과 같습니다:\n",
    "- 미리 정의된 서브워드 후보 집합에서, 각 서브워드의 확률을 학습\n",
    "- 실제 단어를 여러 가지 서브워드 조합으로 분해할 수 있으며, 그 중 확률이 가장 높은 조합을 선택\n",
    "- 학습 과정에서 불필요한 서브워드는 점차 제거하여 최적의 어휘 집합을 만듦\n",
    "\n",
    "이제 간단한 예시로 Unigram 모델을 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3cbd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram Language Model 방식의 서브워드 분할 예시\n",
    "import random\n",
    "\n",
    "# 서브워드 후보 집합(token_candidates)과 말뭉치(corpus)를 입력받아 확률을 학습하는 함수\n",
    "def unigram_model(token_candidates, corpus):\n",
    "    # 각 서브워드 후보에 임의의 초기 확률 할당\n",
    "    token_prob = {token: random.uniform(0.1, 1.0) for token in token_candidates}\n",
    "    total_prob = sum(token_prob.values())\n",
    "    token_prob = {token: prob / total_prob for token, prob in token_prob.items()}  # 정규화\n",
    "\n",
    "    # 확률 업데이트를 반복 (실제 학습에서는 EM 알고리즘 사용)\n",
    "    for _ in range(5):\n",
    "        new_vocab = set()\n",
    "        for word in corpus:\n",
    "            best_tokenization = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                for token in token_candidates:\n",
    "                    # 현재 위치에서 토큰이 일치하면 분할\n",
    "                    if word[i:i + len(token)] == token:\n",
    "                        best_tokenization.append(token)\n",
    "                        i += len(token)\n",
    "                        break\n",
    "            new_vocab.update(best_tokenization)\n",
    "\n",
    "        # 사용된 서브워드만 남기고 확률 재계산\n",
    "        token_prob = {token: token_prob.get(token, 0.1) for token in new_vocab}\n",
    "        total_prob = sum(token_prob.values())\n",
    "        token_prob = {token: prob / total_prob for token, prob in token_prob.items()}\n",
    "\n",
    "    return token_prob\n",
    "\n",
    "# 서브워드 후보 예시\n",
    "token_candidates = ['b', 'a', 'n', 'd', 'ba', 'ban', 'band', 'banana', 'it']\n",
    "corpus = ['banana', 'band', 'bandit']\n",
    "\n",
    "# Unigram 모델로 최종 서브워드 확률 계산\n",
    "final_prob = unigram_model(token_candidates, corpus)\n",
    "print(final_prob)\n",
    "\n",
    "# 다음: 실제 대규모 텍스트에서 단어 사전을 구축하고, 토큰화에 활용하는 방법을 알아봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673f44d",
   "metadata": {},
   "source": [
    "#### 실제 말뭉치에서 단어 사전 구축과 토크나이저\n",
    "\n",
    "서브워드 분할 외에도, 전통적으로는 대규모 말뭉치(corpus)에서 단어의 빈도를 기반으로 **단어 사전(vocabulary)**을 구축합니다. 이 사전은 각 단어에 고유한 인덱스를 부여하여, 텍스트를 숫자 시퀀스로 변환하는 데 사용됩니다. 이 과정은 자연어 처리 모델의 입력 전처리에서 매우 중요한 단계입니다.\n",
    "\n",
    "단어 사전 구축의 주요 단계는 다음과 같습니다:\n",
    "1. 말뭉치에서 모든 문장을 분리하고, 각 문장에서 단어를 추출\n",
    "2. 모든 단어의 빈도를 계산\n",
    "3. 빈도순으로 정렬하여, 자주 등장하는 단어에 낮은 인덱스를 부여\n",
    "4. 단어-인덱스 매핑을 생성하여 토크나이저(tokenizer)로 활용\n",
    "\n",
    "이제 NLTK의 영화 리뷰 데이터셋을 활용해 실제로 단어 사전을 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3034c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK를 이용한 단어 사전(vocabulary) 구축 예시\n",
    "# nltk 패키지가 설치되어 있지 않다면 설치\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# NLTK의 토크나이저와 데이터셋 다운로드\n",
    "nltk.download('punkt')  # 문장, 단어 분리용\n",
    "nltk.download('movie_reviews')  # 영화 리뷰 데이터셋\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# 영화 리뷰 데이터셋에서 모든 문장을 분리\n",
    "documents = list(movie_reviews.sents())\n",
    "all_words = []  # 모든 단어를 저장할 리스트\n",
    "\n",
    "# 각 문장에서 단어를 소문자로 변환하여 리스트에 추가\n",
    "for sentence in documents:\n",
    "    for word in sentence:\n",
    "        all_words.append(word.lower())\n",
    "\n",
    "# 단어별 등장 빈도 계산\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# 빈도순으로 정렬하여 인덱스 부여 (1부터 시작)\n",
    "sorted_vocab = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "index_to_word = {idx+1: word for idx, (word, _) in enumerate(sorted_vocab)}  # 인덱스→단어\n",
    "word_to_index = {word: idx for idx, word in index_to_word.items()}  # 단어→인덱스\n",
    "\n",
    "print(f\"단어 사전 크기: {len(word_to_index)}\")\n",
    "print(f\"상위 10개의 단어와 그 인덱스: {list(word_to_index.items())[:10]}\")\n",
    "\n",
    "# 다음: 구축한 단어 사전을 활용해 텍스트를 숫자 시퀀스로 변환하는 방법을 실습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c259b7",
   "metadata": {},
   "source": [
    "## 서브워드 분절(Subword Segmentation)과 SentencePiece 적용\n",
    "\n",
    "자연어 처리(NLP)에서 텍스트를 컴퓨터가 이해할 수 있는 형태로 바꾸는 첫 단계는, 텍스트를 일정 단위(토큰)로 나누는 것입니다. 그런데 영어처럼 띄어쓰기가 명확하지 않거나, 신조어·오타·희귀어가 많은 경우에는 기존의 단어 단위 토큰화로는 모든 어휘를 다루기 어렵습니다. 이 문제를 해결하기 위해 등장한 것이 **서브워드 분절(subword segmentation)** 기법입니다.\n",
    "\n",
    "서브워드 분절은 단어를 더 작은 단위(서브워드)로 쪼개어, 새로운 단어나 희귀 단어도 기존 서브워드 조합으로 표현할 수 있게 합니다. 대표적인 도구가 바로 **SentencePiece**입니다.\n",
    "\n",
    "이제 실제로 SentencePiece를 설치하고, 영화 리뷰 데이터셋을 이용해 서브워드 분절 모델을 학습해보겠습니다. 이후에는 벡터화(vectorization) 기법으로 넘어가, 텍스트를 어떻게 수치로 표현하는지 살펴볼 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6427187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentencePiece 라이브러리 설치\n",
    "# SentencePiece는 서브워드 분절을 위한 구글의 오픈소스 도구입니다\n",
    "!pip install sentencepiece\n",
    "\n",
    "# 다음 단계: NLTK에서 영화 리뷰 데이터를 수집하고, SentencePiece로 서브워드 모델을 학습합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import nltk  # 자연어처리용 라이브러리\n",
    "import sentencepiece as spm  # 서브워드 분절 도구\n",
    "\n",
    "# NLTK에서 영화 리뷰 데이터셋과 토크나이저 다운로드\n",
    "nltk.download('movie_reviews')  # 영화 리뷰 데이터셋\n",
    "nltk.download('punkt')  # 문장 토크나이저\n",
    "from nltk.corpus import movie_reviews  # 영화 리뷰 데이터셋 불러오기\n",
    "\n",
    "# 1. 영화 리뷰 데이터를 문장 단위로 수집\n",
    "# movie_reviews.sents()는 각 리뷰를 문장 단위로 분리하여 반환\n",
    "documents = list(movie_reviews.sents())\n",
    "\n",
    "# 2. 수집한 문장 데이터를 파일로 저장\n",
    "# SentencePiece는 파일 입력만 지원하므로, 모든 문장을 한 줄씩 저장\n",
    "with open('movie_reviews.txt', 'w', encoding='utf-8') as f:\n",
    "    for sentence in documents:\n",
    "        f.write(\" \".join(sentence) + \"\\n\")  # 각 문장을 띄어쓰기로 합쳐서 한 줄로 저장\n",
    "\n",
    "# 3. SentencePiece 모델 학습\n",
    "# input: 학습에 사용할 텍스트 파일 경로\n",
    "# model_prefix: 생성될 모델/어휘 파일 이름 접두사\n",
    "# vocab_size: 생성할 서브워드(어휘) 개수\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='movie_reviews.txt',\n",
    "    model_prefix='movie_review_spm',\n",
    "    vocab_size=5000\n",
    ")\n",
    "\n",
    "# 4. 학습된 어휘 파일(.vocab)에서 상위 10개 서브워드 출력\n",
    "vocab_file = 'movie_review_spm.vocab'\n",
    "\n",
    "with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "    vocab_list = f.readlines()\n",
    "\n",
    "print(\"상위 10개의 subword:\")\n",
    "for i in range(10):\n",
    "    subword = vocab_list[i].split('\\t')[0]  # 탭으로 구분되어 있음\n",
    "    print(f\"{i}: {subword}\")\n",
    "\n",
    "# 다음: 텍스트를 숫자 벡터로 바꾸는 다양한 벡터화(Vectorization) 기법을 소개합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4fbcba",
   "metadata": {},
   "source": [
    "### SentencePiece 어휘 파일 예시 분석\n",
    "\n",
    "위 코드에서 출력된 상위 10개의 subword(서브워드)는 다음과 같습니다:\n",
    "\n",
    "- `<unk>`, `<s>`, `</s>`: 각각 미등록 토큰(unknown), 문장 시작, 문장 끝을 의미하는 특수 토큰입니다.\n",
    "- `s`, `▁the`, `▁,`, `▁.`, `▁a`, `▁and`, `▁of` 등: 실제로 많이 등장하는 단어 또는 서브워드입니다. `▁`(언더바)는 공백을 의미하며, 단어의 시작을 표시합니다.\n",
    "  - 예시: `▁the`는 \"the\"라는 단어가 문장 내에서 독립적으로 등장할 때를 의미합니다.\n",
    "\n",
    "이처럼 서브워드 분절은 자주 등장하는 단어는 하나의 토큰으로, 드물게 등장하는 단어는 여러 서브워드로 쪼개어 처리할 수 있도록 해줍니다. 이 방식은 희귀 단어, 신조어, 오타 등도 효과적으로 다룰 수 있게 해줍니다.\n",
    "\n",
    "이제 텍스트를 숫자 벡터로 변환하는 다양한 벡터화(Vectorization) 기법에 대해 알아보겠습니다. 이 과정은 텍스트를 기계학습 모델이 이해할 수 있는 형태로 바꾸는 핵심 단계입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd4c4e9",
   "metadata": {},
   "source": [
    "## 표현 변환(벡터화, Vectorization)란 무엇인가?\n",
    "\n",
    "자연어 텍스트는 본질적으로 문자와 단어의 나열이기 때문에, 컴퓨터가 직접적으로 이해하거나 계산할 수 없습니다. 따라서 텍스트를 수치(숫자)로 변환하는 과정이 필요합니다. 이 과정을 **벡터화(vectorization)** 라고 하며, 텍스트의 의미나 구조를 최대한 보존하면서 수치로 바꾸는 것이 중요합니다.\n",
    "\n",
    "### 왜 단순 숫자 인덱스가 아닌 벡터가 필요한가?\n",
    "- 단순히 각 단어에 고유 번호(인덱스)만 부여하면, 단어 간 의미적 유사성이나 관계를 반영할 수 없습니다.\n",
    "- 예를 들어, \"고양이\"와 \"강아지\"는 의미상 가깝지만, 인덱스만 다르면 컴퓨터는 이 둘의 관계를 알 수 없습니다.\n",
    "- 벡터화는 각 단어를 다차원 공간의 점(벡터)으로 표현하여, 의미적으로 비슷한 단어는 가까운 위치에 있도록 만듭니다.\n",
    "\n",
    "### 주요 벡터화 기법\n",
    "- **BoW(Bag Of Words, 단어 집합 모델)**: 문서 내 단어의 등장 여부 혹은 빈도만을 고려\n",
    "- **TF-IDF(Term Frequency - Inverse Document Frequency, 단어 빈도-역문서 빈도)**: 자주 등장하지만 여러 문서에 흔한 단어의 영향력을 줄임\n",
    "- **Word2Vec**: 단어의 의미적 유사성을 반영하는 분산 표현(distributed representation)\n",
    "- **Embedding Layer(임베딩 레이어)**: 신경망에서 학습 가능한 임베딩을 생성\n",
    "\n",
    "이제 실제로 각 벡터화 기법을 실습하기 위해 필요한 라이브러리를 설치하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748df171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 등 분산 표현 학습을 위한 gensim 라이브러리 설치\n",
    "!pip install gensim\n",
    "\n",
    "# 다음: TF-IDF, BoW 등 벡터화 기법 구현을 위한 scikit-learn 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9850e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn은 BoW, TF-IDF 등 다양한 벡터화 도구를 제공합니다\n",
    "# 최신 버전 호환성 문제를 방지하기 위해 버전을 명시적으로 지정합니다\n",
    "!pip install scikit-learn==1.6.1\n",
    "\n",
    "# 다음: BoW, TF-IDF, Word2Vec 등 벡터화 기법을 실제로 적용해봅니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e950106d",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)\n",
    "\n",
    "자연어 처리를 할 때, 텍스트 데이터를 컴퓨터가 이해할 수 있는 숫자 벡터로 바꾸는 것은 매우 중요합니다. 그 중 가장 기본적인 방법이 바로 **Bag of Words(BoW, 단어들의 가방)**입니다.\n",
    "\n",
    "### 왜 BoW가 필요한가?\n",
    "텍스트는 본질적으로 비정형 데이터이기 때문에, 기계 학습(머신러닝) 모델이 직접 처리할 수 없습니다. BoW는 각 문서(혹은 문장)를 고정된 길이의 벡터로 변환하여, 텍스트를 수치화합니다. 이때 각 벡터의 원소는 해당 단어가 문서에 몇 번 등장했는지를 나타냅니다.\n",
    "\n",
    "### BoW의 특징과 한계\n",
    "- **장점**: 구현이 매우 간단하고, 빠르게 동작합니다. 단어의 등장 빈도만으로도 텍스트 분류 등 여러 작업에서 꽤 좋은 성능을 낼 수 있습니다.\n",
    "- **한계**: 단어의 순서(문맥) 정보가 완전히 사라집니다. 예를 들어, \"나는 밥을 먹었다\"와 \"먹었다 나는 밥을\"은 BoW에서 동일하게 처리됩니다. 또한, 단어 간의 의미적 유사성도 반영하지 못합니다.\n",
    "\n",
    "이제 BoW를 실제로 어떻게 구현하는지 살펴보겠습니다. 다음 코드 셀에서는 scikit-learn의 `CountVectorizer`를 사용하여 세 개의 문장으로 이루어진 코퍼스(corpus)를 BoW 벡터로 변환해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c39ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn에서 BoW 벡터화를 위한 CountVectorizer를 불러옵니다\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 예시로 사용할 문장(코퍼스) 정의\n",
    "corpus = [\n",
    "    \"I love natural language processing\",  # 첫 번째 문장\n",
    "    \"Language processing is essential for AI\",  # 두 번째 문장\n",
    "    \"AI is changing the world\"  # 세 번째 문장\n",
    "]\n",
    "\n",
    "# CountVectorizer 객체 생성 (단어 빈도 기반 벡터화 도구)\n",
    "vectorizer = CountVectorizer()\n",
    "# 코퍼스에 등장하는 모든 단어의 빈도를 세어 BoW 행렬로 변환\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# BoW 벡터화 결과 출력\n",
    "print(\"BoW 벡터화 결과:\")\n",
    "print(X.toarray())  # 각 문장을 단어 빈도 벡터로 변환한 결과\n",
    "# BoW에서 사용된 전체 단어(단어 사전, vocabulary) 출력\n",
    "print(\"BoW 단어 사전:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# 다음: BoW의 한계를 보완하는 TF-IDF 벡터화를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c34698",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "BoW는 단어의 빈도만 고려하기 때문에, 모든 문서에서 자주 등장하는 단어(예: 'is', 'the', 'for')가 중요하지 않음에도 불구하고 높은 값을 가질 수 있습니다. 이런 한계를 극복하기 위해 **TF-IDF**라는 기법이 고안되었습니다.\n",
    "\n",
    "### 왜 TF-IDF가 필요한가?\n",
    "TF-IDF는 단어의 빈도(Term Frequency, TF)와 역문서 빈도(Inverse Document Frequency, IDF)를 곱하여, 특정 문서에서 자주 등장하지만 전체 문서에서는 드문 단어에 더 높은 가중치를 부여합니다. 즉, 모든 문서에 흔하게 등장하는 단어는 중요도가 낮고, 특정 문서에서만 두드러지게 나타나는 단어는 중요도가 높다고 판단합니다.\n",
    "\n",
    "- **TF(단어 빈도)**: 한 문서 내에서 특정 단어가 얼마나 자주 등장하는지\n",
    "- **IDF(역문서 빈도)**: 전체 문서 중 해당 단어가 등장한 문서의 비율의 역수(로그 스케일)\n",
    "\n",
    "이렇게 하면, 불용어(Stopwords)와 같이 모든 문서에 자주 등장하는 단어의 영향력을 줄이고, 문서의 특성을 잘 나타내는 단어에 더 집중할 수 있습니다.\n",
    "\n",
    "이제, BoW와 동일한 코퍼스에 대해 TF-IDF 벡터화를 적용해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1a2500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn에서 TF-IDF 벡터화를 위한 TfidfVectorizer를 불러옵니다\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# BoW에서 사용한 것과 동일한 코퍼스를 사용합니다\n",
    "corpus = [\n",
    "    \"I love natural language processing\",  # 첫 번째 문장\n",
    "    \"Language processing is essential for AI\",  # 두 번째 문장\n",
    "    \"AI is changing the world\"  # 세 번째 문장\n",
    "]\n",
    "\n",
    "# TfidfVectorizer 객체 생성 (TF-IDF 기반 벡터화 도구)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "# 코퍼스를 TF-IDF 행렬로 변환\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# TF-IDF 벡터화 결과 출력\n",
    "print(\"TF-IDF 벡터화 결과:\")\n",
    "print(X_tfidf.toarray())  # 각 문장을 TF-IDF 벡터로 변환한 결과\n",
    "# TF-IDF에서 사용된 전체 단어(단어 사전, vocabulary) 출력\n",
    "print(\"TF-IDF 단어 사전:\", tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# 다음: 단어의 의미적 유사성까지 반영하는 Word2Vec 임베딩을 소개합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f8db9",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "BoW와 TF-IDF는 모두 단어의 빈도나 중요도를 기반으로 벡터를 만듭니다. 하지만 이 방식들은 단어 간의 의미적 관계(예: 유사성, 문맥)를 반영하지 못합니다. 이를 극복하기 위해 **Word2Vec**과 같은 분산 표현(distributed representation) 기법이 등장했습니다.\n",
    "\n",
    "### 왜 Word2Vec이 필요한가?\n",
    "자연어에는 단어의 순서와 의미적 유사성이 매우 중요합니다. 예를 들어, '서울'과 '부산'은 모두 도시라는 공통점이 있고, '바나나'와 '복숭아'는 모두 과일입니다. BoW나 TF-IDF로는 이런 의미적 유사성을 표현할 수 없습니다.\n",
    "\n",
    "Word2Vec은 대량의 텍스트 코퍼스를 학습하여, 비슷한 문맥에서 자주 등장하는 단어들이 서로 가까운 벡터(고차원 실수 벡터)로 매핑되도록 만듭니다. 즉, 단어의 의미적, 문맥적 특성을 벡터 공간에 반영할 수 있습니다.\n",
    "\n",
    "### 예시로 이해하기\n",
    "아래 두 문장을 생각해봅시다.\n",
    "\n",
    "- \"나는 __에 살고 있습니다.\"\n",
    "- \"나는 __를 먹고 있습니다.\"\n",
    "\n",
    "첫 번째 문장의 빈칸에는 '서울', '부산', '한국' 등 장소와 관련된 명사가 들어가고, 두 번째 문장에는 '밥', '바나나', '복숭아' 등 음식이 들어갑니다. Word2Vec은 이런 문맥 정보를 학습하여, 장소끼리, 음식끼리 벡터 공간에서 가깝게 위치하도록 만듭니다.\n",
    "\n",
    "이제 다음 단계에서는 Word2Vec의 기본 원리와 실제 구현 방법을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00b6525",
   "metadata": {},
   "source": [
    "## Word2Vec(워드투벡) 임베딩 실습: 영화 리뷰 데이터셋 활용\n",
    "\n",
    "자연어 처리에서 텍스트를 수치적으로 다루기 위해서는 단어를 벡터(수치 배열)로 변환하는 과정이 필수적입니다. 이때 자주 사용되는 방법 중 하나가 Word2Vec(워드투벡) 임베딩입니다.\n",
    "\n",
    "### 왜 Word2Vec이 필요한가?\n",
    "- 기존의 단어 표현 방식(예: One-hot encoding)은 단어 간의 의미적 유사성을 반영하지 못합니다.\n",
    "- Word2Vec은 대량의 텍스트에서 단어의 주변 맥락(Context)을 학습하여, 의미적으로 비슷한 단어들이 서로 가까운 벡터 공간에 위치하도록 만듭니다.\n",
    "- 이렇게 학습된 임베딩은 텍스트 분류, 감성 분석, 추천 시스템 등 다양한 자연어 처리(NLP) 작업에서 핵심적인 역할을 합니다.\n",
    "\n",
    "### 이번 셀에서 할 일\n",
    "- NLTK의 영화 리뷰 데이터셋을 불러오고, 각 리뷰를 단어 리스트로 변환합니다.\n",
    "- Gensim 라이브러리의 Word2Vec 모델로 임베딩을 학습합니다.\n",
    "- 학습된 임베딩에서 특정 단어의 벡터와 유사 단어를 확인합니다.\n",
    "\n",
    "이제 실제로 Word2Vec 임베딩을 학습하는 코드를 살펴보겠습니다. 다음 코드 셀에서는 각 단계별로 자세한 설명을 주석으로 추가하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a599f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK, Gensim 등 필요한 라이브러리 임포트\n",
    "import nltk  # 자연어 처리 도구 모음\n",
    "from nltk.corpus import movie_reviews  # 영화 리뷰 데이터셋\n",
    "from gensim.models import Word2Vec  # Word2Vec 임베딩 모델\n",
    "import random  # (여기서는 사용하지 않지만, 데이터 샘플링 등에 활용 가능)\n",
    "\n",
    "# NLTK의 영화 리뷰 데이터셋 다운로드 (최초 1회만 필요)\n",
    "nltk.download('movie_reviews')  # 영화 리뷰 데이터셋을 로컬에 저장\n",
    "\n",
    "# 영화 리뷰 데이터셋에서 각 리뷰를 단어 리스트로 변환\n",
    "sentences = []  # 각 리뷰를 단어 리스트로 저장할 리스트\n",
    "for fileid in movie_reviews.fileids():  # 모든 리뷰 파일의 ID를 순회\n",
    "    # 각 리뷰를 단어 단위로 분리하여 리스트로 변환 후 sentences에 추가\n",
    "    sentences.append(list(movie_reviews.words(fileid)))\n",
    "\n",
    "# Word2Vec 모델 학습\n",
    "# vector_size: 임베딩 차원 수 (여기서는 100차원)\n",
    "# window: 주변 단어를 고려하는 윈도우 크기 (여기서는 5개 단어)\n",
    "# min_count: 최소 등장 빈도 (5번 미만 등장 단어는 무시)\n",
    "# sg: 1이면 Skip-gram, 0이면 CBOW 방식 (여기서는 Skip-gram)\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, sg=1)\n",
    "\n",
    "# 학습된 임베딩에서 특정 단어의 벡터 확인\n",
    "print(\"단어 'movie'의 벡터:\", model.wv['movie'])  # 'movie'라는 단어의 100차원 벡터 출력\n",
    "\n",
    "# 'movie'와 의미적으로 유사한 단어 10개 출력\n",
    "print(\"단어 'movie'와 유사한 단어들:\", model.wv.most_similar('movie'))\n",
    "\n",
    "# 다음: 임베딩 벡터를 시각화하거나, 다른 단어 쌍의 유사도 계산을 실습해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e24457",
   "metadata": {},
   "source": [
    "### 코드 결과 해설\n",
    "- 위 코드에서는 'movie'라는 단어의 벡터값(임베딩)을 출력하였습니다. 이 벡터는 100차원의 실수 배열로, 단어의 의미를 수치적으로 표현합니다.\n",
    "- 또한, 'movie'와 의미적으로 가장 가까운 단어들을 출력했습니다. 이는 Word2Vec이 단어의 의미적 맥락을 잘 학습했는지 확인하는 대표적인 방법입니다.\n",
    "\n",
    "이처럼 Word2Vec 임베딩을 활용하면 단어 간의 의미적 유사성을 수치적으로 다룰 수 있습니다. \n",
    "\n",
    "이제 다음 단계로, 학습된 임베딩 벡터를 시각화하거나, 임의의 단어 쌍 간 유사도를 계산해보는 실습을 진행하겠습니다."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
